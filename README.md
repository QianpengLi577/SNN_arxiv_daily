# 348、带系统级局部自动增益控制的脉冲耳蜗
- [ ] Spiking Cochlea with System-level Local Automatic Gain Control 
时间：2022年02月14日                         第一作者：Ilya Kiselev                       [链接](https://arxiv.org/abs/2202.06707).                     
## 摘要：由于晶体管失配和模型的复杂性，将局部自动增益控制（AGC）电路纳入硅耳蜗设计一直具有挑战性。为了解决这个问题，我们提出了一种替代的系统级算法，通过测量单个通道的输出脉冲活动，在硅脉冲耳蜗中实现特定于通道的AGC。信道的带通滤波器增益动态地适应输入振幅，以便平均输出脉冲速率保持在定义的范围内。由于这种AGC机制只需要计数和加法运算，因此在未来的设计中可以以较低的硬件成本实现。我们评估了本地AGC算法对输入信号在32 dB输入范围内变化的分类任务的影响。在语音与噪声分类任务中测试了两种接收耳蜗脉冲特征的分类器类型。启用AGC时，逻辑回归分类器的准确度平均提高6%，相对提高40.8%。深度神经网络分类器在AGC情况下表现出类似的改进，与逻辑回归分类器的最佳精度91%相比，其平均精度更高，达到96%。
<details>	<summary>英文摘要</summary>	Including local automatic gain control (AGC) circuitry into a silicon cochlea design has been challenging because of transistor mismatch and model complexity. To address this, we present an alternative system-level algorithm that implements channel-specific AGC in a silicon spiking cochlea by measuring the output spike activity of individual channels. The bandpass filter gain of a channel is adapted dynamically to the input amplitude so that the average output spike rate stays within a defined range. Because this AGC mechanism only needs counting and adding operations, it can be implemented at low hardware cost in a future design. We evaluate the impact of the local AGC algorithm on a classification task where the input signal varies over 32 dB input range. Two classifier types receiving cochlea spike features were tested on a speech versus noise classification task. The logistic regression classifier achieves an average of 6% improvement and 40.8% relative improvement in accuracy when the AGC is enabled. The deep neural network classifier shows a similar improvement for the AGC case and achieves a higher mean accuracy of 96% compared to the best accuracy of 91% from the logistic regression classifier. </details>
<details>	<summary>注释</summary>	Accepted for publication at the IEEE Transactions on Circuits and Systems I - Regular Papers, 2022 DOI: 10.1109/TCSI.2022.3150165 </details>
<details>	<summary>邮件日期</summary>	2022年02月15日</details>

# 347、Motif拓扑和奖励学习改进的脉冲神经网络用于高效的多感官整合
- [ ] Motif-topology and Reward-learning improved Spiking Neural Network for Efficient Multi-sensory Integration 
时间：2022年02月11日                         第一作者：Shuncheng Jia                       [链接](https://arxiv.org/abs/2202.06821).                     
## 摘要：在人工神经网络（ANN）和脉冲神经网络（SNN）中，网络结构和学习原理是形成复杂函数的关键。SNN被认为是新一代人工神经网络，它融合了比ANN更多的生物学特性，包括动态脉冲神经元、功能特定的体系结构和高效的学习范式。在本文中，我们提出了一种母题拓扑和奖励学习改进的SNN（MR-SNN），以实现高效的多感官整合。MR-SNN包含13种类型的3节点基序拓扑，这些基序拓扑首先从独立的单感官学习范式中提取，然后集成到多感官分类中。实验结果表明，与其他不使用基序的传统SNN相比，该MR-SNN具有更高的准确性和更强的鲁棒性。此外，提出的奖赏学习范式在生物学上是合理的，能够更好地解释视觉和听觉感觉信号不一致引起的认知麦格效应。
<details>	<summary>英文摘要</summary>	Network architectures and learning principles are key in forming complex functions in artificial neural networks (ANNs) and spiking neural networks (SNNs). SNNs are considered the new-generation artificial networks by incorporating more biological features than ANNs, including dynamic spiking neurons, functionally specified architectures, and efficient learning paradigms. In this paper, we propose a Motif-topology and Reward-learning improved SNN (MR-SNN) for efficient multi-sensory integration. MR-SNN contains 13 types of 3-node Motif topologies which are first extracted from independent single-sensory learning paradigms and then integrated for multi-sensory classification. The experimental results showed higher accuracy and stronger robustness of the proposed MR-SNN than other conventional SNNs without using Motifs. Furthermore, the proposed reward learning paradigm was biologically plausible and can better explain the cognitive McGurk effect caused by incongruent visual and auditory sensory signals. </details>
<details>	<summary>邮件日期</summary>	2022年02月15日</details>

# 346、基于模拟RRAM的脉冲神经网络中补偿异质性的硬件校准学习
- [ ] Hardware calibrated learning to compensate heterogeneity in analog RRAM-based Spiking Neural Networks 
时间：2022年02月10日                         第一作者：Filippo Moro                       [链接](https://arxiv.org/abs/2202.05094).                     
## 摘要：脉冲神经网络（SNN）可以释放基于模拟电阻随机存取存储器（RRAM）的电路的全部功率，用于低功率信号处理。它们固有的计算稀疏性自然会带来能效效益。实现健壮SNN的主要挑战是模拟CMOS电路和RRAM技术的内在可变性（异质性）。在这项工作中，我们评估了使用130纳米技术节点设计和制造的基于RRAM的神经形态电路的性能和可变性。基于这些结果，我们提出了一种神经形态硬件校准（NHC）SNN，其中学习电路根据测量数据进行校准。我们表明，通过考虑片外学习阶段测量的异质性特征，NHC SNN可以自我校正其硬件非理想性，并学习以高精度解决基准任务。这项工作展示了如何应对神经元和突触的异质性，以提高时间任务中的分类准确性。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) can unleash the full power of analog Resistive Random Access Memories (RRAMs) based circuits for low power signal processing. Their inherent computational sparsity naturally results in energy efficiency benefits. The main challenge implementing robust SNNs is the intrinsic variability (heterogeneity) of both analog CMOS circuits and RRAM technology. In this work, we assessed the performance and variability of RRAM-based neuromorphic circuits that were designed and fabricated using a 130\,nm technology node. Based on these results, we propose a Neuromorphic Hardware Calibrated (NHC) SNN, where the learning circuits are calibrated on the measured data. We show that by taking into account the measured heterogeneity characteristics in the off-chip learning phase, the NHC SNN self-corrects its hardware non-idealities and learns to solve benchmark tasks with high accuracy. This work demonstrates how to cope with the heterogeneity of neurons and synapses for increasing classification accuracy in temporal tasks. </details>
<details>	<summary>注释</summary>	Preprint for ISCAS2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月11日</details>

# 345、通过加权神经元分配实现视觉位置识别的脉冲神经网络
- [ ] Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments 
时间：2022年02月10日                         第一作者：Somayeh Hussaini                       [链接](https://arxiv.org/abs/2109.06452).                     
<details>	<summary>注释</summary>	8 pages, 6 figures, IEEE Robotics and Automation Letters (RA-L), also accepted to IEEE International Conference on Robotics and Automation (ICRA 2022) Journal-ref: IEEE Robotics and Automation Letters 2022 DOI: 10.1109/LRA.2022.3149030 </details>
<details>	<summary>邮件日期</summary>	2022年02月11日</details>

# 344、T-NGA：学习处理脉冲音频传感器事件的时态网络嫁接算法
- [ ] T-NGA: Temporal Network Grafting Algorithm for Learning to Process Spiking Audio Sensor Events 
时间：2022年02月07日                         第一作者：Shu Wang                       [链接](https://arxiv.org/abs/2202.03204).                     
## 摘要：脉冲硅耳蜗传感器将声音编码为来自不同频率通道的异步脉冲流。由于缺少用于刺激耳蜗的标记训练数据集，因此很难根据这些传感器的输出训练深层神经网络。本文提出了一种称为时间网络嫁接算法（T-NGA）的自监督方法，该方法将一个基于谱图特征预训练的递归网络嫁接到耳蜗事件特征上。T-NGA训练只需要暂时对齐的音频频谱图和事件特征。我们的实验表明，嫁接网络的准确性与使用软件脉冲耳蜗模型中的事件从零开始训练语音识别任务的有监督网络的准确性相似。尽管有脉冲硅耳蜗电路的非理想性，但硅耳蜗脉冲记录的嫁接网络准确度仅比使用N-TIDIGITS18数据集的监督网络准确度低5%左右。T-NGA可以训练网络在没有大的标记峰值数据集的情况下处理峰值音频传感器事件。
<details>	<summary>英文摘要</summary>	Spiking silicon cochlea sensors encode sound as an asynchronous stream of spikes from different frequency channels. The lack of labeled training datasets for spiking cochleas makes it difficult to train deep neural networks on the outputs of these sensors. This work proposes a self-supervised method called Temporal Network Grafting Algorithm (T-NGA), which grafts a recurrent network pretrained on spectrogram features so that the network works with the cochlea event features. T-NGA training requires only temporally aligned audio spectrograms and event features. Our experiments show that the accuracy of the grafted network was similar to the accuracy of a supervised network trained from scratch on a speech recognition task using events from a software spiking cochlea model. Despite the circuit non-idealities of the spiking silicon cochlea, the grafted network accuracy on the silicon cochlea spike recordings was only about 5% lower than the supervised network accuracy using the N-TIDIGITS18 dataset. T-NGA can train networks to process spiking audio sensor events in the absence of large labeled spike datasets. </details>
<details>	<summary>注释</summary>	5 pages, 4 figures; accepted at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月08日</details>

# 343、基于时域神经元的高能效高精度脉冲神经网络推理
- [ ] Energy-Efficient High-Accuracy Spiking Neural Network Inference Using Time-Domain Neurons 
时间：2022年02月04日                         第一作者：Joonghyun Song                       [链接](https://arxiv.org/abs/2202.02015).                     
## 摘要：由于在流行的冯·诺依曼体系结构上实现人工神经网络的局限性，最近的研究提出了基于脉冲神经网络（SNN）的神经形态系统，以降低功耗和计算成本。然而，传统的基于电流镜或运算放大器的模拟电压域集成和触发（I&F）神经元电路会带来严重的问题，例如非线性或高功耗，从而降低SNN的推理精度或能量效率。为了同时实现高能量效率和高精度，本文提出了一种低功耗、高线性度的时域I&F神经元电路。在28nm CMOS工艺中设计和模拟，与传统的基于电流镜的神经元相比，该神经元在MNIST推理上的错误率降低了4.3倍以上。此外，所提出的神经元电路的功耗模拟为每个神经元0.230uW，比现有的电压域神经元低几个数量级。
<details>	<summary>英文摘要</summary>	Due to the limitations of realizing artificial neural networks on prevalent von Neumann architectures, recent studies have presented neuromorphic systems based on spiking neural networks (SNNs) to reduce power and computational cost. However, conventional analog voltage-domain integrate-and-fire (I&F) neuron circuits, based on either current mirrors or op-amps, pose serious issues such as nonlinearity or high power consumption, thereby degrading either inference accuracy or energy efficiency of the SNN. To achieve excellent energy efficiency and high accuracy simultaneously, this paper presents a low-power highly linear time-domain I&F neuron circuit. Designed and simulated in a 28nm CMOS process, the proposed neuron leads to more than 4.3x lower error rate on the MNIST inference over the conventional current-mirror-based neurons. In addition, the power consumed by the proposed neuron circuit is simulated to be 0.230uW per neuron, which is orders of magnitude lower than the existing voltage-domain neurons. </details>
<details>	<summary>邮件日期</summary>	2022年02月07日</details>

# 342、低延迟脉冲神经网络的优化电位初始化
- [ ] Optimized Potential Initialization for Low-latency Spiking Neural Networks 
时间：2022年02月03日                         第一作者：Tong Bu                       [链接](https://arxiv.org/abs/2202.01440).                     
## 摘要：脉冲神经网络（SNN）因其低功耗、生物合理性和对抗性鲁棒性等独特特性而受到高度重视。训练深层SNN最有效的方法是通过ANN到SNN的转换，这在深层网络结构和大规模数据集中产生了最好的性能。然而，在准确性和延迟之间需要权衡。为了获得与原始神经网络一样的高精度，需要较长的模拟时间来匹配脉冲神经元的放电频率和模拟神经元的激活值，这阻碍了SNN的实际应用。在本文中，我们的目标是以极低的延迟（少于32个时间步）实现高性能的转换SNN。首先，我们从理论上分析了ANN到SNN的转换，并表明调整阈值确实起到了与权重标准化类似的作用。我们没有引入以模型容量为代价促进ANN到SNN转换的约束，而是采用了一种更直接的方法，通过优化初始膜电位来减少每层的转换损失。此外，我们证明了膜电位的最佳初始化可以实现预期的无误差ANN到SNN的转换。我们在CIFAR-10、CIFAR-100和ImageNet数据集上评估了我们的算法，并使用更少的时间步长实现了最先进的精度。例如，我们在CIFAR-10上以16个时间步长达到了93.38\%的最高精度。此外，我们的方法还可以应用于其他ANN-SNN转换方法，并在时间步长较小时显著提高性能。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have been attached great importance due to the distinctive properties of low power consumption, biological plausibility, and adversarial robustness. The most effective way to train deep SNNs is through ANN-to-SNN conversion, which have yielded the best performance in deep network structure and large-scale datasets. However, there is a trade-off between accuracy and latency. In order to achieve high precision as original ANNs, a long simulation time is needed to match the firing rate of a spiking neuron with the activation value of an analog neuron, which impedes the practical application of SNN. In this paper, we aim to achieve high-performance converted SNNs with extremely low latency (fewer than 32 time-steps). We start by theoretically analyzing ANN-to-SNN conversion and show that scaling the thresholds does play a similar role as weight normalization. Instead of introducing constraints that facilitate ANN-to-SNN conversion at the cost of model capacity, we applied a more direct way by optimizing the initial membrane potential to reduce the conversion loss in each layer. Besides, we demonstrate that optimal initialization of membrane potentials can implement expected error-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10, CIFAR-100 and ImageNet datasets and achieve state-of-the-art accuracy, using fewer time-steps. For example, we reach top-1 accuracy of 93.38\% on CIFAR-10 with 16 time-steps. Moreover, our method can be applied to other ANN-SNN conversion methodologies and remarkably promote performance when the time-steps is small. </details>
<details>	<summary>注释</summary>	Accepted by AAAI 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月04日</details>

# 341、速率编码还是直接编码：哪一种更适合于精确、健壮和节能的脉冲神经网络？
- [ ] Rate Coding or Direct Coding: Which One is Better for Accurate, Robust, and Energy-efficient Spiking Neural Networks? 
时间：2022年01月31日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2202.03133).                     
## 摘要：最近的脉冲神经网络（SNN）工作主要集中在图像分类任务上，因此人们提出了各种编码技术来将图像转换为时间二进制脉冲。其中，速率编码和直接编码被认为是构建实用SNN系统的潜在候选，因为它们在大规模数据集上显示了最先进的性能。尽管使用了这两种编码方案，但人们很少注意以公平的方式比较这两种编码方案。在本文中，我们从三个角度对这两种编码进行了综合分析：准确性、对抗性鲁棒性和能源效率。首先，我们比较了两种编码技术在不同体系结构和数据集下的性能。然后，我们测量了编码技术对两种对抗性攻击方法的鲁棒性。最后，我们在数字硬件平台上比较了两种编码方案的能量效率。我们的结果表明，直接编码可以获得更好的精度，尤其是对于少量的时间步长。相比之下，由于不可微脉冲生成过程，速率编码对对抗性攻击表现出更好的鲁棒性。速率编码也比直接编码产生更高的能量效率，直接编码要求第一层具有多位精度。我们的研究探索了两种编码的特征，这是构建SNN的重要设计考虑因素。该代码可在以下网址获得：https://github.com/Intelligent-Computing-Lab-Yale/Rate-vs-Direct.
<details>	<summary>英文摘要</summary>	Recent Spiking Neural Networks (SNNs) works focus on an image classification task, therefore various coding techniques have been proposed to convert an image into temporal binary spikes. Among them, rate coding and direct coding are regarded as prospective candidates for building a practical SNN system as they show state-of-the-art performance on large-scale datasets. Despite their usage, there is little attention to comparing these two coding schemes in a fair manner. In this paper, we conduct a comprehensive analysis of the two codings from three perspectives: accuracy, adversarial robustness, and energy-efficiency. First, we compare the performance of two coding techniques with various architectures and datasets. Then, we measure the robustness of the coding techniques on two adversarial attack methods. Finally, we compare the energy-efficiency of two coding schemes on a digital hardware platform. Our results show that direct coding can achieve better accuracy especially for a small number of timesteps. In contrast, rate coding shows better robustness to adversarial attacks owing to the non-differentiable spike generation process. Rate coding also yields higher energy-efficiency than direct coding which requires multi-bit precision for the first layer. Our study explores the characteristics of two codings, which is an important design consideration for building SNNs. The code is made available at https://github.com/Intelligent-Computing-Lab-Yale/Rate-vs-Direct. </details>
<details>	<summary>注释</summary>	Accepted to ICASSP2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月08日</details>

# 340、快速精确递归神经网络的脉冲激励秩编码
- [ ] Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks 
时间：2022年01月31日                         第一作者：Alan Jeffares                       [链接](https://arxiv.org/abs/2110.02865).                     
<details>	<summary>注释</summary>	Spotlight paper at ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月01日</details>

# 339、AutoSNN：走向节能的脉冲神经网络
- [ ] AutoSNN: Towards Energy-Efficient Spiking Neural Networks 
时间：2022年01月30日                         第一作者：Byunggook Na                       [链接](https://arxiv.org/abs/2201.12738).                     
## 摘要：模拟大脑中信息传输的脉冲神经网络（SNN）可以通过离散和稀疏的脉冲有效地处理时空信息，因此受到了广泛关注。为了提高SNN的准确性和能效，以前的大多数研究都只关注训练方法，很少研究体系结构的影响。我们调查了之前研究中使用的设计选择，从准确性和脉冲数量方面考虑，发现它们并不最适合SNN。为了进一步提高准确性并减少SNN产生的脉冲，我们提出了一种脉冲感知神经结构搜索框架AutoSNN。我们定义了一个搜索空间，它由没有不良设计选择的架构组成。为了实现峰值感知体系结构搜索，我们引入了一种适应度，它同时考虑了峰值的准确性和数量。AutoSNN成功搜索在准确性和能效方面优于手工SNN的SNN架构。我们充分展示了AutoSNN在包括神经形态数据集在内的各种数据集上的有效性。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) that mimic information transmission in the brain can energy-efficiently process spatio-temporal information through discrete and sparse spikes, thereby receiving considerable attention. To improve accuracy and energy efficiency of SNNs, most previous studies have focused solely on training methods, and the effect of architecture has rarely been studied. We investigate the design choices used in the previous studies in terms of the accuracy and number of spikes and figure out that they are not best-suited for SNNs. To further improve the accuracy and reduce the spikes generated by SNNs, we propose a spike-aware neural architecture search framework called AutoSNN. We define a search space consisting of architectures without undesirable design choices. To enable the spike-aware architecture search, we introduce a fitness that considers both the accuracy and number of spikes. AutoSNN successfully searches for SNN architectures that outperform hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate the effectiveness of AutoSNN on various datasets including neuromorphic datasets. </details>
<details>	<summary>邮件日期</summary>	2022年02月01日</details>

# 338、3D FlowNet：基于事件的3D表示光流估计
- [ ] 3D-FlowNet: Event-based optical flow estimation with 3D representation 
时间：2022年01月28日                         第一作者：Haixin Sun                       [链接](https://arxiv.org/abs/2201.12265).                     
## 摘要：基于事件的摄像头可以跨越基于帧的摄像头的限制，用于重要任务，例如在低照度条件下自动驾驶汽车导航期间的高速运动检测。活动摄像机的高时间分辨率和高动态范围，使它们能够在快速运动和极端光线的情况下工作。然而，传统的计算机视觉方法，如深度神经网络，由于它们是异步和离散的，不能很好地适应处理事件数据。此外，传统的事件数据二维编码表示方法牺牲了时间分辨率。在本文中，我们首先通过将二维编码表示扩展到三维来改进二维编码表示，以更好地保留事件的时间分布。然后，我们提出了3D FlowNet，这是一种新的网络结构，可以根据新的编码方法处理3D输入表示和输出光流估计。采用自监督训练策略来弥补基于事件的摄像机缺乏标记数据集的不足。最后，利用多车辆立体事件摄像机（MVSEC）数据集对所提出的网络进行训练和评估。结果表明，我们的3D FlowNet在训练时间更短的情况下（与Spike FlowNet的100次相比，我们的3D FlowNet的训练时间为30次）优于最先进的训练方法。
<details>	<summary>英文摘要</summary>	Event-based cameras can overpass frame-based cameras limitations for important tasks such as high-speed motion detection during self-driving cars navigation in low illumination conditions. The event cameras' high temporal resolution and high dynamic range, allow them to work in fast motion and extreme light scenarios. However, conventional computer vision methods, such as Deep Neural Networks, are not well adapted to work with event data as they are asynchronous and discrete. Moreover, the traditional 2D-encoding representation methods for event data, sacrifice the time resolution. In this paper, we first improve the 2D-encoding representation by expanding it into three dimensions to better preserve the temporal distribution of the events. We then propose 3D-FlowNet, a novel network architecture that can process the 3D input representation and output optical flow estimations according to the new encoding methods. A self-supervised training strategy is adopted to compensate the lack of labeled datasets for the event-based camera. Finally, the proposed network is trained and evaluated with the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. The results show that our 3D-FlowNet outperforms state-of-the-art approaches with less training epoch (30 compared to 100 of Spike-FlowNet). </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 337、基于神经形态跌倒检测和动作识别数据集的常规视觉模型基准测试
- [ ] Benchmarking Conventional Vision Models on Neuromorphic Fall Detection and Action Recognition Dataset 
时间：2022年01月28日                         第一作者：Karthik Sivarama Krishnan                        [链接](https://arxiv.org/abs/2201.12285).                     
## 摘要：近几年来，基于神经形态视觉的传感器越来越受欢迎，因为它们能够以低功耗感知捕捉时空事件。这些传感器比传统摄像机记录事件或峰值，有助于保护被记录对象的隐私。根据像素亮度变化捕获这些事件，并使用时间、位置和像素强度变化信息对输出数据流进行编码。本文提出并测试了神经形态人类行为识别和跌倒检测数据集上微调的常规视觉模型的性能。来自动态视觉传感摄像机的时空事件流被编码成标准序列图像帧。这些视频帧用于基准测试传统的基于深度学习的体系结构。在这种提出的方法中，我们为这种动态视觉传感（DVS）应用微调了最先进的视觉模型，并将这些模型命名为DVS-R2+1D、DVS-CSN、DVS-C2D、DVS SlowFast、DVS-X3D和DVS MViT。通过比较这些模型的性能，我们发现当前最先进的基于MViT的体系结构DVS MViT优于所有其他模型，精确度为0.958，F-1分数为0.958。第二好的是DVS-C2D，精确度为0.916，F-1分数为0.916。第三名和第四名是DVS-R2+1D和DVS慢速，准确度分别为0.875和0.833，F-1得分分别为0.875和0.861。DVS-CSN和DVS-X3D是表现最差的模型，准确度分别为0.708和0.625，F1得分分别为0.722和0.625。
<details>	<summary>英文摘要</summary>	Neuromorphic vision-based sensors are gaining popularity in recent years with their ability to capture Spatio-temporal events with low power sensing. These sensors record events or spikes over traditional cameras which helps in preserving the privacy of the subject being recorded. These events are captured as per-pixel brightness changes and the output data stream is encoded with time, location, and pixel intensity change information. This paper proposes and benchmarks the performance of fine-tuned conventional vision models on neuromorphic human action recognition and fall detection datasets. The Spatio-temporal event streams from the Dynamic Vision Sensing cameras are encoded into a standard sequence image frames. These video frames are used for benchmarking conventional deep learning-based architectures. In this proposed approach, we fine-tuned the state-of-the-art vision models for this Dynamic Vision Sensing (DVS) application and named these models as DVS-R2+1D, DVS-CSN, DVS-C2D, DVS-SlowFast, DVS-X3D, and DVS-MViT. Upon comparing the performance of these models, we see the current state-of-the-art MViT based architecture DVS-MViT outperforms all the other models with an accuracy of 0.958 and an F-1 score of 0.958. The second best is the DVS-C2D with an accuracy of 0.916 and an F-1 score of 0.916. Third and Fourth are DVS-R2+1D and DVS-SlowFast with an accuracy of 0.875 and 0.833 and F-1 score of 0.875 and 0.861 respectively. DVS-CSN and DVS-X3D were the least performing models with an accuracy of 0.708 and 0.625 and an F1 score of 0.722 and 0.625 respectively. </details>
<details>	<summary>注释</summary>	6 Pages, 2 Figures </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 336、二值化脉冲神经网络中死亡神经元与稀疏性之间的细线
- [ ] The fine line between dead neurons and sparsity in binarized spiking neural networks 
时间：2022年01月28日                         第一作者：Jason K. Eshraghian                       [链接](https://arxiv.org/abs/2201.11915).                     
## 摘要：脉冲神经网络可以通过在时域中编码信息，或通过在更高精度的隐藏状态中处理离散化量来补偿量化误差。理论上，宽动态范围的状态空间可以将多个二值化输入累积在一起，从而提高单个神经元的表征能力。这可以通过增加放电阈值来实现，但如果阈值过高，稀疏的脉冲活动就会变成无脉冲发射。在本文中，我们建议使用“阈值退火”作为触发阈值的预热方法。我们发现，它可以使棘波在多个层面上传播，否则神经元将停止放电，这样做，尽管使用了二值化权重，但在四个不同的数据集上仍能获得极具竞争力的结果。源代码可在https://github.com/jeshraghian/snn-tha/
<details>	<summary>英文摘要</summary>	Spiking neural networks can compensate for quantization error by encoding information either in the temporal domain, or by processing discretized quantities in hidden states of higher precision. In theory, a wide dynamic range state-space enables multiple binarized inputs to be accumulated together, thus improving the representational capacity of individual neurons. This may be achieved by increasing the firing threshold, but make it too high and sparse spike activity turns into no spike emission. In this paper, we propose the use of `threshold annealing' as a warm-up method for firing thresholds. We show it enables the propagation of spikes across multiple layers where neurons would otherwise cease to fire, and in doing so, achieve highly competitive results on four diverse datasets, despite using binarized weights. Source code is available at https://github.com/jeshraghian/snn-tha/ </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 335、具有替代梯度下降的元学习脉冲神经网络
- [ ] Meta-learning Spiking Neural Networks with Surrogate Gradient Descent 
时间：2022年01月26日                         第一作者：Kenneth Stewart                       [链接](https://arxiv.org/abs/2201.10777).                     
## 摘要：在边缘和在线任务执行期间进行适应性“终身”学习是人工智能研究的理想目标。在这方面，实现脉冲神经网络（SNN）的神经形态硬件特别有吸引力，因为它们的实时、基于事件的局部计算范式使它们适合边缘实现和快速学习。然而，作为最先进SNN训练特征的长时间迭代学习与神经形态硬件的物理性质和实时操作不兼容。为了克服这些局限性，在深度学习中越来越多地使用元学习等双层学习。在这项工作中，我们使用替代梯度方法在SNN中演示了基于梯度的元学习，该方法近似于梯度估计的脉冲阈值函数。由于替代梯度可以二次可微，因此可以使用模型不可知元学习（MAML）等有效的二阶梯度元学习方法。我们表明，在基于事件的元数据集上，使用MAML的SNN元训练与使用MAML的传统ANN元训练的性能相匹配或超过。此外，我们还展示了元学习带来的特殊优势：快速学习，无需高精度权重或梯度。我们的研究结果强调了元学习技术如何成为在现实世界问题上部署神经形态学习技术的工具。
<details>	<summary>英文摘要</summary>	Adaptive "life-long" learning at the edge and during online task performance is an aspirational goal of AI research. Neuromorphic hardware implementing Spiking Neural Networks (SNNs) are particularly attractive in this regard, as their real-time, event-based, local computing paradigm makes them suitable for edge implementations and fast learning. However, the long and iterative learning that characterizes state-of-the-art SNN training is incompatible with the physical nature and real-time operation of neuromorphic hardware. Bi-level learning, such as meta-learning is increasingly used in deep learning to overcome these limitations. In this work, we demonstrate gradient-based meta-learning in SNNs using the surrogate gradient method that approximates the spiking threshold function for gradient estimations. Because surrogate gradients can be made twice differentiable, well-established, and effective second-order gradient meta-learning methods such as Model Agnostic Meta Learning (MAML) can be used. We show that SNNs meta-trained using MAML match or exceed the performance of conventional ANNs meta-trained with MAML on event-based meta-datasets. Furthermore, we demonstrate the specific advantages that accrue from meta-learning: fast learning without the requirement of high precision weights or gradients. Our results emphasize how meta-learning techniques can become instrumental for deploying neuromorphic learning technologies on real-world problems. </details>
<details>	<summary>注释</summary>	Submitted to IOP Neuromorphic Computing and Engineering for peer review </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 334、BrainScaleS-2加速的混合可塑性神经形态系统
- [ ] The BrainScaleS-2 accelerated neuromorphic system with hybrid plasticity 
时间：2022年01月26日                         第一作者：Christian Pehle                       [链接](https://arxiv.org/abs/2201.11063).                     
## 摘要：自从电子元件开始进行信息处理以来，神经系统就一直是计算原语组织的隐喻。当今的脑启发计算包括一类方法，从使用新型纳米设备进行计算到研究大规模神经形态架构，如TrueNorth、SpiNNaker、BrainScaleS、Tianjic和Loihi。虽然实现细节有所不同，但脉冲神经网络（有时被称为第三代神经网络）是用于对此类系统的计算建模的常见抽象。在这里，我们描述了BrainScaleS神经形态架构的第二代，强调了该架构支持的应用。它结合了一个定制的模拟加速器核心，支持仿生脉冲神经网络原语的加速物理仿真，以及一个紧密耦合的数字处理器和一个数字事件路由网络。
<details>	<summary>英文摘要</summary>	Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks -- sometimes referred to as the third generation of neural networks -- are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network. </details>
<details>	<summary>注释</summary>	22 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 333、S$^2$NN：用于训练节能单步神经网络的脉冲替代梯度的时间步长缩减
- [ ] S$^2$NN: Time Step Reduction of Spiking Surrogate Gradients for Training Energy Efficient Single-Step Neural Networks 
时间：2022年01月26日                         第一作者：Kazuma Suetake                       [链接](https://arxiv.org/abs/2201.10879).                     
## 摘要：随着神经网络规模的增加，需要能够使其以较低的计算成本和能源效率运行的技术。从这些需求出发，人们提出了各种有效的神经网络模式，如脉冲神经网络（SNN）或二元神经网络（BNN）。然而，它们有一些棘手的缺点，比如推理精度和延迟降低。为了解决这些问题，我们提出了一种单步神经网络（S$^2$NN），这是一种计算成本低、精度高的节能神经网络。建议的S$^2$NN将隐藏层之间的信息通过脉冲处理为SNN。然而，它没有时间维度，因此在训练和推理阶段没有延迟。因此，与需要时间序列处理的SNN相比，建议的S$^2$NN具有更低的计算成本。然而S$^2$NN不能采用na“{i}由于脉冲的不可微性，我们采用了反向传播算法。我们通过将多时间步长SNN的替代梯度减少到单个时间步长，推导出了一个合适的神经元模型。我们通过实验证明，与现有的SNN和BNN神经元模型相比，获得的神经元模型使S$^2$NN能够更准确、更高效地进行训练。我们还表明，建议的S$^2$NN可以实现与全精度网络相当的精度，同时具有高能效。
<details>	<summary>英文摘要</summary>	As the scales of neural networks increase, techniques that enable them to run with low computational cost and energy efficiency are required. From such demands, various efficient neural network paradigms, such as spiking neural networks (SNNs) or binary neural networks (BNNs), have been proposed. However, they have sticky drawbacks, such as degraded inference accuracy and latency. To solve these problems, we propose a single-step neural network (S$^2$NN), an energy-efficient neural network with low computational cost and high precision. The proposed S$^2$NN processes the information between hidden layers by spikes as SNNs. Nevertheless, it has no temporal dimension so that there is no latency within training and inference phases as BNNs. Thus, the proposed S$^2$NN has a lower computational cost than SNNs that require time-series processing. However, S$^2$NN cannot adopt na\"{i}ve backpropagation algorithms due to the non-differentiability nature of spikes. We deduce a suitable neuron model by reducing the surrogate gradient for multi-time step SNNs to a single-time step. We experimentally demonstrated that the obtained neuron model enables S$^2$NN to train more accurately and energy-efficiently than existing neuron models for SNNs and BNNs. We also showed that the proposed S$^2$NN could achieve comparable accuracy to full-precision networks while being highly energy-efficient. </details>
<details>	<summary>注释</summary>	19 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 332、基于事件的电位辅助脉冲神经网络视频重建
- [ ] Event-based Video Reconstruction via Potential-assisted Spiking Neural Network 
时间：2022年01月25日                         第一作者：Lin Zhu                       [链接](https://arxiv.org/abs/2201.10943).                     
## 摘要：神经形态视觉传感器是一种新的仿生成像模式，它报告称为“事件”的异步、连续每像素亮度变化，具有高时间分辨率和高动态范围。到目前为止，基于事件的图像重建方法都是基于人工神经网络（ANN）或手工制作的时空平滑技术。在本文中，我们首先通过全脉冲神经网络（SNN）结构实现图像重建工作。作为仿生神经网络，SNN在异步二进制峰值随时间分布的情况下运行，可能会在事件驱动硬件上带来更高的计算效率。我们提出了一种基于事件的视频重建框架，该框架基于一个完全脉冲神经网络（EVSNN），该网络利用了漏积分与激发（LIF）神经元和膜电位（MP）神经元。我们发现，脉冲神经元有潜力存储有用的时间信息（记忆），以完成这种时间相关的任务。此外，为了更好地利用时间信息，我们提出了一种混合电位辅助框架（PA-EVSNN），该框架利用了脉冲神经元的膜电位。该神经元称为自适应膜电位（AMP）神经元，它根据输入峰值自适应地更新膜电位。实验结果表明，我们的模型在IJRR、MVSEC和HQF数据集上的性能与基于ANN的模型相当。EVSNN和PA-EVSNN的能耗分别比其ANN结构的计算效率高19.36$\倍和7.75$\倍。
<details>	<summary>英文摘要</summary>	Neuromorphic vision sensor is a new bio-inspired imaging paradigm that reports asynchronous, continuously per-pixel brightness changes called `events' with high temporal resolution and high dynamic range. So far, the event-based image reconstruction methods are based on artificial neural networks (ANN) or hand-crafted spatiotemporal smoothing techniques. In this paper, we first implement the image reconstruction work via fully spiking neural network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. We propose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Membrane Potential (MP) neuron. We find that the spiking neurons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Furthermore, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AMP) neuron, which adaptively updates the membrane potential according to the input spikes. The experimental results demonstrate that our models achieve comparable performance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are 19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN architectures, respectively. </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 331、神经体系结构寻找脉冲神经网络
- [ ] Neural Architecture Search for Spiking Neural Networks 
时间：2022年01月23日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2201.10355).                     
## 摘要：脉冲神经网络（SNN）由于其固有的高稀疏性激活，作为传统人工神经网络（ANN）的一种潜在的节能替代方案，受到了广泛关注。然而，大多数以前的SNN方法使用类似ANN的体系结构（例如VGG网络或ResNet），这可能为SNN中二进制信息的时序处理提供次优性能。为了解决这个问题，在本文中，我们介绍了一种新的神经结构搜索（NAS）方法来寻找更好的SNN结构。受最近从初始化时的激活模式中找到最佳体系结构的NAS方法的启发，我们选择的体系结构可以在不经过训练的情况下跨不同的数据样本表示不同的脉冲激活模式。此外，为了利用峰值之间的时间相关性，我们在层之间搜索前馈连接和反向连接（即时间反馈连接）。有趣的是，通过我们的搜索算法发现的SNASNet在向后连接的情况下实现了更高的性能，这表明了设计SNN体系结构以适当使用时态信息的重要性。我们在三个图像识别基准上进行了大量实验，结果表明，SNASNet以显著更低的时间步长（5个时间步长）实现了最先进的性能。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. However, most prior SNN methods use ANN-like architectures (e.g., VGG-Net or ResNet), which could provide sub-optimal performance for temporal sequence processing of binary information in SNNs. To address this, in this paper, we introduce a novel Neural Architecture Search (NAS) approach for finding better SNN architectures. Inspired by recent NAS approaches that find the optimal architecture from activation patterns at initialization, we select the architecture that can represent diverse spike activation patterns across different data samples without training. Furthermore, to leverage the temporal correlation among the spikes, we search for feed forward connections as well as backward connections (i.e., temporal feedback connections) between layers. Interestingly, SNASNet found by our search algorithm achieves higher performance with backward connections, demonstrating the importance of designing SNN architecture for suitably using temporal information. We conduct extensive experiments on three image recognition benchmarks where we show that SNASNet achieves state-of-the-art performance with significantly lower timesteps (5 timesteps). </details>
<details>	<summary>邮件日期</summary>	2022年01月26日</details>

# 330、使用普通设备的摄像头和机器视觉速度提高1000倍
- [ ] 1000x Faster Camera and Machine Vision with Ordinary Devices 
时间：2022年01月23日                         第一作者：Tiejun Huang                       [链接](https://arxiv.org/abs/2201.09302).                     
## 摘要：在数码相机中，我们发现了一个主要的局限性：从胶片相机继承的图像和视频形式阻碍了它捕捉快速变化的光子世界。在这里，我们介绍了vidar，一种位序列阵列，其中每个位表示光子的累积是否已达到阈值，以记录和重建任何时刻的场景辐射。通过仅使用消费级CMOS传感器和集成电路，我们开发了一款比传统相机快1000倍的vidar相机。通过将vidar视为生物视觉中的脉冲序列，我们进一步开发了一个基于脉冲神经网络的机器视觉系统，该系统将机器的速度与生物视觉的机制结合起来，实现了比人类视觉快1000倍的高速目标检测和跟踪。我们展示了vidar摄像机和super vision系统在辅助裁判和目标指向系统中的应用。我们的研究有望从根本上彻底改变图像和视频概念及相关行业，包括摄影、电影和视觉媒体，并开启一个新的神经网络驱动的无速度机器视觉时代。
<details>	<summary>英文摘要</summary>	In digital cameras, we find a major limitation: the image and video form inherited from a film camera obstructs it from capturing the rapidly changing photonic world. Here, we present vidar, a bit sequence array where each bit represents whether the accumulation of photons has reached a threshold, to record and reconstruct the scene radiance at any moment. By employing only consumer-level CMOS sensors and integrated circuits, we have developed a vidar camera that is 1,000x faster than conventional cameras. By treating vidar as spike trains in biological vision, we have further developed a spiking neural network-based machine vision system that combines the speed of the machine and the mechanism of biological vision, achieving high-speed object detection and tracking 1,000x faster than human vision. We demonstrate the utility of the vidar camera and the super vision system in an assistant referee and target pointing system. Our study is expected to fundamentally revolutionize the image and video concepts and related industries, including photography, movies, and visual media, and to unseal a new spiking neural network-enabled speed-free machine vision era. </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 329、深度强化学习与脉冲Q学习
- [ ] Deep Reinforcement Learning with Spiking Q-learning 
时间：2022年01月21日                         第一作者：Ding Chen                       [链接](https://arxiv.org/abs/2201.09754).                     
## 摘要：在特殊的神经形态硬件的帮助下，脉冲神经网络（SNN）有望以更低的能耗实现人工智能。将SNN和深度强化学习（RL）相结合，为实际控制任务提供了一种有前途的节能方法。目前，基于SNN的RL方法很少。大多数算法要么缺乏泛化能力，要么在训练中使用人工神经网络（ANN）来估计值函数。前者需要为每个场景调整大量的超参数，而后者限制了不同类型的RL算法的应用，忽略了训练中的巨大能量消耗。为了开发一种鲁棒的基于脉冲的RL方法，我们从昆虫中发现的非脉冲中间神经元中汲取灵感，提出了深脉冲Q网络（DSQN），使用非脉冲神经元的膜电压作为Q值的表示，它可以直接使用端到端RL从高维感觉输入学习鲁棒策略。在17个Atari游戏上进行的实验表明，DSQN在大多数游戏中都优于基于人工神经网络的深度Q网络（DQN）。此外，实验结果表明，DSQN具有良好的学习稳定性和对抗性攻击的鲁棒性。
<details>	<summary>英文摘要</summary>	With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combing SNNs and deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can directly learn robust policies from high-dimensional sensory inputs using end-to-end RL. Experiments conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks of DSQN. </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 328、神经形态混合脉冲运动检测器
- [ ] NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector 
时间：2022年01月19日                         第一作者：Pedro Machado                        [链接](https://arxiv.org/abs/2112.06102).                     
<details>	<summary>邮件日期</summary>	2022年01月21日</details>

# 327、POPPINS：一种基于群体的数字脉冲神经形态处理器，具有整数二次积分和激发神经元
- [ ] POPPINS : A Population-Based Digital Spiking Neuromorphic Processor with Integer Quadratic Integrate-and-Fire Neurons 
时间：2022年01月19日                         第一作者：Zuo-Wei Yeh                       [链接](https://arxiv.org/abs/2201.07490).                     
## 摘要：人脑作为生物处理系统的内部运作在很大程度上仍然是个谜。受人脑功能的启发，并基于对果蝇等其他物种的简单神经网络系统的分析，神经形态计算系统吸引了相当大的兴趣。在细胞水平的连接组学研究中，我们可以识别生物神经网络（称为群体）的特征，这些特征不仅构成网络中的循环完整连接，还构成每个神经元中的外部刺激和自我连接。基于网络中脉冲传输和输入数据的低数据带宽，脉冲神经网络具有低延迟和低功耗的设计。在这项研究中，我们提出了一种可配置的基于群体的数字脉冲神经形态处理器，采用180nm处理技术，具有两个可配置的层次群体。此外，处理器中的这些神经元可以配置为新模型、整数二次积分和fire神经元模型，其中包含一个无符号的8位膜电位值。该处理器可以实时执行智能决策以避免事故。此外，该方法还可以开发仿生神经形态系统和各种低功耗、低延迟的推理处理应用。
<details>	<summary>英文摘要</summary>	The inner operations of the human brain as a biological processing system remain largely a mystery. Inspired by the function of the human brain and based on the analysis of simple neural network systems in other species, such as Drosophila, neuromorphic computing systems have attracted considerable interest. In cellular-level connectomics research, we can identify the characteristics of biological neural network, called population, which constitute not only recurrent fullyconnection in network, also an external-stimulus and selfconnection in each neuron. Relying on low data bandwidth of spike transmission in network and input data, Spiking Neural Networks exhibit low-latency and low-power design. In this study, we proposed a configurable population-based digital spiking neuromorphic processor in 180nm process technology with two configurable hierarchy populations. Also, these neurons in the processor can be configured as novel models, integer quadratic integrate-and-fire neuron models, which contain an unsigned 8-bit membrane potential value. The processor can implement intelligent decision making for avoidance in real-time. Moreover, the proposed approach enables the developments of biomimetic neuromorphic system and various low-power, and low-latency inference processing applications. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 326、时态计算机组织
- [ ] Temporal Computer Organization 
时间：2022年01月19日                         第一作者：James E. Smith                       [链接](https://arxiv.org/abs/2201.07742).                     
## 摘要：本文档重点介绍在使用时间瞬变进行通信和计算的技术中实现的计算系统。虽然用一般术语描述，但脉冲神经网络的实现是主要的兴趣。作为背景，本文总结了一个构造时态网络的代数。然后，描述了由同步段组成的系统组织。这些段在内部进行前馈，并在段之间进行反馈。同步时钟在每个计算步骤或周期结束时重置网络段。在其基本形式中，同步时钟仅执行重置功能。在神经网络的背景下，这满足了生物学的合理性。然而，功能完整性受到限制。通过允许将同步时钟用作作为时间参考值的附加功能输入，消除了该限制。
<details>	<summary>英文摘要</summary>	This document is focused on computing systems implemented in technologies that communicate and compute with temporal transients. Although described in general terms, implementations of spiking neural networks are of primary interest. As background, an algebra for constructing temporal networks is summarized. Then, a system organization consisting of synchronized segments is described. The segments are feedforward internally with feedback between segments. A synchronizing clock resets network segments at the end of each computation step or cycle. In its basic form, the synchronizing clock merely performs a reset function. In the context of neural networks, this satisfies biological plausibility. However, functional completeness is restricted. This restriction is removed by allowing use of the synchronizing clock as an additional function input that acts as a temporal reference value. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 325、脉冲神经网络的FPGA优化硬件加速
- [ ] FPGA-optimized Hardware acceleration for Spiking Neural Networks 
时间：2022年01月18日                         第一作者：Alessio Carpegna                       [链接](https://arxiv.org/abs/2201.06993).                     
## 摘要：人工智能（AI）在许多不同的任务中获得了成功和重要性。人工智能系统的日益普及和复杂性促使研究人员开发专用硬件加速器。脉冲神经网络（SNN）在这个意义上代表了一个有前途的解决方案，因为它们实现的模型更适合可靠的硬件设计。此外，从神经科学的角度来看，它们更好地模仿人脑。这项工作提出了一种用于SNN的硬件加速器的开发，该加速器带有离线训练，应用于图像识别任务，使用MNIST作为目标数据集。许多技术被用于最小化面积和最大化性能，例如用简单的位移位替换乘法运算，以及最小化花费在非活动脉冲上的时间，这对于更新神经元的内部状态是无用的。该设计以Xilinx Artix-7 FPGA为目标，总共使用了约40%的可用硬件资源，并将分类时间减少了三个数量级，与全精度软件相比，对精度的影响较小，为4.5%。
<details>	<summary>英文摘要</summary>	Artificial intelligence (AI) is gaining success and importance in many different tasks. The growing pervasiveness and complexity of AI systems push researchers towards developing dedicated hardware accelerators. Spiking Neural Networks (SNN) represent a promising solution in this sense since they implement models that are more suitable for a reliable hardware design. Moreover, from a neuroscience perspective, they better emulate a human brain. This work presents the development of a hardware accelerator for an SNN, with off-line training, applied to an image recognition task, using the MNIST as the target dataset. Many techniques are used to minimize the area and to maximize the performance, such as the replacement of the multiplication operation with simple bit shifts and the minimization of the time spent on inactive spikes, useless for the update of neurons' internal state. The design targets a Xilinx Artix-7 FPGA, using in total around the 40% of the available hardware resources and reducing the classification time by three orders of magnitude, with a small 4.5% impact on the accuracy, if compared to its software, full precision counterpart. </details>
<details>	<summary>注释</summary>	6 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 324、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2022年01月14日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 323、将STDP引入多层递归脉冲神经网络
- [ ] Including STDP to eligibility propagation in multi-layer recurrent spiking neural networks 
时间：2022年01月05日                         第一作者：Werner van der Veen                       [链接](https://arxiv.org/abs/2201.07602).                     
## 摘要：与基于深度学习的方法相比，神经形态系统中的脉冲神经网络（Spiking neural networks，SNN）具有更高的能量效率，但目前还没有明确的竞争学习算法来训练此类SNN。资格传播（e-prop）提供了一种在低功耗神经形态硬件中训练有竞争力的复发性SNN的有效且生物学上合理的方法。在本报告中，再现了e-prop在语音分类任务中的先前表现，并分析了包含STDP样行为的影响。在ALIF神经元模型中加入STDP可以提高分类性能，但Izhikevich e-prop神经元的情况并非如此。最后，我们发现在单层循环SNN中实现的e-prop始终优于多层变体。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) in neuromorphic systems are more energy efficient compared to deep learning-based methods, but there is no clear competitive learning algorithm for training such SNNs. Eligibility propagation (e-prop) offers an efficient and biologically plausible way to train competitive recurrent SNNs in low-power neuromorphic hardware. In this report, previous performance of e-prop on a speech classification task is reproduced, and the effects of including STDP-like behavior are analyzed. Including STDP to the ALIF neuron model improves the classification performance, but this is not the case for the Izhikevich e-prop neuron. Finally, it was found that e-prop implemented in a single-layer recurrent SNN consistently outperforms a multi-layer variant. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 322、具有时间截断局部反向传播的脉冲神经网络的有效训练
- [ ] Efficient Training of Spiking Neural Networks with Temporally-Truncated Local Backpropagation through Time 
时间：2021年12月13日                         第一作者：Wenzhe Guo                       [链接](https://arxiv.org/abs/2201.07210).                     
## 摘要：由于复杂的神经动力学和触发函数的内在不可微性，直接训练脉冲神经网络（SNN）仍然具有挑战性。为训练SNN而提出的著名的时间反向传播（backpropagation through time，BPTT）算法存在较大的内存占用，并且禁止反向和更新解锁，因此无法利用局部监督训练方法的潜力。本文提出了一种高效、直接的SNN训练算法，该算法将局部监督训练方法与时间截断的BPTT算法相结合。该算法探索了BPTT中的时间和空间局部性，有助于显著降低计算成本，包括GPU内存利用率、主存访问和算术运算。我们深入探讨了与时间截断长度和局部训练块大小有关的设计空间，并测试了它们对运行不同类型任务的不同网络的分类精度的影响。结果表明，时间截断对基于帧的数据集的分类精度有负面影响，但会提高动态视觉传感器（DVS）记录数据集的分类精度。尽管会导致信息丢失，但本地培训能够缓解过度拟合。时间截断和局部训练的联合作用可以减缓精度下降，甚至提高精度。此外，与标准端到端BPTT相比，对AlexNet分类CIFAR10-DVS数据集等深层SNS模型进行训练，可使精确度提高7.26%，GPU内存减少89.94%，内存访问减少10.79%，MAC操作减少99.64%。
<details>	<summary>英文摘要</summary>	Directly training spiking neural networks (SNNs) has remained challenging due to complex neural dynamics and intrinsic non-differentiability in firing functions. The well-known backpropagation through time (BPTT) algorithm proposed to train SNNs suffers from large memory footprint and prohibits backward and update unlocking, making it impossible to exploit the potential of locally-supervised training methods. This work proposes an efficient and direct training algorithm for SNNs that integrates a locally-supervised training method with a temporally-truncated BPTT algorithm. The proposed algorithm explores both temporal and spatial locality in BPTT and contributes to significant reduction in computational cost including GPU memory utilization, main memory access and arithmetic operations. We thoroughly explore the design space concerning temporal truncation length and local training block size and benchmark their impact on classification accuracy of different networks running different types of tasks. The results reveal that temporal truncation has a negative effect on the accuracy of classifying frame-based datasets, but leads to improvement in accuracy on dynamic-vision-sensor (DVS) recorded datasets. In spite of resulting information loss, local training is capable of alleviating overfitting. The combined effect of temporal truncation and local training can lead to the slowdown of accuracy drop and even improvement in accuracy. In addition, training deep SNNs models such as AlexNet classifying CIFAR10-DVS dataset leads to 7.26% increase in accuracy, 89.94% reduction in GPU memory, 10.79% reduction in memory access, and 99.64% reduction in MAC operations compared to the standard end-to-end BPTT. </details>
<details>	<summary>注释</summary>	16 </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 321、通过直接训练的深度脉冲Q网络实现人的水平控制
- [ ] Human-Level Control through Directly-Trained Deep Spiking Q-Networks 
时间：2021年12月13日                         第一作者：Guisong Liu                       [链接](https://arxiv.org/abs/2201.07211).                     
## 摘要：作为第三代神经网络，脉冲神经网络（Spiking neural networks，SNNs）由于具有较高的能量效率，在神经形态硬件方面有着巨大的潜力。然而，由于二进制输出和脉冲函数的不可微性，深度脉冲强化学习（DSRL）即基于SNN的强化学习（RL）仍处于初级阶段。为了解决这些问题，本文提出了一种深度脉冲Q网络（DSQN）。具体来说，我们提出了一种基于泄漏集成与激发（LIF）神经元和深度Q网络（DQN）的直接训练深度脉冲强化学习体系结构。然后，我们对深度脉冲Q网络采用了直接脉冲学习算法。我们进一步从理论上证明了在DSQN中使用LIF神经元的优势。在17款表现最好的Atari游戏上进行了综合实验，将我们的方法与最先进的转换方法进行比较。实验结果证明了该方法在性能、稳定性、鲁棒性和能量效率方面的优越性。据我们所知，我们的工作是第一个通过直接训练的SNN在多个Atari游戏中实现最先进性能的工作。
<details>	<summary>英文摘要</summary>	As the third-generation neural networks, Spiking Neural Networks (SNNs) have great potential on neuromorphic hardware because of their high energy-efficiency. However, Deep Spiking Reinforcement Learning (DSRL), i.e., the Reinforcement Learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the non-differentiable property of the spiking function. To address these issues, we propose a Deep Spiking Q-Network (DSQN) in this paper. Specifically, we propose a directly-trained deep spiking reinforcement learning architecture based on the Leaky Integrate-and-Fire (LIF) neurons and Deep Q-Network (DQN). Then, we adapt a direct spiking learning algorithm for the Deep Spiking Q-Network. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiority of our method in terms of performance, stability, robustness and energy-efficiency. To the best of our knowledge, our work is the first one to achieve state-of-the-art performance on multiple Atari games with the directly-trained SNN. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 320、通过解决脉冲神经网络中的退化问题来推进深度剩余学习
- [ ] Advancing Deep Residual Learning by Solving the Crux of Degradation in Spiking Neural Networks 
时间：2021年12月09日                         第一作者：Yifan Hu                       [链接](https://arxiv.org/abs/2201.07209).                     
## 摘要：尽管神经形态计算的发展很快，但脉冲神经网络（SNN）的深度不足和表现力不足严重限制了其实际应用范围。剩余学习和捷径已被证明是训练深层神经网络的一种重要方法，但之前的工作很少评估它们对基于棘波的通信和时空动力学特征的适用性。这种疏忽导致信息流动受阻，并伴随着降级问题。在本文中，我们确定了关键点，然后提出了一种新的SNN剩余块，它能够显著扩展直接训练的SNN的深度，例如，在CIFAR-10上多达482层，在ImageNet上多达104层，而不会观察到任何轻微的退化问题。我们在基于帧和神经形态的数据集上验证了我们的方法的有效性，我们的SRM-ResNet104在ImageNet上获得了76.02%的优越结果，这是在直接训练的SNN领域中的首次。估计了巨大的能量效率，由此产生的网络平均每个神经元只需要一个脉冲来对输入样本进行分类。我们相信，我们强大且可扩展的建模将为SNN的进一步探索提供强有力的支持。
<details>	<summary>英文摘要</summary>	Despite the rapid progress of neuromorphic computing, the inadequate depth and the resulting insufficient representation power of spiking neural networks (SNNs) severely restrict their application scope in practice. Residual learning and shortcuts have been evidenced as an important approach for training deep neural networks, but rarely did previous work assess their applicability to the characteristics of spike-based communication and spatiotemporal dynamics. This negligence leads to impeded information flow and the accompanying degradation problem. In this paper, we identify the crux and then propose a novel residual block for SNNs, which is able to significantly extend the depth of directly trained SNNs, e.g., up to 482 layers on CIFAR-10 and 104 layers on ImageNet, without observing any slight degradation problem. We validate the effectiveness of our methods on both frame-based and neuromorphic datasets, and our SRM-ResNet104 achieves a superior result of 76.02% accuracy on ImageNet, the first time in the domain of directly trained SNNs. The great energy efficiency is estimated and the resulting networks need on average only one spike per neuron for classifying an input sample. We believe our powerful and scalable modeling will provide a strong support for further exploration of SNNs. </details>
<details>	<summary>注释</summary>	arXiv admin note: substantial text overlap with arXiv:2112.08954 </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 319、稀疏脉冲梯度下降
- [ ] Sparse Spiking Gradient Descent 
时间：2022年01月13日                         第一作者：Nicolas Perez-Nieves                        [链接](https://arxiv.org/abs/2105.08810).                     
<details>	<summary>邮件日期</summary>	2022年01月14日</details>

# 318、利用基于时间的神经元提高脉冲神经网络的精度
- [ ] Improving Spiking Neural Network Accuracy Using Time-based Neurons 
时间：2022年01月05日                         第一作者：Hanseok Kim                       [链接](https://arxiv.org/abs/2201.01394).                     
## 摘要：由于von Neumann体系结构下运行深度学习模型在降低功耗方面的基本限制，基于模拟神经元的低功耗脉冲神经网络的神经形态计算系统的研究备受关注。为了集成大量神经元，神经元需要设计成占据一个小的区域，但随着技术规模的缩小，模拟神经元很难扩展，并且它们的电压余量/动态范围和电路非线性都会降低。有鉴于此，本文首先对现有基于电流镜的电压域神经元在28nm工艺中的非线性行为进行了建模，并表明神经元的非线性效应会严重降低SNN推理精度。然后，为了缓解这个问题，我们提出了一种新的神经元，它在时域处理传入的脉冲，并大大提高了线性度，从而与现有的电压域神经元相比提高了推理精度。在MNIST数据集上测试，该神经元的推理错误率与理想神经元的推理错误率相差不到0.1%。
<details>	<summary>英文摘要</summary>	Due to the fundamental limit to reducing power consumption of running deep learning models on von-Neumann architecture, research on neuromorphic computing systems based on low-power spiking neural networks using analog neurons is in the spotlight. In order to integrate a large number of neurons, neurons need to be designed to occupy a small area, but as technology scales down, analog neurons are difficult to scale, and they suffer from reduced voltage headroom/dynamic range and circuit nonlinearities. In light of this, this paper first models the nonlinear behavior of existing current-mirror-based voltage-domain neurons designed in a 28nm process, and show SNN inference accuracy can be severely degraded by the effect of neuron's nonlinearity. Then, to mitigate this problem, we propose a novel neuron, which processes incoming spikes in the time domain and greatly improves the linearity, thereby improving the inference accuracy compared to the existing voltage-domain neuron. Tested on the MNIST dataset, the inference error rate of the proposed neuron differs by less than 0.1% from that of the ideal neuron. </details>
<details>	<summary>邮件日期</summary>	2022年01月06日</details>

# 317、一种基于感受野的鲁棒视觉采样模型
- [ ] A Robust Visual Sampling Model Inspired by Receptive Field 
时间：2022年01月04日                         第一作者：Liwen Hu                       [链接](https://arxiv.org/abs/2201.01030).                     
## 摘要：模拟视网膜中央凹的脉冲相机可以通过发射脉冲来报告每像素亮度强度的累积。作为一种具有高时间分辨率的仿生视觉传感器，它在计算机视觉领域有着巨大的潜力。然而，现有的Spike相机的采样模型容易受到量化和噪声的影响，无法有效地捕捉物体的纹理细节。在这项工作中，受感受野（RVSM）的启发，提出了一种鲁棒的视觉采样模型，该模型使用高斯差分（DoG）和高斯滤波器生成的小波滤波器来模拟感受野。利用类似于小波逆变换的方法，可以将RVSM的峰值数据转换成图像。为了测试性能，我们还提出了一个包含各种运动场景的高速运动峰值数据集（HMD）。通过比较HMD中的重建图像，我们发现RVSM可以大大提高脉冲相机的信息捕获能力。更重要的是，RVSM模仿感受野机制采集区域信息，能够有效滤除高强度噪声，大大改善了脉冲相机对噪声敏感的问题。此外，由于采样结构的强泛化性，RVSM也适用于其他神经形态视觉传感器。以上实验是在一个Spike摄像机模拟器上完成的。
<details>	<summary>英文摘要</summary>	Spike camera mimicking the retina fovea can report per-pixel luminance intensity accumulation by firing spikes. As a bio-inspired vision sensor with high temporal resolution, it has a huge potential for computer vision. However, the sampling model in current Spike camera is so susceptible to quantization and noise that it cannot capture the texture details of objects effectively. In this work, a robust visual sampling model inspired by receptive field (RVSM) is proposed where wavelet filter generated by difference of Gaussian (DoG) and Gaussian filter are used to simulate receptive field. Using corresponding method similar to inverse wavelet transform, spike data from RVSM can be converted into images. To test the performance, we also propose a high-speed motion spike dataset (HMD) including a variety of motion scenes. By comparing reconstructed images in HMD, we find RVSM can improve the ability of capturing information of Spike camera greatly. More importantly, due to mimicking receptive field mechanism to collect regional information, RVSM can filter high intensity noise effectively and improves the problem that Spike camera is sensitive to noise largely. Besides, due to the strong generalization of sampling structure, RVSM is also suitable for other neuromorphic vision sensor. Above experiments are finished in a Spike camera simulator. </details>
<details>	<summary>邮件日期</summary>	2022年01月05日</details>

# 316、通过正则化和归一化改进脉冲神经网络中的代理梯度学习
- [ ] Improving Surrogate Gradient Learning in Spiking Neural Networks via Regularization and Normalization 
时间：2021年12月13日                         第一作者：N                       [链接](https://arxiv.org/abs/2201.02538).                     
## 摘要：脉冲神经网络（SNN）不同于深度学习中使用的经典网络：神经元使用称为脉冲的电脉冲进行通信，就像生物神经元一样。SNN对人工智能技术很有吸引力，因为它们可以在低功耗的神经形态芯片上实现。然而，SNN通常比其模拟对应物精度更低。在本报告中，我们研究了各种正则化和规范化技术，目的是改进SNN中的替代梯度学习。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are different from the classical networks used in deep learning: the neurons communicate using electrical impulses called spikes, just like biological neurons. SNNs are appealing for AI technology, because they could be implemented on low power neuromorphic chips. However, SNNs generally remain less accurate than their analog counterparts. In this report, we examine various regularization and normalization techniques with the goal of improving surrogate gradient learning in SNNs. </details>
<details>	<summary>注释</summary>	Bachelor Thesis </details>
<details>	<summary>邮件日期</summary>	2022年01月10日</details>

# 315、可塑性函数对神经装配的影响
- [ ] Effects of Plasticity Functions on Neural Assemblies 
时间：2021年12月29日                         第一作者：Christodoulos Constantinides                       [链接](https://arxiv.org/abs/2112.14853).                     
## 摘要：我们探讨了各种可塑性功能对神经元组装的影响。为了弥合实验理论和计算理论之间的鸿沟，我们使用了一个概念框架，即组装演算，它是一个基于神经元组装描述大脑功能的正式系统。集合演算包括投射、关联和合并神经元集合的操作。我们的研究重点是用装配演算模拟不同的塑性函数。我们的主要贡献是修改和评估投影操作。我们用Oja和脉冲时间依赖塑性（STDP）规则进行实验，并测试各种超参数的影响。
<details>	<summary>英文摘要</summary>	We explore the effects of various plasticity functions on assemblies of neurons. To bridge the gap between experimental and computational theories we make use of a conceptual framework, the Assembly Calculus, which is a formal system for the description of brain function based on assemblies of neurons. The Assembly Calculus includes operations for projecting, associating, and merging assemblies of neurons. Our research is focused on simulating different plasticity functions with Assembly Calculus. Our main contribution is the modification and evaluation of the projection operation. We experiment with Oja's and Spike Time-Dependent Plasticity (STDP) rules and test the effect of various hyper-parameters. </details>
<details>	<summary>邮件日期</summary>	2022年01月03日</details>

# 314、硅神经元事件计时的可靠性
- [ ] Reliability of Event Timing in Silicon Neurons 
时间：2021年12月28日                         第一作者：Tai Miyazaki Kirby                       [链接](https://arxiv.org/abs/2112.14134).                     
## 摘要：模拟低压电子学在以前所未有的能源效率生产硅神经元（SIN）方面显示出巨大的潜力。然而，它们对工艺、电压和温度（PVT）变化以及噪声的固有高敏感性长期以来被认为是开发有效神经形态解决方案的主要瓶颈。受生物物理新皮质神经元棘波传导研究的启发，我们证明了与生物神经元类似，模拟捷联惯导系统中固有的噪声和可变性可以与可靠的棘波传导共存。我们通过展示三种不同类型的可靠事件传输：单脉冲传输、突发传输和半中心振荡器（HCO）网络的开关控制，在最近的爆破神经元的神经形态模型上说明了这一特性。
<details>	<summary>英文摘要</summary>	Analog, low-voltage electronics show great promise in producing silicon neurons (SiNs) with unprecedented levels of energy efficiency. Yet, their inherently high susceptibility to process, voltage and temperature (PVT) variations, and noise has long been recognised as a major bottleneck in developing effective neuromorphic solutions. Inspired by spike transmission studies in biophysical, neocortical neurons, we demonstrate that the inherent noise and variability can coexist with reliable spike transmission in analog SiNs, similarly to biological neurons. We illustrate this property on a recent neuromorphic model of a bursting neuron by showcasing three different relevant types of reliable event transmission: single spike transmission, burst transmission, and the on-off control of a half-centre oscillator (HCO) network. </details>
<details>	<summary>邮件日期</summary>	2021年12月30日</details>

# 313、N-Omniglot：一个用于时空稀疏少镜头学习的大规模数据集
- [ ] N-Omniglot: a Large-scale Dataset for Spatio-Temporal Sparse Few-shot Learning 
时间：2021年12月25日                         第一作者：Yang Li                       [链接](https://arxiv.org/abs/2112.13230).                     
## 摘要：少镜头学习是人脑最重要的能力之一。然而，目前的人工智能系统在实现这一能力方面遇到了困难，生物上合理的脉冲神经网络（SNN）也是如此。传统少数镜头学习领域的数据集提供的时间信息量很少。而神经形态数据集的缺乏阻碍了SNN少数镜头学习的发展。在这里，我们使用动态视觉传感器（DVS）提供了第一个神经形态数据集：N-Omniglot。它包含1623类手写字符，每类只有20个样本。N-Omniglot消除了对SNN的神经形态数据集的需要，该数据集具有高度稀疏性和巨大的时间一致性。此外，由于笔划的时间顺序信息，该数据集为在少数镜头学习领域开发SNNs算法提供了强大的挑战和合适的基准。我们还提供了改进的最近邻、卷积网络、连体网和元学习算法，用于验证。
<details>	<summary>英文摘要</summary>	Few-shot learning (learning with a few samples) is one of the most important capacities of the human brain. However, the current artificial intelligence systems meet difficulties in achieving this ability, so as the biologically plausible spiking neural networks (SNNs). Datasets for traditional few-shot learning domains provide few amounts of temporal information. And the absence of the neuromorphic datasets has hindered the development of few-shot learning for SNNs. Here, we provide the first neuromorphic dataset: N-Omniglot, using the Dynamic Vision Sensor (DVS). It contains 1623 categories of handwritten characters, with only 20 samples per class. N-Omniglot eliminates the need for a neuromorphic dataset for SNNs with high spareness and tremendous temporal coherence. Additionally, the dataset provides a powerful challenge and a suitable benchmark for developing SNNs algorithm in the few-shot learning domain due to the chronological information of strokes. We also provide the improved nearest neighbor, convolutional network, SiameseNet, and meta-learning algorithm in spiking version for verification. </details>
<details>	<summary>邮件日期</summary>	2021年12月28日</details>

# 312、向强大的深脉冲神经网络推进剩余学习
- [ ] Advancing Residual Learning towards Powerful Deep Spiking Neural Networks 
时间：2021年12月23日                         第一作者：Yifan Hu                       [链接](https://arxiv.org/abs/2112.08954).                     
<details>	<summary>邮件日期</summary>	2021年12月24日</details>

# 311、深度神经网络可以转换为超低延迟脉冲神经网络吗？
- [ ] Can Deep Neural Networks be Converted to Ultra Low-Latency Spiking Neural Networks? 
时间：2021年12月22日                         第一作者：Gourav Datta                        [链接](https://arxiv.org/abs/2112.12133).                     
## 摘要：脉冲神经网络（SNN）通过随时间分布的二进制脉冲进行操作，已成为资源受限设备的一种有前途的节能ML范式。然而，目前最先进的（SOTA）SNN需要多个时间步才能达到可接受的推理精度，从而增加峰值活动，从而增加能耗。SNN的SOTA训练策略涉及非脉冲深度神经网络（DNN）的转换。在本文中，我们确定SOTA转换策略不能产生超低延迟，因为它们错误地假设DNN和SNN预激活值是均匀分布的。我们提出了一种新的训练算法，能够准确地捕获这些分布，最小化DNN和转换后的SNN之间的误差。由此产生的SNN具有超低延迟和高激活稀疏性，从而显著提高了计算效率。特别是，我们在几个VGG和ResNet体系结构上对CIFAR-10和CIFAR-100数据集的图像识别任务评估了我们的框架。与iso体系结构标准DNN相比，我们在CIFAR-100数据集上仅使用2个时间步长就获得了64.19%的顶级精度，计算能量降低了约159.2倍。与其他SOTA SNN模型相比，我们的模型推理速度快2.5-8倍（即，时间步长更少）。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs), that operate via binary spikes distributed over time, have emerged as a promising energy efficient ML paradigm for resource-constrained devices. However, the current state-of-the-art (SOTA) SNNs require multiple time steps for acceptable inference accuracy, increasing spiking activity and, consequently, energy consumption. SOTA training strategies for SNNs involve conversion from a non-spiking deep neural network (DNN). In this paper, we determine that SOTA conversion strategies cannot yield ultra low latency because they incorrectly assume that the DNN and SNN pre-activation values are uniformly distributed. We propose a new training algorithm that accurately captures these distributions, minimizing the error between the DNN and converted SNN. The resulting SNNs have ultra low latency and high activation sparsity, yielding significant improvements in compute efficiency. In particular, we evaluate our framework on image recognition tasks from CIFAR-10 and CIFAR-100 datasets on several VGG and ResNet architectures. We obtain top-1 accuracy of 64.19% with only 2 time steps on the CIFAR-100 dataset with ~159.2x lower compute energy compared to an iso-architecture standard DNN. Compared to other SOTA SNN models, our models perform inference 2.5-8x faster (i.e., with fewer time steps). </details>
<details>	<summary>注释</summary>	Accepted to DATE 2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月23日</details>

# 310、通过时间向前传播实现动态脉冲神经网络的精确在线训练
- [ ] Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time 
时间：2021年12月20日                         第一作者：Bojian Yin                       [链接](https://arxiv.org/abs/2112.11231).                     
## 摘要：大脑中脉冲神经元之间的事件驱动和稀疏通信特性为灵活高效的人工智能带来了巨大的希望。学习算法的最新进展表明，与标准的递归神经网络相比，脉冲神经元的递归网络可以有效地训练，以获得具有竞争力的性能。尽管如此，由于这些学习算法使用时间误差反向传播（BPTT），它们的内存需求很高，训练速度很慢，并且与在线学习不兼容。这限制了这些学习算法在相对较小的网络和有限的时间序列长度上的应用。已经提出了具有较低计算和内存复杂度的BPTT在线近似（e-prop，OSTL），但在实践中也受到内存限制，并且作为近似，其性能并不优于标准BPTT训练。在这里，我们展示了一种新开发的BPTT替代方案，即通过时间的前向传播（FPTT）如何应用于脉冲神经网络。与BPTT不同，FPTT试图最大限度地降低持续的动态规范化损失风险。因此，FPTT可以在线计算，并且相对于序列长度具有固定的复杂性。结合一种新的动态脉冲神经元模型——液体时间常数神经元，我们证明了用FPTT训练的SNN优于在线BPTT近似，在时间分类任务上接近或超过离线BPTT精度。因此，这种方法可以在长序列上以对记忆友好的在线方式训练SNN，并将SNN扩展到新颖复杂的神经结构。
<details>	<summary>英文摘要</summary>	The event-driven and sparse nature of communication between spiking neurons in the brain holds great promise for flexible and energy-efficient AI. Recent advances in learning algorithms have demonstrated that recurrent networks of spiking neurons can be effectively trained to achieve competitive performance compared to standard recurrent neural networks. Still, as these learning algorithms use error-backpropagation through time (BPTT), they suffer from high memory requirements, are slow to train, and are incompatible with online learning. This limits the application of these learning algorithms to relatively small networks and to limited temporal sequence lengths. Online approximations to BPTT with lower computational and memory complexity have been proposed (e-prop, OSTL), but in practice also suffer from memory limitations and, as approximations, do not outperform standard BPTT training. Here, we show how a recently developed alternative to BPTT, Forward Propagation Through Time (FPTT) can be applied in spiking neural networks. Different from BPTT, FPTT attempts to minimize an ongoing dynamically regularized risk on the loss. As a result, FPTT can be computed in an online fashion and has fixed complexity with respect to the sequence length. When combined with a novel dynamic spiking neuron model, the Liquid-Time-Constant neuron, we show that SNNs trained with FPTT outperform online BPTT approximations, and approach or exceed offline BPTT accuracy on temporal classification tasks. This approach thus makes it feasible to train SNNs in a memory-friendly online fashion on long sequences and scale up SNNs to novel and complex neural architectures. </details>
<details>	<summary>注释</summary>	12 pages, 4 figures </details>
<details>	<summary>邮件日期</summary>	2021年12月22日</details>

# 309、基于事件视觉的脉冲卷积网络对抗攻击
- [ ] Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision 
时间：2021年12月20日                         第一作者：Julian B\"uchel                       [链接](https://arxiv.org/abs/2110.02929).                     
<details>	<summary>注释</summary>	16 pages, preprint, submitted to ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 308、平衡态隐式微分训练反馈脉冲神经网络
- [ ] Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State 
时间：2021年12月17日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2109.14247).                     
<details>	<summary>注释</summary>	Accepted by NeurIPS 2021 (Spotlight) </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 307、脉冲相机的光流估计
- [ ] Optical Flow Estimation for Spiking Camera 
时间：2021年12月17日                         第一作者：Liwen Hu                       [链接](https://arxiv.org/abs/2110.03916).                     
<details>	<summary>注释</summary>	The first two authors contributed equally </details>
<details>	<summary>邮件日期</summary>	2021年12月20日</details>

# 306、向强大的深脉冲神经网络推进剩余学习
- [ ] Advancing Residual Learning towards Powerful Deep Spiking Neural Networks 
时间：2021年12月15日                         第一作者：Yifan Hu                       [链接](https://arxiv.org/abs/2112.08954).                     
## 摘要：尽管神经形态计算发展迅速，但脉冲神经网络（SNN）的容量和表示能力不足严重限制了其实际应用范围。剩余学习和捷径已被证明是训练深层神经网络的一种重要方法，但以前的工作很少评估它们对基于棘波的通信和时空动力学特征的适用性。在本文中，我们首先发现，这种疏忽导致阻碍信息流，并伴随着退化问题，在以前的剩余SNN。然后，我们提出了一种新的面向SNN的残差块MS ResNet，它能够显著扩展直接训练的SNN的深度，例如，在CIFAR-10上可以扩展到482层，在ImageNet上可以扩展到104层，而不会观察到任何轻微的退化问题。我们在基于帧和神经形态的数据集上验证了MS-ResNet104的有效性，MS-ResNet104在ImageNet上获得了76.02%的准确率，这是在直接训练SNN领域中的首次。我们还观察到，平均每个神经元只需要一个脉冲就可以对输入样本进行分类，这具有很高的能量效率。我们相信，我们强大且可扩展的模型将为SNN的进一步开发提供强大支持。
<details>	<summary>英文摘要</summary>	Despite the rapid progress of neuromorphic computing, inadequate capacity and insufficient representation power of spiking neural networks (SNNs) severely restrict their application scope in practice. Residual learning and shortcuts have been evidenced as an important approach for training deep neural networks, but rarely did previous work assess their applicability to the characteristics of spike-based communication and spatiotemporal dynamics. In this paper, we first identify that this negligence leads to impeded information flow and accompanying degradation problem in previous residual SNNs. Then we propose a novel SNN-oriented residual block, MS-ResNet, which is able to significantly extend the depth of directly trained SNNs, e.g. up to 482 layers on CIFAR-10 and 104 layers on ImageNet, without observing any slight degradation problem. We validate the effectiveness of MS-ResNet on both frame-based and neuromorphic datasets, and MS-ResNet104 achieves a superior result of 76.02% accuracy on ImageNet, the first time in the domain of directly trained SNNs. Great energy efficiency is also observed that on average only one spike per neuron is needed to classify an input sample. We believe our powerful and scalable models will provide a strong support for further exploration of SNNs. </details>
<details>	<summary>邮件日期</summary>	2021年12月17日</details>

# 305、利用生物神经元和突触进行规划
- [ ] Planning with Biological Neurons and Synapses 
时间：2021年12月15日                         第一作者：Francesco d'Amore                       [链接](https://arxiv.org/abs/2112.08186).                     
## 摘要：我们重新讨论了块世界中的规划问题，并为此任务实现了一个已知的启发式方法。重要的是，我们的实现在生物学上是合理的，因为它完全是通过神经元的脉冲来实现的。尽管在过去的五十年里，区块世界已经取得了很多成就，但我们相信这是同类算法中的第一个。输入是编码初始块堆栈集和目标集的符号序列，输出是运动命令序列，如“将顶部块放入表上堆栈1”。该程序是在汇编演算中编写的，汇编演算是最近提出的一种计算框架，旨在通过弥合神经活动和认知功能之间的差距来模拟大脑中的计算。它的基本对象是神经元的集合（稳定的神经元集合，它们的同时放电意味着主体正在思考一个对象、概念、单词等），它的命令包括投射和合并，它的执行模型基于广泛接受的神经科学原理。这个框架中的一个程序基本上建立了一个神经元和突触的动态系统，最终以很高的概率完成了任务。这项工作的目的是从经验上证明，汇编演算中合理的大型程序能够正确可靠地执行；而这种相当现实的——如果理想化的话——更高的认知功能，比如街区世界的规划，可以通过这样的程序成功地实现。
<details>	<summary>英文摘要</summary>	We revisit the planning problem in the blocks world, and we implement a known heuristic for this task. Importantly, our implementation is biologically plausible, in the sense that it is carried out exclusively through the spiking of neurons. Even though much has been accomplished in the blocks world over the past five decades, we believe that this is the first algorithm of its kind. The input is a sequence of symbols encoding an initial set of block stacks as well as a target set, and the output is a sequence of motion commands such as ``put the top block in stack 1 on the table''. The program is written in the Assembly Calculus, a recently proposed computational framework meant to model computation in the brain by bridging the gap between neural activity and cognitive function. Its elementary objects are assemblies of neurons (stable sets of neurons whose simultaneous firing signifies that the subject is thinking of an object, concept, word, etc.), its commands include project and merge, and its execution model is based on widely accepted tenets of neuroscience. A program in this framework essentially sets up a dynamical system of neurons and synapses that eventually, with high probability, accomplishes the task. The purpose of this work is to establish empirically that reasonably large programs in the Assembly Calculus can execute correctly and reliably; and that rather realistic -- if idealized -- higher cognitive functions, such as planning in the blocks world, can be implemented successfully by such programs. </details>
<details>	<summary>邮件日期</summary>	2021年12月16日</details>

# 304、全脉冲变分自动编码器
- [ ] Fully Spiking Variational Autoencoder 
时间：2021年12月14日                         第一作者：Hiromichi Kamata                       [链接](https://arxiv.org/abs/2110.00375).                     
<details>	<summary>注释</summary>	Accepted to AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 303、使用Extoll进行大规模Spike通信
- [ ] BrainScaleS Large Scale Spike Communication using Extoll 
时间：2021年12月14日                         第一作者：Tobias Thommes                       [链接](https://arxiv.org/abs/2111.15296).                     
<details>	<summary>注释</summary>	3 pages, 2 figures, submitted to the Neuro Inspired Computational Elements 2020 (NICE'2020) conference, accepted and presented as a poster in March 2021; 1st replacement: add acknowledgement of DFG (German Research Foundation) </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 302、基于事件的卷积神经网络加速器的突触压缩
- [ ] Synapse Compression for Event-Based Convolutional-Neural-Network Accelerators 
时间：2021年12月13日                         第一作者：Lennart Bamberg                       [链接](https://arxiv.org/abs/2112.07019).                     
## 摘要：制造可行的神经形态芯片需要新的计算机架构，以实现大脑轻松支持的大规模并行和高效的信息处理。新兴的基于事件的体系结构使这一梦想成为现实。然而，突触连接的大内存需求阻碍了现代卷积神经网络（CNN）在大规模并行、基于事件（脉冲）结构上的应用。这项工作克服了这一障碍，提供了一个轻量级的硬件方案，将突触内存需求压缩数千倍，使复杂的CNN能够在一个小型芯片上执行。12纳米技术中的硅实现表明，该技术仅增加了系统的实现成本2%，尽管与之前发布的最佳技术相比，总内存占用减少了374倍。
<details>	<summary>英文摘要</summary>	Manufacturing-viable neuromorphic chips require novel computer architectures to achieve the massively parallel and efficient information processing the brain supports so effortlessly. Emerging event-based architectures are making this dream a reality. However, the large memory requirements for synaptic connectivity are a showstopper for the execution of modern convolutional neural networks (CNNs) on massively parallel, event-based (spiking) architectures. This work overcomes this roadblock by contributing a lightweight hardware scheme to compress the synaptic memory requirements by several thousand times, enabling the execution of complex CNNs on a single chip of small form factor. A silicon implementation in a 12-nm technology shows that the technique increases the system's implementation cost by only 2%, despite achieving a total memory-footprint reduction of up to 374x compared to the best previously published technique. </details>
<details>	<summary>注释</summary>	Preprint submitted to IEEE Transactions on Parallel and Distributed Systems </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 301、神经形态混合脉冲运动检测器
- [ ] NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector 
时间：2021年12月12日                         第一作者：Pedro Machado                       [链接](https://arxiv.org/abs/2112.06102).                     
## 摘要：脊椎动物的视网膜在处理诸如检测运动物体等琐碎的视觉任务方面非常高效，但对于现代计算机来说这是一项复杂的任务。物体运动的检测是由名为物体运动敏感神经节细胞（OMS-GC）的特殊视网膜神经节细胞完成的。OMS-GC处理连续信号，并产生视觉皮层后处理的脉冲模式。本文提出的神经形态混合脉冲运动检测器（NeuroHSMD）使用现场可编程门阵列（FPGA）加速了HSMD算法。混合脉冲运动检测器（HSMD）算法是第一个使用定制的3层脉冲神经网络（SNN）增强动态背景减法（DBS）算法的混合算法，该网络生成OMS-GC脉冲样响应。使用相同的2012年变更检测（CDnet2012）和2014年变更检测（CDnet2014）基准数据集，将NeuroHSMD算法与HSMD算法进行比较。结果表明，神经HSMD实时生成与HSMD算法相同的结果，且质量没有下降。此外，本文提出的神经HSMD完全用开放式计算机语言（OpenCL）实现，因此很容易在其他设备中复制，如图形处理器单元（GPU）和中央处理器单元集群（CPU）。
<details>	<summary>英文摘要</summary>	Vertebrate retinas are highly-efficient in processing trivial visual tasks such as detecting moving objects, yet a complex task for modern computers. The detection of object motion is done by specialised retinal ganglion cells named Object-motion-sensitive ganglion cells (OMS-GC). OMS-GC process continuous signals and generate spike patterns that are post-processed by the Visual Cortex. The Neuromorphic Hybrid Spiking Motion Detector (NeuroHSMD) proposed in this work accelerates the HSMD algorithm using Field-Programmable Gate Arrays (FPGAs). The Hybrid Spiking Motion Detector (HSMD) algorithm was the first hybrid algorithm to enhance dynamic background subtraction (DBS) algorithms with a customised 3-layer spiking neural network (SNN) that generates OMS-GC spiking-like responses. The NeuroHSMD algorithm was compared against the HSMD algorithm, using the same 2012 change detection (CDnet2012) and 2014 change detection (CDnet2014) benchmark datasets. The results show that the NeuroHSMD has produced the same results as the HSMD algorithm in real-time without degradation of quality. Moreover, the NeuroHSMD proposed in this paper was completely implemented in Open Computer Language (OpenCL) and therefore is easily replicated in other devices such as Graphical Processor Units (GPUs) and clusters of Central Processor Units (CPUs). </details>
<details>	<summary>邮件日期</summary>	2021年12月14日</details>

# 300、随机STT-MRAM切换的同步无监督STDP学习
- [ ] Synchronous Unsupervised STDP Learning with Stochastic STT-MRAM Switching 
时间：2021年12月10日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2112.05707).                     
## 摘要：在神经形态系统中，模拟电阻状态用于存储权重受到制造不精确性和限制突触权重精度的设备随机性的阻碍。这一挑战可以通过模拟自旋转移转矩磁阻随机存取存储器（STT-MRAM）二元状态的随机切换来解决。然而，以前基于STT-MRAM的方法是以异步方式运行的，很难通过实验实现。本文提出了一种利用STT-MRAM的随机切换进行无监督学习的带时钟电路的同步脉冲神经网络系统。建议的系统使单层网络能够在MNIST数据集上实现90%的推理精度。
<details>	<summary>英文摘要</summary>	The use of analog resistance states for storing weights in neuromorphic systems is impeded by fabrication imprecision and device stochasticity that limit the precision of synapse weights. This challenge can be resolved by emulating analog behavior with the stochastic switching of the binary states of spin-transfer torque magnetoresistive random-access memory (STT-MRAM). However, previous approaches based on STT-MRAM operate in an asynchronous manner that is difficult to implement experimentally. This paper proposes a synchronous spiking neural network system with clocked circuits that perform unsupervised learning leveraging the stochastic switching of STT-MRAM. The proposed system enables a single-layer network to achieve 90% inference accuracy on the MNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2021年12月13日</details>

# 299、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年12月07日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>注释</summary>	Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年12月08日</details>

# 298、混合SNN-ANN：基于事件视觉的节能分类和目标检测
- [ ] Hybrid SNN-ANN: Energy-Efficient Classification and Object Detection for Event-Based Vision 
时间：2021年12月06日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2112.03423).                     
## 摘要：基于事件的视觉传感器对事件流而非图像帧中的局部像素亮度变化进行编码，并产生稀疏、节能的场景编码，此外还具有低延迟、高动态范围和缺少运动模糊的特点。基于事件的传感器在目标识别方面的最新进展来自使用反向传播训练的深层神经网络的转换。然而，将这些方法用于事件流需要转换为同步范式，这不仅会损失计算效率，而且还会错过提取时空特征的机会。在本文中，我们提出了一种用于基于事件的模式识别和目标检测的深度神经网络端到端训练的混合体系结构，结合了用于高效基于事件的特征提取的脉冲神经网络（SNN）主干，以及随后的模拟神经网络（ANN）头来解决同步分类和检测任务。这是通过将标准反向传播与代理梯度训练相结合来实现的，以通过SNN传播梯度。混合SNN ANN可以在不进行转换的情况下进行训练，并生成比ANN对应网络更具计算效率的高精度网络。我们展示了基于事件的分类和目标检测数据集的结果，其中只需要调整ANN头的结构以适应任务，并且不需要转换基于事件的输入。由于ANN和SNN需要不同的硬件模式来最大限度地提高其效率，我们设想SNN主干和ANN头部可以在不同的处理单元上执行，从而分析两部分之间通信所需的带宽。混合网络是一种很有前途的体系结构，可以在不影响效率的情况下进一步推进基于事件的视觉的机器学习方法。
<details>	<summary>英文摘要</summary>	Event-based vision sensors encode local pixel-wise brightness changes in streams of events rather than image frames and yield sparse, energy-efficient encodings of scenes, in addition to low latency, high dynamic range, and lack of motion blur. Recent progress in object recognition from event-based sensors has come from conversions of deep neural networks, trained with backpropagation. However, using these approaches for event streams requires a transformation to a synchronous paradigm, which not only loses computational efficiency, but also misses opportunities to extract spatio-temporal features. In this article we propose a hybrid architecture for end-to-end training of deep neural networks for event-based pattern recognition and object detection, combining a spiking neural network (SNN) backbone for efficient event-based feature extraction, and a subsequent analog neural network (ANN) head to solve synchronous classification and detection tasks. This is achieved by combining standard backpropagation with surrogate gradient training to propagate gradients through the SNN. Hybrid SNN-ANNs can be trained without conversion, and result in highly accurate networks that are substantially more computationally efficient than their ANN counterparts. We demonstrate results on event-based classification and object detection datasets, in which only the architecture of the ANN heads need to be adapted to the tasks, and no conversion of the event-based input is necessary. Since ANNs and SNNs require different hardware paradigms to maximize their efficiency, we envision that SNN backbone and ANN head can be executed on different processing units, and thus analyze the necessary bandwidth to communicate between the two parts. Hybrid networks are promising architectures to further advance machine learning approaches for event-based vision, without having to compromise on efficiency. </details>
<details>	<summary>注释</summary>	Accepted at DAGM German Conference on Pattern Recognition (GCPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年12月08日</details>

# 297、BS4NN：具有时间编码和学习的二值化脉冲神经网络
- [ ] BS4NN: Binarized Spiking Neural Networks with Temporal Coding and Learning 
时间：2021年12月06日                         第一作者：Saeed Reza Kheradpisheh                       [链接](https://arxiv.org/abs/2007.04039).                     
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 296、基于代理的脉冲神经网络训练
- [ ] Spiking neural networks trained via proxy 
时间：2021年12月05日                         第一作者：Saeed Reza Kheradpisheh                       [链接](https://arxiv.org/abs/2109.13208).                     
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 295、多阈值超低潜伏期脉冲神经网络的反向传播直接训练
- [ ] Direct Training via Backpropagation for Ultra-low Latency Spiking Neural Networks with Multi-threshold 
时间：2021年11月25日                         第一作者：Changqing Xu                       [链接](https://arxiv.org/abs/2112.07426).                     
## 摘要：脉冲神经网络（SNN）可以利用时空信息，具有能量效率的特性，是深度神经网络（DNN）的一个很好的替代方案。事件驱动的信息处理使得SNNs可以减少DNNs的昂贵计算量，节省大量的能量消耗。然而，较高的训练和推理延迟限制了更深层次SNN的发展。snn在训练和推理过程中通常需要数十甚至数百个时间步长，这不仅会增加延迟，而且会造成能量消耗的浪费。为了克服这个问题，我们提出了一种新的基于反向传播（BP）的多阈值超低延迟（1-2个时间步长）SNN训练方法。为了提高每个脉冲的信息容量，我们引入了多阈值泄漏集成和触发（LIF）模型。在我们提出的训练方法中，我们提出了三个近似的脉冲活动导数，以解决基于BP的SNN直接训练中存在的不可微问题。实验结果表明，我们提出的方法在MNIST、FashionMNIST和CIFAR10上的平均准确率分别为99.56%、93.08%和87.90%，只需2个时间步长。对于CIFAR10数据集，我们提出的方法比以前报道的直接训练SNN的精度提高了1.12%，时间步长更少。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) can utilize spatio-temporal information and have a nature of energy efficiency which is a good alternative to deep neural networks(DNNs). The event-driven information processing makes SNNs can reduce the expensive computation of DNNs and save a lot of energy consumption. However, high training and inference latency is a limitation of the development of deeper SNNs. SNNs usually need tens or even hundreds of time steps during the training and inference process which causes not only the increase of latency but also the waste of energy consumption. To overcome this problem, we proposed a novel training method based on backpropagation (BP) for ultra-low latency(1-2 time steps) SNN with multi-threshold. In order to increase the information capacity of each spike, we introduce the multi-threshold Leaky Integrate and Fired (LIF) model. In our proposed training method, we proposed three approximated derivative for spike activity to solve the problem of the non-differentiable issue which cause difficulties for direct training SNNs based on BP. The experimental results show that our proposed method achieves an average accuracy of 99.56%, 93.08%, and 87.90% on MNIST, FashionMNIST, and CIFAR10, respectively with only 2 time steps. For the CIFAR10 dataset, our proposed method achieve 1.12% accuracy improvement over the previously reported direct trained SNNs with fewer time steps. </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 294、Nengo上使用脉冲神经网络的稀疏分布记忆
- [ ] Sparse Distributed Memory using Spiking Neural Networks on Nengo 
时间：2021年12月03日                         第一作者：Rohan Deepak Ajwani                       [链接](https://arxiv.org/abs/2109.03111).                     
<details>	<summary>注释</summary>	8 pages, 11 figures, accepted as poster in Bernstein Conference 2021 ACM-class: H.3.2; I.5.5 </details>
<details>	<summary>邮件日期</summary>	2021年12月06日</details>

# 293、BioLCNet：报酬调制局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2021年12月03日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
<details>	<summary>注释</summary>	12 pages, 5 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年12月06日</details>

# 292、PrivateSNN：保护隐私的脉冲神经网络
- [ ] PrivateSNN: Privacy-Preserving Spiking Neural Networks 
时间：2021年12月02日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2104.03414).                     
<details>	<summary>注释</summary>	Accepted to AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月03日</details>

# 291、立体画：用脉冲神经网络进行深度学习
- [ ] StereoSpike: Depth Learning with a Spiking Neural Network 
时间：2021年11月25日                         第一作者：Ulysse Ran\c{c}on                       [链接](https://arxiv.org/abs/2109.13751).                     
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 290、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年11月25日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
<details>	<summary>注释</summary>	14 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 289、基于信息瓶颈的Hebbian学习规则自然地将工作记忆和突触更新联系起来
- [ ] Information Bottleneck-Based Hebbian Learning Rule Naturally Ties Working Memory and Synaptic Updates 
时间：2021年11月24日                         第一作者：Kyle Daruwalla                        [链接](https://arxiv.org/abs/2111.13187).                     
## 摘要：人工神经网络通过反向传播训练极深的网络，成功地解决了各种各样的问题。反向传播直接应用于脉冲神经网络包含生物学上不可信的成分，如权重传递问题或独立的推理和学习阶段。各种方法分别处理不同的组件，但完整的解决方案仍然是无形的。这里，我们采用另一种方法，完全避免反向传播及其相关问题。最近在深度学习方面的工作建议通过信息瓶颈（IB）对网络的每一层进行独立培训。随后的研究指出，这种分层方法避免了错误在各层之间的传播，从而形成了一种生物学上合理的范式。不幸的是，IB是使用一批样本计算的。之前的工作通过仅使用两个样本（当前和以前的样本）的权重更新解决了这一问题。我们的工作采用不同的方法，将权重更新分解为局部和全局组件。局部分量是Hebbian分量，仅取决于当前样本。全局分量根据一批样本计算分层调制信号。我们证明了这种调制信号可以通过一个像储存器一样具有工作记忆（WM）的辅助电路来学习。因此，我们可以使用大于两个的批量大小，批量大小决定了WM所需的容量。据我们所知，我们的规则是第一个生物学上合理的机制，直接将突触更新与任务的WM相结合。我们在合成数据集和图像分类数据集（如MNIST）上评估了我们的规则，并探讨了WM能力对学习绩效的影响。我们希望我们的工作是理解记忆在学习中的机制作用的第一步。
<details>	<summary>英文摘要</summary>	Artificial neural networks have successfully tackled a large variety of problems by training extremely deep networks via back-propagation. A direct application of back-propagation to spiking neural networks contains biologically implausible components, like the weight transport problem or separate inference and learning phases. Various methods address different components individually, but a complete solution remains intangible. Here, we take an alternate approach that avoids back-propagation and its associated issues entirely. Recent work in deep learning proposed independently training each layer of a network via the information bottleneck (IB). Subsequent studies noted that this layer-wise approach circumvents error propagation across layers, leading to a biologically plausible paradigm. Unfortunately, the IB is computed using a batch of samples. The prior work addresses this with a weight update that only uses two samples (the current and previous sample). Our work takes a different approach by decomposing the weight update into a local and global component. The local component is Hebbian and only depends on the current sample. The global component computes a layer-wise modulatory signal that depends on a batch of samples. We show that this modulatory signal can be learned by an auxiliary circuit with working memory (WM) like a reservoir. Thus, we can use batch sizes greater than two, and the batch size determines the required capacity of the WM. To the best of our knowledge, our rule is the first biologically plausible mechanism to directly couple synaptic updates with a WM of the task. We evaluate our rule on synthetic datasets and image classification datasets like MNIST, and we explore the effect of the WM capacity on learning performance. We hope our work is a first-step towards understanding the mechanistic role of memory in learning. </details>
<details>	<summary>注释</summary>	21 pages, 10 figures, under review </details>
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 288、实时智能车辆监控系统
- [ ] Real-time smart vehicle surveillance system 
时间：2021年11月24日                         第一作者：Shantha Kumar S                       [链接](https://arxiv.org/abs/2111.12289).                     
## 摘要：在过去十年中，全球犯罪活动激增。据印度警察局称，车辆盗窃是最难侦破的犯罪之一，所有记录在案的案件中有近19%与机动车盗窃有关。为了战胜这些对手，我们提出了一种实时车辆监控系统，该系统使用CCTV视频源检测和跟踪可疑车辆。该系统提取车辆的各种属性，如品牌、型号、颜色、车牌号和车牌类型。各种图像处理和深度学习算法的使用，以满足该系统的目标。提取的特征可以用作报告违法行为的证据。尽管系统使用了更多的参数，但它仍然能够以最小的延迟和精度损失进行实时预测。
<details>	<summary>英文摘要</summary>	Over the last decade, there has been a spike in criminal activity all around the globe. According to the Indian police department, vehicle theft is one of the least solved offenses, and almost 19% of all recorded cases are related to motor vehicle theft. To overcome these adversaries, we propose a real-time vehicle surveillance system, which detects and tracks the suspect vehicle using the CCTV video feed. The proposed system extracts various attributes of the vehicle such as Make, Model, Color, License plate number, and type of the license plate. Various image processing and deep learning algorithms are employed to meet the objectives of the proposed system. The extracted features can be used as evidence to report violations of law. Although the system uses more parameters, it is still able to make real time predictions with minimal latency and accuracy loss. </details>
<details>	<summary>邮件日期</summary>	2021年11月25日</details>

# 287、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年11月24日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
<details>	<summary>注释</summary>	14 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月25日</details>

# 286、用于节能嵌入式神经形态计算的多核大小μ脑设计
- [ ] Design of Many-Core Big Little \mu Brain for Energy-Efficient Embedded Neuromorphic Computing 
时间：2021年11月23日                         第一作者：M. Lakshmi Varshika                       [链接](https://arxiv.org/abs/2111.11838).                     
## 摘要：随着嵌入式系统中基于脉冲的深度学习推理应用的增加，这些系统倾向于集成神经形态加速器，如$\mu$Brain，以提高能源效率。我们提出了一种基于$\mu$大脑的可扩展多核神经形态硬件设计，以加速脉冲深度卷积神经网络（SDCNN）的计算。为了提高能源效率，内核在神经元和突触容量方面被设计为异构的（大内核的容量比小内核高），并且它们使用并行分段总线互连进行互连，与传统的基于网格的片上网络（NoC）相比，这导致更低的延迟和能量。我们提出了一个名为SentryOS的系统软件框架，将SDCNN推理应用程序映射到所提出的设计。SentryOS由编译器和运行时管理器组成。编译器利用大小$\mu$脑内核的内部架构，将SDCNN应用程序编译成子网。运行时管理器将这些子网络调度到核心上，并通过管道将其执行以提高吞吐量。我们使用五种常用的SDCNN推理应用程序评估了提议的大-小-多核心神经形态设计和系统软件框架，结果表明，提议的解决方案降低了能量（37%到98%），减少了延迟（9%到25%），并提高了应用程序吞吐量（20%到36%）。我们还表明，SentryOS可以很容易地扩展到其他脉冲神经形态加速器。
<details>	<summary>英文摘要</summary>	As spiking-based deep learning inference applications are increasing in embedded systems, these systems tend to integrate neuromorphic accelerators such as $\mu$Brain to improve energy efficiency. We propose a $\mu$Brain-based scalable many-core neuromorphic hardware design to accelerate the computations of spiking deep convolutional neural networks (SDCNNs). To increase energy efficiency, cores are designed to be heterogeneous in terms of their neuron and synapse capacity (big cores have higher capacity than the little ones), and they are interconnected using a parallel segmented bus interconnect, which leads to lower latency and energy compared to a traditional mesh-based Network-on-Chip (NoC). We propose a system software framework called SentryOS to map SDCNN inference applications to the proposed design. SentryOS consists of a compiler and a run-time manager. The compiler compiles an SDCNN application into subnetworks by exploiting the internal architecture of big and little $\mu$Brain cores. The run-time manager schedules these sub-networks onto cores and pipeline their execution to improve throughput. We evaluate the proposed big little many-core neuromorphic design and the system software framework with five commonlyused SDCNN inference applications and show that the proposed solution reduces energy (between 37% and 98%), reduces latency (between 9% and 25%), and increases application throughput (between 20% and 36%). We also show that SentryOS can be easily extended for other spiking neuromorphic accelerators. </details>
<details>	<summary>注释</summary>	Accepted for publication at DATE 2022 </details>
<details>	<summary>邮件日期</summary>	2021年11月24日</details>

# 285、BioLCNet：报酬调制局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2021年11月23日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
<details>	<summary>注释</summary>	12 pages, 5 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年11月24日</details>

# 284、BioLeaF：一个用于训练脉冲神经网络的生物似然学习框架
- [ ] BioLeaF: A Bio-plausible Learning Framework for Training of Spiking Neural Networks 
时间：2021年11月14日                         第一作者：Yukun Yang                       [链接](https://arxiv.org/abs/2111.13188).                     
## 摘要：我们的大脑由生物神经元组成，这些神经元通过精确的脉冲计时编码信息，但我们大脑的结构和学习规则在很大程度上仍然未知。与最近发展的基于反向传播（BP）的方法相比，基于反向传播的方法能够以高精度训练脉冲神经网络（SNN），生物学上可行的方法仍处于初级阶段。在这项工作中，我们希望回答这样一个问题：是否有可能通过基于BP的规则和生物似是而非的机制来训练SNN，从而获得可比的准确性。我们提出了一个新的生物似是而非的学习框架，由两个部分组成：一个新的体系结构及其支持的学习规则。通过两种类型的细胞和四种类型的突触连接，所提出的局部微电路结构可以通过局部反馈连接计算和传播错误信号，并支持具有全局定义的脉冲错误函数的多层SNN的训练。在我们的微电路架构下，我们采用了在局部隔室中运行的脉冲时间依赖性可塑性（STDP）规则来更新突触权重，并以生物学上合理的方式实现监督学习。最后，我们从优化的角度对该框架进行了解释，并在特定情况下证明了它与基于BP的规则的等价性。我们的实验表明，所提出的框架显示了与基于BP的规则相当的学习精度，并可能为生物系统中如何协调学习提供新的见解。
<details>	<summary>英文摘要</summary>	Our brain consists of biological neurons encoding information through accurate spike timing, yet both the architecture and learning rules of our brain remain largely unknown. Comparing to the recent development of backpropagation-based (BP-based) methods that are able to train spiking neural networks (SNNs) with high accuracy, biologically plausible methods are still in their infancy. In this work, we wish to answer the question of whether it is possible to attain comparable accuracy of SNNs trained by BP-based rules with bio-plausible mechanisms. We propose a new bio-plausible learning framework, consisting of two components: a new architecture, and its supporting learning rules. With two types of cells and four types of synaptic connections, the proposed local microcircuit architecture can compute and propagate error signals through local feedback connections and support training of multi-layers SNNs with a globally defined spiking error function. Under our microcircuit architecture, we employ the Spike-Timing-Dependent-Plasticity (STDP) rule operating in local compartments to update synaptic weights and achieve supervised learning in a biologically plausible manner. Finally, We interpret the proposed framework from an optimization point of view and show the equivalence between it and the BP-based rules under a special circumstance. Our experiments show that the proposed framework demonstrates learning accuracy comparable to BP-based rules and may provide new insights on how learning is orchestrated in biological systems. </details>
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 283、通过乘法突触的脉冲神经网络的非线性计算
- [ ] Nonlinear computations in spiking neural networks through multiplicative synapses 
时间：2021年11月22日                         第一作者：Michele Nardin                       [链接](https://arxiv.org/abs/2009.03857).                     
<details>	<summary>注释</summary>	This article has been peer-reviewed and recommended by Peer Community In Neuroscience Journal-ref: Peer Community In Neuroscience, 2021 DOI: 10.24072/pci.cneuro.100003 </details>
<details>	<summary>邮件日期</summary>	2021年11月23日</details>

# 282、E3NE：一种在FPGA上使用新兴神经编码加速脉冲神经网络的端到端框架
- [ ] E3NE: An End-to-End Framework for Accelerating Spiking Neural Networks with Emerging Neural Encoding on FPGAs 
时间：2021年11月19日                         第一作者：Daniel Gerlinghoff                       [链接](https://arxiv.org/abs/2111.10027).                     
## 摘要：编译器框架对于广泛使用基于FPGA的深度学习加速器至关重要。它们允许不熟悉硬件工程的研究人员和开发人员利用特定领域逻辑所获得的性能。传统的人工神经网络有多种框架。然而，没有太多的研究投入到为脉冲神经网络（SNN）优化的框架的创建上。这种新一代的神经网络对于在边缘设备上部署AI变得越来越有趣，因为边缘设备具有很强的功率和资源约束。我们的端到端框架E3NE自动化了FPGA高效SNN推理逻辑的生成。基于PyTorch模型和用户参数，它应用各种优化并评估基于spike加速器固有的权衡。多层次的并行性和新兴神经编码方案的使用使得效率优于以前的SNN硬件实现。对于类似型号，E3NE使用的硬件资源不到50%，功耗减少20%，同时将延迟降低了一个数量级。此外，可伸缩性和通用性允许部署大规模SNN模型AlexNet和VGG。
<details>	<summary>英文摘要</summary>	Compiler frameworks are crucial for the widespread use of FPGA-based deep learning accelerators. They allow researchers and developers, who are not familiar with hardware engineering, to harness the performance attained by domain-specific logic. There exists a variety of frameworks for conventional artificial neural networks. However, not much research effort has been put into the creation of frameworks optimized for spiking neural networks (SNNs). This new generation of neural networks becomes increasingly interesting for the deployment of AI on edge devices, which have tight power and resource constraints. Our end-to-end framework E3NE automates the generation of efficient SNN inference logic for FPGAs. Based on a PyTorch model and user parameters, it applies various optimizations and assesses trade-offs inherent to spike-based accelerators. Multiple levels of parallelism and the use of an emerging neural encoding scheme result in an efficiency superior to previous SNN hardware implementations. For a similar model, E3NE uses less than 50% of hardware resources and 20% less power, while reducing the latency by an order of magnitude. Furthermore, scalability and generality allowed the deployment of the large-scale SNN models AlexNet and VGG. </details>
<details>	<summary>注释</summary>	Accepted by IEEE Transactions on Parallel and Distributed Systems </details>
<details>	<summary>邮件日期</summary>	2021年11月22日</details>

# 281、用于神经形态信息处理的共振隧穿二极管纳米光电脉冲节点
- [ ] Resonant tunnelling diode nano-optoelectronic spiking nodes for neuromorphic information processing 
时间：2021年11月19日                         第一作者：Mat\v{e}j Hejda                       [链接](https://arxiv.org/abs/2107.06721).                     
<details>	<summary>注释</summary>	Updated with feedback from first round of reviews. Updated figure with 3D model </details>
<details>	<summary>邮件日期</summary>	2021年11月22日</details>

# 280、用局部规则训练的脉冲网络的连续学习
- [ ] Continuous learning of spiking networks trained with local rules 
时间：2021年11月18日                         第一作者：Dmitry Antonov                       [链接](https://arxiv.org/abs/2111.09553).                     
## 摘要：人工神经网络（ANN）在顺序学习过程中会经历灾难性遗忘（CF）。相反，大脑可以持续学习，而不会出现任何灾难性遗忘的迹象。脉冲神经网络（SNN）是下一代人工神经网络，它借鉴了生物神经网络的许多特性。因此，SNN有可能更好地抵抗CF。在本文中，我们研究了SNN对CF的易感性，并测试了几种受生物学启发的缓解灾难性遗忘的方法。SNN的训练采用基于脉冲时间依赖性可塑性（STDP）的生物学上合理的局部训练规则。当地培训禁止直接使用基于全局损失函数梯度的CF预防方法。我们开发并测试了基于随机朗之万动力学确定突触重要性（权重）的方法，无需梯度。此外，还测试了其他几种模拟神经网络的灾难性遗忘预防方法。这些实验是在SpykeTorch环境中的免费数据集上进行的。
<details>	<summary>英文摘要</summary>	Artificial neural networks (ANNs) experience catastrophic forgetting (CF) during sequential learning. In contrast, the brain can learn continuously without any signs of catastrophic forgetting. Spiking neural networks (SNNs) are the next generation of ANNs with many features borrowed from biological neural networks. Thus, SNNs potentially promise better resilience to CF. In this paper, we study the susceptibility of SNNs to CF and test several biologically inspired methods for mitigating catastrophic forgetting. SNNs are trained with biologically plausible local training rules based on spike-timing-dependent plasticity (STDP). Local training prohibits the direct use of CF prevention methods based on gradients of a global loss function. We developed and tested the method to determine the importance of synapses (weights) based on stochastic Langevin dynamics without the need for the gradients. Several other methods of catastrophic forgetting prevention adapted from analog neural networks were tested as well. The experiments were performed on freely available datasets in the SpykeTorch environment. </details>
<details>	<summary>注释</summary>	25 pages </details>
<details>	<summary>邮件日期</summary>	2021年11月19日</details>

# 279、转换脉冲神经网络的L4范数权重调整
- [ ] L4-Norm Weight Adjustments for Converted Spiking Neural Networks 
时间：2021年11月17日                         第一作者：Jason Allred                       [链接](https://arxiv.org/abs/2111.09446).                     
## 摘要：由于稀疏的、事件驱动的计算，脉冲神经网络（SNN）因其潜在的能源效率优势正在被探索。非脉冲人工神经网络通常采用反向传播的随机梯度下降法进行训练。脉冲神经元的不可微放电事件阻碍了反向传播真实梯度的计算。另一方面，使用近似梯度是有效的，但在许多时间步长上计算成本很高。因此，训练脉冲神经网络的一种常用技术是训练拓扑等价的非脉冲网络，然后将其转换为脉冲网络，用比例速率编码的泊松脉冲序列替换实值输入。转换后的SNN功能足够好，因为脉冲神经元的平均预放电膜电位与输入速率向量和神经元权重向量的点积成正比，类似于非脉冲网络的功能。然而，这种转换只考虑膜电位的平均值，而不考虑膜电位的时间变化。由于预放电膜电位的标准偏差与神经元权重向量的L4范数成正比，我们在转换过程中提出了基于L4范数的权重调整，以提高转换网络的分类精度。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are being explored for their potential energy efficiency benefits due to sparse, event-driven computation. Non-spiking artificial neural networks are typically trained with stochastic gradient descent using backpropagation. The calculation of true gradients for backpropagation in spiking neural networks is impeded by the non-differentiable firing events of spiking neurons. On the other hand, using approximate gradients is effective, but computationally expensive over many time steps. One common technique, then, for training a spiking neural network is to train a topologically-equivalent non-spiking network, and then convert it to an spiking network, replacing real-valued inputs with proportionally rate-encoded Poisson spike trains. Converted SNNs function sufficiently well because the mean pre-firing membrane potential of a spiking neuron is proportional to the dot product of the input rate vector and the neuron weight vector, similar to the functionality of a non-spiking network. However, this conversion only considers the mean and not the temporal variance of the membrane potential. As the standard deviation of the pre-firing membrane potential is proportional to the L4-norm of the neuron weight vector, we propose a weight adjustment based on the L4-norm during the conversion process in order to improve classification accuracy of the converted network. </details>
<details>	<summary>邮件日期</summary>	2021年11月19日</details>

# 278、量子叠加激励的脉冲神经网络
- [ ] Quantum Superposition Inspired Spiking Neural Network 
时间：2021年11月17日                         第一作者：Yinqian Sun                       [链接](https://arxiv.org/abs/2010.12197).                     
<details>	<summary>邮件日期</summary>	2021年11月18日</details>

# 277、从卷积到脉冲：社区目前忽略的环境指标
- [ ] From Convolutions towards Spikes: The Environmental Metric that the Community currently Misses 
时间：2021年11月16日                         第一作者：Aviral Chharia                       [链接](https://arxiv.org/abs/2111.08361).                     
## 摘要：如今，人工智能社区痴迷于将“最先进的”分数（80%的论文在NeurIPS中）作为主要的性能指标，因此一个重要参数，即环境指标，仍然没有报告。十年前，计算能力是一个限制因素；然而，在可预见的未来环境中，挑战将是开发环境友好且节能的算法。人类的大脑已经自我优化了将近一百万年，消耗的能量与典型的笔记本电脑一样。因此，开发受自然启发的算法是一种解决方案。在这项研究中，我们发现目前使用的人工神经网络并不是我们在自然界中发现的，这也是为什么尽管性能较低，但反映哺乳动物视觉皮层的脉冲神经网络却吸引了很多人的兴趣。我们进一步强调限制研究人员使用基于峰值的计算大规模开发神经形态节能微芯片的硬件缺口。使用神经形态处理器代替传统的GPU可能更环保、更高效。这些处理器将使SNN成为解决该问题的理想方案。本文对当前的差距、比较研究的缺乏进行了深入的关注，同时在神经科学和深度学习这两个领域的交叉点提出了新的研究方向。此外，我们定义了一个新的评估指标“自然”，用于报告人工智能模型的碳足迹。
<details>	<summary>英文摘要</summary>	Today, the AI community is obsessed with 'state-of-the-art' scores (80% papers in NeurIPS) as the major performance metrics, due to which an important parameter, i.e., the environmental metric, remains unreported. Computational capabilities were a limiting factor a decade ago; however, in foreseeable future circumstances, the challenge will be to develop environment-friendly and power-efficient algorithms. The human brain, which has been optimizing itself for almost a million years, consumes the same amount of power as a typical laptop. Therefore, developing nature-inspired algorithms is one solution to it. In this study, we show that currently used ANNs are not what we find in nature, and why, although having lower performance, spiking neural networks, which mirror the mammalian visual cortex, have attracted much interest. We further highlight the hardware gaps restricting the researchers from using spike-based computation for developing neuromorphic energy-efficient microchips on a large scale. Using neuromorphic processors instead of traditional GPUs might be more environment friendly and efficient. These processors will turn SNNs into an ideal solution for the problem. This paper presents in-depth attention highlighting the current gaps, the lack of comparative research, while proposing new research directions at the intersection of two fields -- neuroscience and deep learning. Further, we define a new evaluation metric 'NATURE' for reporting the carbon footprint of AI models. </details>
<details>	<summary>注释</summary>	NeurIPS 2021 Human-Centered AI Workshop </details>
<details>	<summary>邮件日期</summary>	2021年11月17日</details>

# 276、Spiking CapsNet：一种在胶囊之间具有生物学上合理的路由规则的Spiking神经网络
- [ ] Spiking CapsNet: A Spiking Neural Network With A Biologically Plausible Routing Rule Between Capsules 
时间：2021年11月15日                         第一作者：Dongcheng Zhao                       [链接](https://arxiv.org/abs/2111.07785).                     
## 摘要：脉冲神经网络（SNN）以其强大的时空信息表示能力而备受关注。胶囊神经网络（CapsNet）在不同层次上具有良好的组装和耦合特性。在这里，我们通过将胶囊引入到脉冲神经网络的建模中，提出了脉冲CapsNet。此外，我们还提出了一种更具生物学意义的依赖于脉冲时间的可塑性路由机制。通过充分考虑低电平脉冲胶囊和高电平脉冲胶囊之间的时空关系，进一步提高了它们之间的耦合能力。我们已经在MNIST和FashionMNIST数据集上验证了实验。与其他优秀的SNN模型相比，我们的算法仍然具有较高的性能。我们的脉冲CapsNet充分结合了SNN和CapsNet的优点，对噪声和仿射变换具有很强的鲁棒性。通过向测试数据集添加不同的椒盐噪声和高斯噪声，实验结果表明，当噪声较大时，我们的峰值CapsNet表现出更稳健的性能，而人工神经网络不能正确地进行澄清。此外，我们的脉冲CapsNet对AffNIST数据集上的仿射变换具有很强的泛化能力。
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN) has attracted much attention due to their powerful spatio-temporal information representation ability. Capsule Neural Network (CapsNet) does well in assembling and coupling features at different levels. Here, we propose Spiking CapsNet by introducing the capsules into the modelling of spiking neural networks. In addition, we propose a more biologically plausible Spike Timing Dependent Plasticity routing mechanism. By fully considering the spatio-temporal relationship between the low-level spiking capsules and the high-level spiking capsules, the coupling ability between them is further improved. We have verified experiments on the MNIST and FashionMNIST datasets. Compared with other excellent SNN models, our algorithm still achieves high performance. Our Spiking CapsNet fully combines the strengthens of SNN and CapsNet, and shows strong robustness to noise and affine transformation. By adding different Salt-Pepper and Gaussian noise to the test dataset, the experimental results demonstrate that our Spiking CapsNet shows a more robust performance when there is more noise, while the artificial neural network can not correctly clarify. As well, our Spiking CapsNet shows strong generalization to affine transformation on the AffNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2021年11月16日</details>

# 275、非监督学习优化的脉冲神经元突触可塑性模型
- [ ] A Spiking Neuron Synaptic Plasticity Model Optimized for Unsupervised Learning 
时间：2021年11月12日                         第一作者：Mikhail Kiselev                       [链接](https://arxiv.org/abs/2111.06768).                     
## 摘要：脉冲神经网络（SNN）被认为是执行各种学习任务（无监督、有监督和强化学习）的基础。SNN中的学习是通过突触可塑性实现的，突触可塑性规则决定突触重量的动态，通常取决于突触前和突触后神经元的活动。不同学习机制的多样性假设不同形式的突触可塑性可能对无监督和有监督学习最有效，因为在活体神经元中观察到，与基本的棘波时间依赖可塑性（STDP）模型存在多种偏差。在本论文中，我们制定了无监督学习问题对塑性规则的具体要求，并构建了一个新的塑性模型来推广STDP并满足这些要求。这种可塑性模型是本文提出的一种新的监督学习算法SCoBUL（Spike-Correlation-Based Unsupervised learning）的主要逻辑组成部分。我们还提供了计算机模拟实验的结果，证实了这些突触可塑性规则和SCoBUL算法的有效性。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) are considered as a perspective basis for performing all kinds of learning tasks - unsupervised, supervised and reinforcement learning. Learning in SNN is implemented through synaptic plasticity - the rules which determine dynamics of synaptic weights depending usually on activity of the pre- and post-synaptic neurons. Diversity of various learning regimes assumes that different forms of synaptic plasticity may be most efficient for, for example, unsupervised and supervised learning, as it is observed in living neurons demonstrating many kinds of deviations from the basic spike timing dependent plasticity (STDP) model. In the present paper, we formulate specific requirements to plasticity rules imposed by unsupervised learning problems and construct a novel plasticity model generalizing STDP and satisfying these requirements. This plasticity model serves as main logical component of the novel supervised learning algorithm called SCoBUL (Spike Correlation Based Unsupervised Learning) proposed in this work. We also present the results of computer simulation experiments confirming efficiency of these synaptic plasticity rules and the algorithm SCoBUL. </details>
<details>	<summary>邮件日期</summary>	2021年11月15日</details>

# 274、基于STDP的事件数据无监督Spiking实例分割
- [ ] Unsupervised Spiking Instance Segmentation on Event Data using STDP 
时间：2021年11月12日                         第一作者：Paul Kirkl                       [链接](https://arxiv.org/abs/2111.05283).                     
<details>	<summary>注释</summary>	20 Pages, 13 Figures </details>
<details>	<summary>邮件日期</summary>	2021年11月15日</details>

# 273、利用剩余脉冲神经网络进行精确特征提取的关键
- [ ] Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks 
时间：2021年11月12日                         第一作者：Alex Vicente-Sola                       [链接](https://arxiv.org/abs/2111.05955).                     
<details>	<summary>注释</summary>	13 pages, 5 figures, 14 tables ACM-class: I.2.6; I.2.10; I.4.8; I.5.2; D.2.13 </details>
<details>	<summary>邮件日期</summary>	2021年11月15日</details>

# 272、利用剩余脉冲神经网络进行精确特征提取的关键
- [ ] Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks 
时间：2021年11月10日                         第一作者：Alex Vicente-Sola (1)                       [链接](https://arxiv.org/abs/2111.05955).                     
## 摘要：脉冲神经网络（SNN）已成为传统人工神经网络（ANN）的一种有趣的替代方案，这得益于其时间处理能力以及在神经形态硬件中的低交换（大小、重量和功率）和节能实现。然而，训练SNN所涉及的挑战限制了它们在准确性方面的性能，从而限制了它们的应用。因此，改进学习算法和神经网络结构以获得更精确的特征提取是当前SNN研究的重点之一。在这篇文章中，我们对现代扣球体系结构的关键组件进行了研究。我们对从性能最好的网络中获取的图像分类数据集中的不同技术进行了经验比较。我们设计了一个成功剩余网络（ResNet）体系结构的脉冲版本，并在其上测试了不同的组件和训练策略。我们的研究结果为SNN设计提供了最先进的指导，在尝试构建最佳视觉特征提取器时，可以做出明智的选择。最后，我们的网络在CIFAR-10（94.1%）和CIFAR-100（74.5%）数据集中的性能优于以前的SNN体系结构，并与DVS-CIFAR10（71.3%）中的最新技术相匹配，参数比以前的最新技术更少，并且不需要ANN-SNN转换。代码可在https://github.com/VicenteAlex/Spiking_ResNet.
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have become an interesting alternative to conventional artificial neural networks (ANN) thanks to their temporal processing capabilities and their low-SWaP (Size, Weight, and Power) and energy efficient implementations in neuromorphic hardware. However the challenges involved in training SNNs have limited their performance in terms of accuracy and thus their applications. Improving learning algorithms and neural architectures for a more accurate feature extraction is therefore one of the current priorities in SNN research. In this paper we present a study on the key components of modern spiking architectures. We empirically compare different techniques in image classification datasets taken from the best performing networks. We design a spiking version of the successful residual network (ResNet) architecture and test different components and training strategies on it. Our results provide a state of the art guide to SNN design, which allows to make informed choices when trying to build the optimal visual feature extractor. Finally, our network outperforms previous SNN architectures in CIFAR-10 (94.1%) and CIFAR-100 (74.5%) datasets and matches the state of the art in DVS-CIFAR10 (71.3%), with less parameters than the previous state of the art and without the need for ANN-SNN conversion. Code available at https://github.com/VicenteAlex/Spiking_ResNet. </details>
<details>	<summary>注释</summary>	13 pages, 5 figures, 14 tables ACM-class: I.2.6; I.2.10; I.4.8; I.5.2; D.2.13 </details>
<details>	<summary>邮件日期</summary>	2021年11月12日</details>

# 271、从头开始训练低潜伏期深脉冲神经网络的批标准化方法
- [ ] Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch 
时间：2021年11月10日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2010.01729).                     
<details>	<summary>注释</summary>	Accepted to Frontiers in Neuroscience (2021) </details>
<details>	<summary>邮件日期</summary>	2021年11月12日</details>

# 270、基于STDP的事件数据无监督Spiking实例分割
- [ ] Unsupervised Spiking Instance Segmentation on Event Data using STDP 
时间：2021年11月09日                         第一作者：Paul Kirkl                       [链接](https://arxiv.org/abs/2111.05283).                     
## 摘要：脉冲神经网络（SNN）和神经形态工程领域带来了如何处理机器学习（ML）和计算机视觉（CV）问题的范式转变。这种范式的转变来自基于事件的感知和处理的适应。基于事件的视觉传感器允许生成与场景动态相关的稀疏和异步事件。不仅可以捕获空间信息，还可以捕获高保真的时间信息。同时避免了传统高帧速率方法的额外开销和冗余。然而，随着范式的变化，传统的CV和ML中的许多技术都不适用于这些基于事件的时空视觉流。因此，存在数量有限的识别、检测和分割方法。在本文中，我们提出了一种新的方法，只需使用训练用于对象识别的脉冲时间相关可塑性训练脉冲卷积神经网络的权值即可执行实例分割。这利用了网络内部特征表示的空间和时间方面，增加了这种新的鉴别能力。我们通过成功地将用于人脸检测的单类无监督网络转换为多人人脸识别和实例分割网络，突出了这一新功能。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) and the field of Neuromorphic Engineering has brought about a paradigm shift in how to approach Machine Learning (ML) and Computer Vision (CV) problem. This paradigm shift comes from the adaption of event-based sensing and processing. An event-based vision sensor allows for sparse and asynchronous events to be produced that are dynamically related to the scene. Allowing not only the spatial information but a high-fidelity of temporal information to be captured. Meanwhile avoiding the extra overhead and redundancy of conventional high frame rate approaches. However, with this change in paradigm, many techniques from traditional CV and ML are not applicable to these event-based spatial-temporal visual streams. As such a limited number of recognition, detection and segmentation approaches exist. In this paper, we present a novel approach that can perform instance segmentation using just the weights of a Spike Time Dependent Plasticity trained Spiking Convolutional Neural Network that was trained for object recognition. This exploits the spatial and temporal aspects of the network's internal feature representations adding this new discriminative capability. We highlight the new capability by successfully transforming a single class unsupervised network for face detection into a multi-person face recognition and instance segmentation network. </details>
<details>	<summary>注释</summary>	20 Pages, 13 Figures </details>
<details>	<summary>邮件日期</summary>	2021年11月10日</details>

# 269、稳定的终身学习：用脉冲神经元解决塑性神经网络的不稳定性
- [ ] Stable Lifelong Learning: Spiking neurons as a solution to instability in plastic neural networks 
时间：2021年11月07日                         第一作者：Samuel Schmidgall                       [链接](https://arxiv.org/abs/2111.04113).                     
## 摘要：突触可塑性是神经网络中自我调节的无监督学习的有力方法。最近，人们对利用人工神经网络（ANN）和突触可塑性进行终生学习的兴趣又重新兴起。可塑性已被证明可以提高这些网络的学习能力，从而推广到新的环境环境中。然而，这些经过训练的网络的长期稳定性尚未得到检验。这项工作表明，将可塑性与人工神经网络结合使用会导致不稳定性超过训练期间使用的预定寿命。这种不稳定性可能导致寻求回报行为的急剧下降，或迅速导致达到环境终端状态。这种行为被证明在两种不同的环境下，在许多训练时间范围内，对几种塑性规则是一致的：手推车杆平衡问题和四足运动问题。我们通过使用脉冲神经元来解决这种不稳定性。
<details>	<summary>英文摘要</summary>	Synaptic plasticity poses itself as a powerful method of self-regulated unsupervised learning in neural networks. A recent resurgence of interest has developed in utilizing Artificial Neural Networks (ANNs) together with synaptic plasticity for intra-lifetime learning. Plasticity has been shown to improve the learning capabilities of these networks in generalizing to novel environmental circumstances. However, the long-term stability of these trained networks has yet to be examined. This work demonstrates that utilizing plasticity together with ANNs leads to instability beyond the pre-specified lifespan used during training. This instability can lead to the dramatic decline of reward seeking behavior, or quickly lead to reaching environment terminal states. This behavior is shown to hold consistent for several plasticity rules on two different environments across many training time-horizons: a cart-pole balancing problem and a quadrupedal locomotion problem. We present a solution to this instability through the use of spiking neurons. </details>
<details>	<summary>邮件日期</summary>	2021年11月09日</details>

# 268、一种用于人工智能的长短时记忆在基于脉冲的神经形态硬件中的应用
- [ ] A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware 
时间：2021年11月07日                         第一作者：Philipp Plank                       [链接](https://arxiv.org/abs/2107.03992).                     
<details>	<summary>注释</summary>	Philipp Plank and Arjun Rao have contributed equally to this work as first authors </details>
<details>	<summary>邮件日期</summary>	2021年11月09日</details>

# 267、WaveSense：用于关键词识别的带脉冲神经网络的高效时间卷积
- [ ] WaveSense: Efficient Temporal Convolutions with Spiking Neural Networks for Keyword Spotting 
时间：2021年11月02日                         第一作者：Philipp Weidel                       [链接](https://arxiv.org/abs/2111.01456).                     
## 摘要：超低功耗局部信号处理是常开设备边缘应用的一个重要方面。模拟脉冲神经网络的神经形态处理器显示出强大的计算能力，同时满足该领域所需的有限功率预算。在这项工作中，我们提出脉冲神经动力学作为扩张的时间卷积的自然替代。我们将这一想法扩展到WaveSense，这是一种受WaveNet架构启发的脉冲神经网络。WaveSense使用简单的神经动力学、固定的时间常数和简单的前馈结构，因此特别适合于神经形态实现。我们在几个数据集上测试了该模型的关键字识别能力。结果表明，所提出的网络优于其他脉冲神经网络，达到了CNN和LSTMs等人工神经网络的最新性能。
<details>	<summary>英文摘要</summary>	Ultra-low power local signal processing is a crucial aspect for edge applications on always-on devices. Neuromorphic processors emulating spiking neural networks show great computational power while fulfilling the limited power budget as needed in this domain. In this work we propose spiking neural dynamics as a natural alternative to dilated temporal convolutions. We extend this idea to WaveSense, a spiking neural network inspired by the WaveNet architecture. WaveSense uses simple neural dynamics, fixed time-constants and a simple feed-forward architecture and hence is particularly well suited for a neuromorphic implementation. We test the capabilities of this model on several datasets for keyword-spotting. The results show that the proposed network beats the state of the art of other spiking neural networks and reaches near state-of-the-art performance of artificial neural networks such as CNNs and LSTMs. </details>
<details>	<summary>邮件日期</summary>	2021年11月03日</details>

# 266、带神经网络鉴别器的脉冲生成对抗网络：局部训练、贝叶斯模型和持续元学习
- [ ] Spiking Generative Adversarial Networks With a Neural Network Discriminator: Local Training, Bayesian Models, and Continual Meta-Learning 
时间：2021年11月02日                         第一作者：Bleema Rosenfeld                       [链接](https://arxiv.org/abs/2111.01750).                     
## 摘要：神经形态数据以由脉冲编码的时空模式携带信息。因此，神经形态计算中的一个中心问题是训练脉冲神经网络（SNN）以再现响应给定脉冲刺激的时空脉冲模式。大多数现有方法通过将每个输入分配给特定的期望输出脉冲序列，以确定性方式对SNN的输入-输出行为进行建模。相比之下，为了充分利用脉冲的时间编码能力，本工作建议训练SNN以匹配脉冲信号的分布，而不是单个脉冲信号。为此，本文介绍了一种新的混合结构，包括通过SNN实现的条件生成器和通过常规人工神经网络（ANN）实现的鉴别器。ANN的作用是在训练期间，按照生成性对抗网络（GANs）的原则，在对抗性迭代学习策略中向SNN提供反馈。为了更好地捕捉多模态时空分布，所提出的方法（称为SpikeGAN）被进一步扩展以支持生成器权重的贝叶斯学习。最后，通过提出Spikgan的在线元学习变体，解决了具有时变统计的设置问题。与基于（静态）信念网络和最大似然（或经验风险最小化）的现有解决方案相比，实验深入了解了所提出方法的优点。
<details>	<summary>英文摘要</summary>	Neuromorphic data carries information in spatio-temporal patterns encoded by spikes. Accordingly, a central problem in neuromorphic computing is training spiking neural networks (SNNs) to reproduce spatio-temporal spiking patterns in response to given spiking stimuli. Most existing approaches model the input-output behavior of an SNN in a deterministic fashion by assigning each input to a specific desired output spiking sequence. In contrast, in order to fully leverage the time-encoding capacity of spikes, this work proposes to train SNNs so as to match distributions of spiking signals rather than individual spiking signals. To this end, the paper introduces a novel hybrid architecture comprising a conditional generator, implemented via an SNN, and a discriminator, implemented by a conventional artificial neural network (ANN). The role of the ANN is to provide feedback during training to the SNN within an adversarial iterative learning strategy that follows the principle of generative adversarial network (GANs). In order to better capture multi-modal spatio-temporal distribution, the proposed approach -- termed SpikeGAN -- is further extended to support Bayesian learning of the generator's weight. Finally, settings with time-varying statistics are addressed by proposing an online meta-learning variant of SpikeGAN. Experiments bring insights into the merits of the proposed approach as compared to existing solutions based on (static) belief networks and maximum likelihood (or empirical risk minimization). </details>
<details>	<summary>邮件日期</summary>	2021年11月03日</details>

# 265、通过局部突触可塑性学习基于事件的时空特征描述符：计算机视觉的生物学现实视角
- [ ] Learning Event-based Spatio-Temporal Feature Descriptors via Local Synaptic Plasticity: A Biologically-realistic Perspective of Computer Vision 
时间：2021年11月01日                         第一作者：Ali Safa                       [链接](https://arxiv.org/abs/2111.00791).                     
## 摘要：我们提出了一个基于优化的理论，描述了在视觉皮层中经验观察到的具有棘波时间依赖性可塑性（STDP）学习的棘波皮层集合。使用我们的方法，我们为基于事件的摄像机构建了一类完全连接、卷积和基于动作的特征描述符，我们分别在N-MNIST、挑战性CIFAR10-DVS和IBM DVS128手势数据集上对其进行评估。与传统的基于事件的最先进特征描述符相比，我们报告了显著的准确性改进（在CIFAR10-DVS上为+8%）。我们报告，与最先进的基于STDP的系统相比，精确度有了很大提高（N-MNIST为+10%，IBM DVS128为+7.74%）。除了在神经形态边缘设备中进行超低功耗学习外，我们的工作还有助于为实现生物现实、基于优化的皮层视觉理论铺平道路。
<details>	<summary>英文摘要</summary>	We present an optimization-based theory describing spiking cortical ensembles equipped with Spike-Timing-Dependent Plasticity (STDP) learning, as empirically observed in the visual cortex. Using our methods, we build a class of fully-connected, convolutional and action-based feature descriptors for event-based camera that we respectively assess on N-MNIST, challenging CIFAR10-DVS and on the IBM DVS128 gesture dataset. We report significant accuracy improvements compared to conventional state-of-the-art event-based feature descriptors (+8% on CIFAR10-DVS). We report large improvements in accuracy compared to state-of-the-art STDP-based systems (+10% on N-MNIST, +7.74% on IBM DVS128 Gesture). In addition to ultra-low-power learning in neuromorphic edge devices, our work helps paving the way towards a biologically-realistic, optimization-based theory of cortical vision. </details>
<details>	<summary>邮件日期</summary>	2021年11月02日</details>

# 264、BioGrad：基于生物似有理梯度的脉冲神经网络学习
- [ ] BioGrad: Biologically Plausible Gradient-Based Learning for Spiking Neural Networks 
时间：2021年10月27日                         第一作者：Guangzhi Tang                       [链接](https://arxiv.org/abs/2110.14092).                     
## 摘要：在新兴的神经形态芯片的推动下，脉冲神经网络（SNN）正在为人工智能问题提供节能、大规模并行和低延迟的解决方案。为了利用这些计算优势，SNN需要通过遵循大脑启发的神经形态原理的学习算法进行训练，即基于事件、局部和在线计算。然而，最先进的SNN训练算法是基于backprop的，并不遵循上述原则。由于其有限的生物学合理性，将backprop应用于SNN需要非局部反馈路径来传输连续值误差，并且依赖于未来时间步长的梯度。对backprop引入生物学上合理的修改有助于克服其一些局限性，但限制了backprop的近似程度，从而妨碍了其性能。我们为SNN提出了一个生物学上合理的基于梯度的学习算法，该算法在功能上等同于backprop，同时遵守所有三个神经形态原则。我们引入了具有局部资格跟踪的多室脉冲神经元来计算学习所需的梯度，并引入了一个周期性的“睡眠”阶段来进一步改进对backprop的近似，在此期间，局部Hebbian规则将反馈和前馈权重对齐。我们的方法在MNIST（98.13%）和基于事件的N-MNIST（97.59%）数据集上实现了与backprop相同的性能水平。我们在Intel的Loihi上部署了我们的学习算法来为MNIST训练一个1隐藏层网络，获得了93.32%的测试精度，同时每个训练样本消耗的能量比GPU上的BioGrad少400倍。我们的工作表明，最佳学习在神经形态计算中是可行的，进一步追求其生物学合理性可以更好地抓住这一新兴计算范式的好处。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) are delivering energy-efficient, massively parallel, and low-latency solutions to AI problems, facilitated by the emerging neuromorphic chips. To harness these computational benefits, SNN need to be trained by learning algorithms that adhere to brain-inspired neuromorphic principles, namely event-based, local, and online computations. Yet, the state-of-the-art SNN training algorithms are based on backprop that does not follow the above principles. Due to its limited biological plausibility, the application of backprop to SNN requires non-local feedback pathways for transmitting continuous-valued errors, and relies on gradients from future timesteps. The introduction of biologically plausible modifications to backprop has helped overcome several of its limitations, but limits the degree to which backprop is approximated, which hinders its performance. We propose a biologically plausible gradient-based learning algorithm for SNN that is functionally equivalent to backprop, while adhering to all three neuromorphic principles. We introduced multi-compartment spiking neurons with local eligibility traces to compute the gradients required for learning, and a periodic "sleep" phase to further improve the approximation to backprop during which a local Hebbian rule aligns the feedback and feedforward weights. Our method achieved the same level of performance as backprop with multi-layer fully connected SNN on MNIST (98.13%) and the event-based N-MNIST (97.59%) datasets. We deployed our learning algorithm on Intel's Loihi to train a 1-hidden-layer network for MNIST, and obtained 93.32% test accuracy while consuming 400 times less energy per training sample than BioGrad on GPU. Our work shows that optimal learning is feasible in neuromorphic computing, and further pursuing its biological plausibility can better capture the benefits of this emerging computing paradigm. </details>
<details>	<summary>注释</summary>	14 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月28日</details>

# 263、利用星形胶质细胞调制塑性组织的混沌动力学边缘提高液体状态机性能
- [ ] Increasing Liquid State Machine Performance with Edge-of-Chaos Dynamics Organized by Astrocyte-modulated Plasticity 
时间：2021年10月26日                         第一作者：Vladimir A. Ivanov                       [链接](https://arxiv.org/abs/2111.01760).                     
## 摘要：液体状态机（LSM）结合了低训练复杂度和生物合理性，这使得它成为边缘和神经形态计算范式的一个有吸引力的机器学习框架。LSM最初是作为大脑计算模型提出的，它调整其内部权值时没有梯度的反向传播，这导致与多层神经网络相比性能较低。神经科学的最新发现表明，星形胶质细胞，一种长期被忽视的非神经性脑细胞，调节突触可塑性和大脑动力学，将大脑网络调整到有序和混沌之间计算最优临界相变附近。受这种对大脑网络如何自我调节的颠覆性理解的启发，我们提出了神经元-星形胶质细胞液体状态机（NALSM），它通过自组织的近临界动力学来解决性能低下的问题。与生物模型类似，星形胶质细胞模型整合了神经元活动，并向棘波时间依赖性可塑性（STDP）提供全局反馈，STDP围绕与混沌边缘相关的临界分支因子自组织NALSM动力学。我们证明，与可比的LSM方法相比，NALSM实现了最先进的精度，而不需要数据特定的手动调整。NALSM在MNIST、N-MNIST和Fashion MNIST上的最高准确率分别为97.61%、97.51%和85.84%，与当前通过反向传播训练的全连接多层脉冲神经网络相比，NALSM取得了相当的性能。我们的研究结果表明，大脑启发的机器学习方法的进一步发展有可能达到深度学习的性能，并在边缘支持鲁棒和节能的神经形态计算。
<details>	<summary>英文摘要</summary>	The liquid state machine (LSM) combines low training complexity and biological plausibility, which has made it an attractive machine learning framework for edge and neuromorphic computing paradigms. Originally proposed as a model of brain computation, the LSM tunes its internal weights without backpropagation of gradients, which results in lower performance compared to multi-layer neural networks. Recent findings in neuroscience suggest that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. Inspired by this disruptive understanding of how brain networks self-tune, we propose the neuron-astrocyte liquid state machine (NALSM) that addresses under-performance through self-organized near-critical dynamics. Similar to its biological counterpart, the astrocyte model integrates neuronal activity and provides global feedback to spike-timing-dependent plasticity (STDP), which self-organizes NALSM dynamics around a critical branching factor that is associated with the edge-of-chaos. We demonstrate that NALSM achieves state-of-the-art accuracy versus comparable LSM methods, without the need for data-specific hand-tuning. With a top accuracy of 97.61% on MNIST, 97.51% on N-MNIST, and 85.84% on Fashion-MNIST, NALSM achieved comparable performance to current fully-connected multi-layer spiking neural networks trained via backpropagation. Our findings suggest that the further development of brain-inspired machine learning methods has the potential to reach the performance of deep learning, with the added benefits of supporting robust and energy-efficient neuromorphic computing on the edge. </details>
<details>	<summary>注释</summary>	23 pages, 9 figures, NeurIPS 2021 Journal-ref: 35th Conference on Neural Information Processing Systems (NeurIPS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年11月03日</details>

# 262、基于事件的脉冲神经网络光流自监督学习
- [ ] Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks 
时间：2021年10月25日                         第一作者：Jesse Hagenaars                       [链接](https://arxiv.org/abs/2106.01862).                     
<details>	<summary>注释</summary>	Accepted at NeurIPS 2021; code and additional material can be found at https://mavlab.tudelft.nl/event_flow/ </details>
<details>	<summary>邮件日期</summary>	2021年10月27日</details>

# 261、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年10月25日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>注释</summary>	Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 260、脉冲神经形态芯片学习纠缠量子态
- [ ] Spiking neuromorphic chip learns entangled quantum states 
时间：2021年10月25日                         第一作者：Stefanie Czischek                       [链接](https://arxiv.org/abs/2008.01039).                     
<details>	<summary>注释</summary>	9+13 pages, 4+2 figures; Submission to SciPost </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 259、平衡态隐式微分训练反馈脉冲神经网络
- [ ] Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State 
时间：2021年10月24日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2109.14247).                     
<details>	<summary>注释</summary>	Accepted by NeurIPS 2021 (Spotlight) </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 258、ES ImageNet：一个用于脉冲神经网络的百万事件流分类数据集
- [ ] ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks 
时间：2021年10月23日                         第一作者：Yihan Lin                       [链接](https://arxiv.org/abs/2110.12211).                     
## 摘要：随着事件驱动算法，特别是脉冲神经网络（SNN）在神经形态视觉处理方面的不断改进，迫切需要一个更具挑战性的事件流数据集。然而，众所周知，使用像动态视觉传感器（DVS）这样的神经形态摄像机创建ES数据集既耗时又昂贵。在这项工作中，我们提出了一种称为全向离散梯度（ODG）的快速有效算法，将流行的计算机视觉数据集ILSVRC2012转换为其事件流（ES）版本，将大约1300000帧图像生成1000个类别的ES样本。这样，我们提出了一个称为ES ImageNet的ES数据集，它比目前其他神经形态分类数据集大几十倍，完全由软件生成。ODG算法实现图像运动，利用不同方向上的离散梯度信息生成局部值变化，为将基于帧的图像转换为事件流提供了一种低成本、高速的方法，并使用边缘积分从事件流重建高质量图像。此外，我们以多种方式分析了ES ImageNet的统计数据，并使用著名的深度神经网络算法和脉冲神经网络算法提供了数据集的性能基准。我们相信这项工作将为SNN和神经形态视觉提供一个新的大规模基准数据集。
<details>	<summary>英文摘要</summary>	With event-driven algorithms, especially the spiking neural networks (SNNs), achieving continuous improvement in neuromorphic vision processing, a more challenging event-stream-dataset is urgently needed. However, it is well known that creating an ES-dataset is a time-consuming and costly task with neuromorphic cameras like dynamic vision sensors (DVS). In this work, we propose a fast and effective algorithm termed Omnidirectional Discrete Gradient (ODG) to convert the popular computer vision dataset ILSVRC2012 into its event-stream (ES) version, generating about 1,300,000 frame-based images into ES-samples in 1000 categories. In this way, we propose an ES-dataset called ES-ImageNet, which is dozens of times larger than other neuromorphic classification datasets at present and completely generated by the software. The ODG algorithm implements an image motion to generate local value changes with discrete gradient information in different directions, providing a low-cost and high-speed way for converting frame-based images into event streams, along with Edge-Integral to reconstruct the high-quality images from event streams. Furthermore, we analyze the statistics of the ES-ImageNet in multiple ways, and a performance benchmark of the dataset is also provided using both famous deep neural network algorithms and spiking neural network algorithms. We believe that this work shall provide a new large-scale benchmark dataset for SNNs and neuromorphic vision. </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 257、基于时间到第一脉冲编码的脉冲神经网络的硬件实现
- [ ] Hardware Implementation of Spiking Neural Networks Using Time-To-First-Spike Encoding 
时间：2021年10月22日                         第一作者：Seongbin Oh                       [链接](https://arxiv.org/abs/2006.05033).                     
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 256、基于生物似然时空调整的反向传播训练深脉冲神经网络
- [ ] Backpropagation with Biologically Plausible Spatio-Temporal Adjustment For Training Deep Spiking Neural Networks 
时间：2021年10月22日                         第一作者：Guobin Shen                       [链接](https://arxiv.org/abs/2110.08858).                     
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 255、一种用于脉冲神经网络静态图像编码的自适应采样和边缘检测方法
- [ ] An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks 
时间：2021年10月19日                         第一作者：Peyton Ch                       [链接](https://arxiv.org/abs/2110.10217).                     
## 摘要：目前使用卷积神经网络进行图像分类的最新方法通常受到延迟和功耗的限制。这就限制了可以采用这些方法的设备，尤其是低功耗边缘设备。脉冲神经网络（SNN）被认为是第三代人工神经网络，其目的是通过从生物神经元通信过程中获得灵感来解决这些延迟和功率限制。然而，在将图像等数据输入SNN之前，必须先将其编码到脉冲序列中。在此，我们提出了一种使用边缘检测将静态图像编码为时间脉冲序列的方法和一种用于SNN的自适应信号采样方法。边缘检测过程包括首先对2D静态图像执行Canny边缘检测，然后使用图像-信号转换方法将检测到的边缘图像转换为两个X和Y信号。自适应信令方法包括对信号进行采样，使信号保持足够的细节，并对信号中的突变敏感。然后，诸如基于阈值的表示（TBR）和前向步进（SF）之类的时间编码机制能够用于将采样信号转换为脉冲序列。我们使用各种错误和指标来优化和评估所提出的图像编码方法的效率和精度。与传统SF和TBR编码相比，使用边缘检测和自适应时间编码机制生成的脉冲序列的原始信号和重构信号之间的比较结果显示，在用于编码MNIST数据集时，平均均方根误差（RMSE）分别减少了18倍和7倍。
<details>	<summary>英文摘要</summary>	Current state-of-the-art methods of image classification using convolutional neural networks are often constrained by both latency and power consumption. This places a limit on the devices, particularly low-power edge devices, that can employ these methods. Spiking neural networks (SNNs) are considered to be the third generation of artificial neural networks which aim to address these latency and power constraints by taking inspiration from biological neuronal communication processes. Before data such as images can be input into an SNN, however, they must be first encoded into spike trains. Herein, we propose a method for encoding static images into temporal spike trains using edge detection and an adaptive signal sampling method for use in SNNs. The edge detection process consists of first performing Canny edge detection on the 2D static images and then converting the edge detected images into two X and Y signals using an image-to-signal conversion method. The adaptive signaling approach consists of sampling the signals such that the signals maintain enough detail and are sensitive to abrupt changes in the signal. Temporal encoding mechanisms such as threshold-based representation (TBR) and step-forward (SF) are then able to be used to convert the sampled signals into spike trains. We use various error and indicator metrics to optimize and evaluate the efficiency and precision of the proposed image encoding approach. Comparison results between the original and reconstructed signals from spike trains generated using edge-detection and adaptive temporal encoding mechanism exhibit 18x and 7x reduction in average root mean square error (RMSE) compared to the conventional SF and TBR encoding, respectively, while used for encoding MNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2021年10月22日</details>

# 254、HIRE-SNN：通过使用精心设计的输入噪声进行训练，利用节能深脉冲神经网络固有的鲁棒性
- [ ] HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise 
时间：2021年10月06日                         第一作者：Souvik Kundu                       [链接](https://arxiv.org/abs/2110.11417).                     
## 摘要：低潜伏期深脉冲神经网络（SNN）已成为传统人工神经网络（ANN）的一种很有前途的替代方案，因为它们在事件驱动的神经形态硬件上具有提高能量效率的潜力。然而，包括SNN在内的神经网络会受到各种对抗性攻击，因此必须对其进行训练，以便在许多应用中保持对此类攻击的弹性。然而，由于与SNN相关的高昂训练成本，在各种对抗性攻击下对深层SNN的分析和优化在很大程度上被忽略了。在本文中，我们首先详细分析了低延迟SNN对流行的基于梯度的攻击的固有鲁棒性，即快速梯度符号法（FGSM）和投影梯度下降法（PGD）。受此分析的启发，为了利用模型的鲁棒性抵御这些攻击，我们提出了一种SNN训练算法，该算法使用精心编制的输入噪声，并且不需要额外的训练时间。为了评估我们算法的优点，我们在CIFAR-10和CIFAR-100数据集上对VGG和ResNet的变体进行了广泛的实验。与标准训练的直接输入SNN相比，我们训练的模型在FGSM和PGD攻击生成的图像上的分类精度分别提高了13.7%和10.1%，而干净图像的精度损失可以忽略不计。我们的模型也优于在速率编码输入上训练的固有鲁棒SNN，在攻击生成的图像上具有改进的或类似的分类性能，同时延迟和计算能量分别降低25倍和4.6倍。
<details>	<summary>英文摘要</summary>	Low-latency deep spiking neural networks (SNNs) have become a promising alternative to conventional artificial neural networks (ANNs) because of their potential for increased energy efficiency on event-driven neuromorphic hardware. Neural networks, including SNNs, however, are subject to various adversarial attacks and must be trained to remain resilient against such attacks for many applications. Nevertheless, due to prohibitively high training costs associated with SNNs, analysis, and optimization of deep SNNs under various adversarial attacks have been largely overlooked. In this paper, we first present a detailed analysis of the inherent robustness of low-latency SNNs against popular gradient-based attacks, namely fast gradient sign method (FGSM) and projected gradient descent (PGD). Motivated by this analysis, to harness the model robustness against these attacks we present an SNN training algorithm that uses crafted input noise and incurs no additional training time. To evaluate the merits of our algorithm, we conducted extensive experiments with variants of VGG and ResNet on both CIFAR-10 and CIFAR-100 datasets. Compared to standard trained direct input SNNs, our trained models yield improved classification accuracy of up to 13.7% and 10.1% on FGSM and PGD attack-generated images, respectively, with negligible loss in clean image accuracy. Our models also outperform inherently robust SNNs trained on rate-coded inputs with improved or similar classification performance on attack-generated images while having up to 25x and 4.6x lower latency and computation energy, respectively. </details>
<details>	<summary>注释</summary>	10 pages, 11 figures, 7 tables, International Conference on Computer Vision </details>
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 253、基于生物似然时空调整的反向传播训练深脉冲神经网络
- [ ] Backpropagation with Biologically Plausible Spatio-Temporal Adjustment For Training Deep Spiking Neural Networks 
时间：2021年10月17日                         第一作者：Guobin Shen                       [链接](https://arxiv.org/abs/2110.08858).                     
## 摘要：脉冲神经网络（SNN）模拟人脑中的信息处理操作，在包含丰富时空信息的脉冲序列中表示和传输信息，在许多认知任务中表现出优异的性能。此外，事件驱动的信息处理使得在神经形态芯片上实现节能。深度学习的成功离不开反向传播。由于离散的信息传输，直接将反向传播应用于SNN的训练与传统的深度神经网络相比仍存在性能差距。此外，为了获得更好的性能，需要大量的模拟时间，这会导致较高的延迟。为了解决这些问题，我们提出了一种生物学上合理的空间调整方法，它重新考虑了膜电位和峰值之间的关系，并实现了对不同时间步长梯度的合理调整。它精确地控制误差沿空间维度的反向传播。其次，我们提出了一种生物学上合理的时间调整方法，使误差在时间维度上跨棘波传播，从而克服了传统棘波神经元在单个棘波周期内的时间依赖性问题。我们已经在多个数据集上验证了我们的算法，实验结果表明，我们的算法大大减少了网络延迟和能耗，同时也提高了网络性能。我们在神经形态数据集N-MNIST、DVS手势和DVS-CIFAR10上取得了最先进的性能。对于静态数据集MNIST和CIFAR10，我们已经超过了大多数传统的SNN反向传播训练算法，并取得了相对优越的性能。
<details>	<summary>英文摘要</summary>	The spiking neural network (SNN) mimics the information processing operation in the human brain, represents and transmits information in spike trains containing wealthy spatial and temporal information, and shows superior performance on many cognitive tasks. In addition, the event-driven information processing enables the energy-efficient implementation on neuromorphic chips. The success of deep learning is inseparable from backpropagation. Due to the discrete information transmission, directly applying the backpropagation to the training of the SNN still has a performance gap compared with the traditional deep neural networks. Also, a large simulation time is required to achieve better performance, which results in high latency. To address the problems, we propose a biological plausible spatial adjustment, which rethinks the relationship between membrane potential and spikes and realizes a reasonable adjustment of gradients to different time steps. And it precisely controls the backpropagation of the error along the spatial dimension. Secondly, we propose a biologically plausible temporal adjustment making the error propagate across the spikes in the temporal dimension, which overcomes the problem of the temporal dependency within a single spike period of the traditional spiking neurons. We have verified our algorithm on several datasets, and the experimental results have shown that our algorithm greatly reduces the network latency and energy consumption while also improving network performance. We have achieved state-of-the-art performance on the neuromorphic datasets N-MNIST, DVS-Gesture, and DVS-CIFAR10. For the static datasets MNIST and CIFAR10, we have surpassed most of the traditional SNN backpropagation training algorithm and achieved relatively superior performance. </details>
<details>	<summary>邮件日期</summary>	2021年10月19日</details>

# 252、HyperSeed：矢量符号体系结构的无监督学习
- [ ] HyperSeed: Unsupervised Learning with Vector Symbolic Architectures 
时间：2021年10月15日                         第一作者：Evgeny Osipov                       [链接](https://arxiv.org/abs/2110.08343).                     
## 摘要：受生物神经形态硬件最新创新的启发，本文提出了一种新的无监督机器学习方法Hyperseed，该方法利用向量符号体系结构（VSA）快速学习未标记数据的拓扑保持特征映射。它依赖于VSAs的两个主要功能：绑定操作和叠加计算。在本文中，我们介绍了在傅里叶全息约化表示VSA模型中表示的Hyperseed的算法部分，它特别适合于在脉冲神经形态硬件上实现。Hyperseed算法有两个独特的新颖之处：1）仅从少量输入数据样本进行学习；2）基于单个向量运算的学习规则。这些特性在合成数据集以及示例性基准用例、虹膜分类和使用n-gram统计的语言识别任务上得到了演示。
<details>	<summary>英文摘要</summary>	Motivated by recent innovations in biologically-inspired neuromorphic hardware, this paper presents a novel unsupervised machine learning approach named Hyperseed that leverages Vector Symbolic Architectures (VSA) for fast learning a topology preserving feature map of unlabelled data. It relies on two major capabilities of VSAs: the binding operation and computing in superposition. In this paper, we introduce the algorithmic part of Hyperseed expressed within Fourier Holographic Reduced Representations VSA model, which is specifically suited for implementation on spiking neuromorphic hardware. The two distinctive novelties of the Hyperseed algorithm are: 1) Learning from only few input data samples and 2) A learning rule based on a single vector operation. These properties are demonstrated on synthetic datasets as well as on illustrative benchmark use-cases, IRIS classification and a language identification task using n-gram statistics. </details>
<details>	<summary>邮件日期</summary>	2021年10月19日</details>

# 251、进化脉冲神经元细胞自动机和网络模拟体外神经元活动
- [ ] Evolving spiking neuron cellular automata and networks to emulate in vitro neuronal activity 
时间：2021年10月15日                         第一作者：J{\o}rgen Jensen Farner                       [链接](https://arxiv.org/abs/2110.08242).                     
## 摘要：神经启发模型和系统在非传统计算中有着巨大的应用潜力。通常，生物神经元的机制在模拟或物理系统中被建模或模仿，试图利用大脑的一些计算能力。然而，在神经系统中起作用的生物机制是复杂的，很难捕捉和设计；因此，采用数据驱动的方法将神经行为的特征转移到人工基底上会更简单。在本研究中，我们使用进化算法（EA）在体外产生模拟生物神经元行为模式的脉冲神经系统。该方法的目的是开发一种生成能够显示复杂行为的模型的方法，该模型可能适合用作计算基础。我们的模型能够产生一定程度的网络范围的同步性，并根据用于其进化的目标数据（来自一系列神经元培养密度和成熟度）显示出一系列行为。顶级模型的基因组表明，模型中连接的兴奋性和密度在决定所产生活动的复杂性方面起着重要作用。
<details>	<summary>英文摘要</summary>	Neuro-inspired models and systems have great potential for applications in unconventional computing. Often, the mechanisms of biological neurons are modeled or mimicked in simulated or physical systems in an attempt to harness some of the computational power of the brain. However, the biological mechanisms at play in neural systems are complicated and challenging to capture and engineer; thus, it can be simpler to turn to a data-driven approach to transfer features of neural behavior to artificial substrates. In the present study, we used an evolutionary algorithm (EA) to produce spiking neural systems that emulate the patterns of behavior of biological neurons in vitro. The aim of this approach was to develop a method of producing models capable of exhibiting complex behavior that may be suitable for use as computational substrates. Our models were able to produce a level of network-wide synchrony and showed a range of behaviors depending on the target data used for their evolution, which was from a range of neuronal culture densities and maturities. The genomes of the top-performing models indicate the excitability and density of connections in the model play an important role in determining the complexity of the produced activity. </details>
<details>	<summary>注释</summary>	To be published in proceedings of IEEE SSCI 2021 as part of ICES symposium (International Conference on Evolvable Systems, IEEE Symposium Series on Computational Intelligence 2021) </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 250、超越分类：直接训练用于语义分割的脉冲神经网络
- [ ] Beyond Classification: Directly Training Spiking Neural Networks for Semantic Segmentation 
时间：2021年10月14日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2110.07742).                     
## 摘要：脉冲神经网络（SNN）由于其稀疏、异步和二进制事件驱动的处理，最近成为人工神经网络（ANN）的低功耗替代品。由于其能源效率，SNN很有可能被部署到现实世界中资源受限的系统中，如自动驾驶车辆和无人机。然而，由于SNN的不可微性和复杂的神经元动力学特性，以往的SNN优化方法大多局限于图像识别。在本文中，我们探讨了SNN在分类之外的应用，并给出了配置有脉冲神经元的语义分割网络。具体来说，我们首先研究了两种有代表性的SNN优化技术（即ANN-SNN转换和代理梯度学习）在语义分割数据集上的识别任务。我们观察到，当从ANN转换时，SNN由于特征的空间差异而遭受高延迟和低性能。因此，我们使用替代梯度学习直接训练网络，比ANN-SNN转换具有更低的延迟和更高的性能。此外，我们为SNN域重新设计了两种基本的ANN分段结构（即完全卷积网络和DeepLab）。我们在两个公共语义分割基准上进行了实验，包括PASCAL VOC2012数据集和DDD17基于事件的数据集。除了说明SNN用于语义分割的可行性外，我们还表明SNN在该领域比ANN更具鲁棒性和能量效率。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have recently emerged as the low-power alternative to Artificial Neural Networks (ANNs) because of their sparse, asynchronous, and binary event-driven processing. Due to their energy efficiency, SNNs have a high possibility of being deployed for real-world, resource-constrained systems such as autonomous vehicles and drones. However, owing to their non-differentiable and complex neuronal dynamics, most previous SNN optimization methods have been limited to image recognition. In this paper, we explore the SNN applications beyond classification and present semantic segmentation networks configured with spiking neurons. Specifically, we first investigate two representative SNN optimization techniques for recognition tasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic segmentation datasets. We observe that, when converted from ANNs, SNNs suffer from high latency and low performance due to the spatial variance of features. Therefore, we directly train networks with surrogate gradient learning, resulting in lower latency and higher performance than ANN-SNN conversion. Moreover, we redesign two fundamental ANN segmentation architectures (i.e., Fully Convolutional Networks and DeepLab) for the SNN domain. We conduct experiments on two public semantic segmentation benchmarks including the PASCAL VOC2012 dataset and the DDD17 event-based dataset. In addition to showing the feasibility of SNNs for semantic segmentation, we show that SNNs can be more robust and energy-efficient compared to their ANN counterparts in this domain. </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 249、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年10月14日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>注释</summary>	Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 248、一种训练脉冲神经网络的时间编码方法
- [ ] A Time Encoding approach to training Spiking Neural Networks 
时间：2021年10月13日                         第一作者：Karen Adam                       [链接](https://arxiv.org/abs/2110.06735).                     
## 摘要：虽然脉冲神经网络（SNN）越来越流行，但用于训练它们的算法似乎不足以解决与经典人工神经网络（ANN）相同的任务。在本文中，我们提供了一个额外的工具来帮助我们理解和训练SNN使用理论从时间编码领域。时间编码机（TEM）可以用来对神经元进行建模、整合和激发，并且具有很好的重建特性。我们将看到如何从TEM领域获得灵感，将SNN的峰值时间解释为SNN权重矩阵的约束。更具体地说，我们研究如何通过解决一组线性约束来训练单层SNN，以及如何通过利用SNN发出的脉冲的全部或无和异步特性来训练双层SNN。脉冲的这些特性导致了反向传播的替代方案，这在经典ANN中同时和分级激活的情况下是不可能的。
<details>	<summary>英文摘要</summary>	While Spiking Neural Networks (SNNs) have been gaining in popularity, it seems that the algorithms used to train them are not powerful enough to solve the same tasks as those tackled by classical Artificial Neural Networks (ANNs). In this paper, we provide an extra tool to help us understand and train SNNs by using theory from the field of time encoding. Time encoding machines (TEMs) can be used to model integrate-and-fire neurons and have well-understood reconstruction properties. We will see how one can take inspiration from the field of TEMs to interpret the spike times of SNNs as constraints on the SNNs' weight matrices. More specifically, we study how to train one-layer SNNs by solving a set of linear constraints, and how to train two-layer SNNs by leveraging the all-or-none and asynchronous properties of the spikes emitted by SNNs. These properties of spikes result in an alternative to backpropagation which is not possible in the case of simultaneous and graded activations as in classical ANNs. </details>
<details>	<summary>注释</summary>	5 pages, 5 figures, submitted to IEEE ICASSP 2022 </details>
<details>	<summary>邮件日期</summary>	2021年10月14日</details>

# 247、一种优化的无梯度深脉冲神经网络结构
- [ ] An optimised deep spiking neural network architecture without gradients 
时间：2021年10月12日                         第一作者：Yeshwanth Bethi                       [链接](https://arxiv.org/abs/2109.12813).                     
<details>	<summary>注释</summary>	18 pages, 6 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年10月13日</details>

# 246、SCFlow：用于脉冲相机的光流估计
- [ ] SCFlow: Optical Flow Estimation for Spiking Camera 
时间：2021年10月08日                         第一作者：Liwen Hu                       [链接](https://arxiv.org/abs/2110.03916).                     
## 摘要：作为一种具有高时间分辨率的仿生传感器，脉冲相机在实际应用中有着巨大的潜力，特别是在高速场景中的运动估计方面。光流估计在基于图像和基于事件的视觉中取得了显著的成功，但现有的方法不能直接应用于脉冲摄像机的脉冲流。传统的光流算法不能很好地匹配脉冲流数据。本文提出了一种新的用于脉冲相机光流估计的深度学习管道SCFlow。重要的是，我们引入了给定脉冲流的适当输入表示，该脉冲流作为唯一输入馈入SCFlow。我们介绍\textit{first}脉冲相机模拟器（SPCS）。此外，基于SPCS，我们首先提出了两个用于脉冲相机的光流数据集（脉冲飞行物和照片真实感高速运动，分别表示为SPIFT和PHM），对应于随机高速和精心设计的场景。实验结果表明，SCFlow可以预测不同高速场景中脉冲流的光流，并在数据集上表现出优于现有方法的优势\textit{所有代码和构造的数据集将在发布后发布}。
<details>	<summary>英文摘要</summary>	As a bio-inspired sensor with high temporal resolution, Spiking camera has an enormous potential in real applications, especially for motion estimation in high-speed scenes. Optical flow estimation has achieved remarkable success in image-based and event-based vision, but % existing methods cannot be directly applied in spike stream from spiking camera. conventional optical flow algorithms are not well matched to the spike stream data. This paper presents, SCFlow, a novel deep learning pipeline for optical flow estimation for spiking camera. Importantly, we introduce an proper input representation of a given spike stream, which is fed into SCFlow as the sole input. We introduce the \textit{first} spiking camera simulator (SPCS). Furthermore, based on SPCS, we first propose two optical flow datasets for spiking camera (SPIkingly Flying Things and Photo-realistic High-speed Motion, denoted as SPIFT and PHM respectively) corresponding to random high-speed and well-designed scenes. Empirically, we show that the SCFlow can predict optical flow from spike stream in different high-speed scenes, and express superiority to existing methods on the datasets. \textit{All codes and constructed datasets will be released after publication}. </details>
<details>	<summary>注释</summary>	The first two authors contributed equally </details>
<details>	<summary>邮件日期</summary>	2021年10月11日</details>

# 245、你只需要一个时间步：训练超低延迟的脉冲神经网络
- [ ] One Timestep is All You Need: Training Spiking Neural Networks with Ultra Low Latency 
时间：2021年10月01日                         第一作者：Sayeed Shafayet Chowdhury                       [链接](https://arxiv.org/abs/2110.05929).                     
## 摘要：脉冲神经网络（SNN）是常用深度神经网络（DNN）的节能替代方案。通过事件驱动的信息处理，SNN可以大大降低DNN昂贵的计算需求，同时实现相当的性能。然而，高推断延迟是深度SNN边缘部署的一个重要障碍。在多个时间步上进行计算不仅会增加延迟以及由于操作数量增加而导致的总体能量预算，而且还会导致获取膜电位的内存访问开销，这两种情况都会降低SNN的能量效益。为了克服这一瓶颈并充分利用SNN的潜力，我们提出了一种迭代初始化和重新训练SNN的方法（IIR-SNN），在时间轴上执行单次激发推理。该方法从使用T个时间步长（T>1）训练的SNN开始。然后，在减少延迟的每个阶段，使用前一阶段训练的具有较高时间步长的网络作为具有较低时间步长的后续训练的初始化。这是一种压缩方法，因为网络在时域中逐渐缩小。在本文中，我们使用直接输入编码并选择T=5，因为根据文献，它是在ImageNet上实现令人满意的性能所需的最小延迟。提出的方案允许我们获得高达单位延迟的SNN，在推理过程中需要一次前向传递。使用VGG16，我们在CIFAR-10、CIFAR-100和ImageNet上分别实现了93.05%、70.15%和67.71%的顶级精度，只需1个时间步。此外，与其他最先进的SNN相比，IIR SNN执行推理的延迟减少了5-2500倍，保持了相当甚至更好的准确性。此外，与标准DNN相比，建议的IIR SNN提供了25-33倍的能源效率，同时在分类性能方面与标准DNN相当。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are energy efficient alternatives to commonly used deep neural networks (DNNs). Through event-driven information processing, SNNs can reduce the expensive compute requirements of DNNs considerably, while achieving comparable performance. However, high inference latency is a significant hindrance to the edge deployment of deep SNNs. Computation over multiple timesteps not only increases latency as well as overall energy budget due to higher number of operations, but also incurs memory access overhead of fetching membrane potentials, both of which lessen the energy benefits of SNNs. To overcome this bottleneck and leverage the full potential of SNNs, we propose an Iterative Initialization and Retraining method for SNNs (IIR-SNN) to perform single shot inference in the temporal axis. The method starts with an SNN trained with T timesteps (T>1). Then at each stage of latency reduction, the network trained at previous stage with higher timestep is utilized as initialization for subsequent training with lower timestep. This acts as a compression method, as the network is gradually shrunk in the temporal domain. In this paper, we use direct input encoding and choose T=5, since as per literature, it is the minimum required latency to achieve satisfactory performance on ImageNet. The proposed scheme allows us to obtain SNNs with up to unit latency, requiring a single forward pass during inference. We achieve top-1 accuracy of 93.05%, 70.15% and 67.71% on CIFAR-10, CIFAR-100 and ImageNet, respectively using VGG16, with just 1 timestep. In addition, IIR-SNNs perform inference with 5-2500X reduced latency compared to other state-of-the-art SNNs, maintaining comparable or even better accuracy. Furthermore, in comparison with standard DNNs, the proposed IIR-SNNs provide25-33X higher energy efficiency, while being comparable to them in classification performance. </details>
<details>	<summary>邮件日期</summary>	2021年10月13日</details>

# 244、基于事件视觉的脉冲卷积网络对抗攻击
- [ ] Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision 
时间：2021年10月06日                         第一作者：Julian B\"uchel                       [链接](https://arxiv.org/abs/2110.02929).                     
## 摘要：使用动态视觉传感器的基于事件的传感在低功耗视觉应用中获得了发展。脉冲神经网络能够很好地处理基于事件的数据的稀疏性，适合部署在低功耗的神经形态硬件上。作为一个新兴领域，脉冲神经网络对潜在恶意对手攻击的敏感性迄今为止很少受到关注。在这项工作中，我们展示了白盒对抗攻击算法如何适应基于事件的视觉数据的离散和稀疏性质，以及如何适应脉冲神经网络的连续时间设置。我们在N-MNIST和IBM手势神经形态视觉数据集上测试了我们的方法，结果表明，通过注入相对较少的适当位置的事件，对抗性干扰可以获得较高的成功率。我们还首次验证了这些扰动直接作用于神经形态硬件的有效性。最后，我们讨论了由此产生的扰动的性质和未来可能的方向。
<details>	<summary>英文摘要</summary>	Event-based sensing using dynamic vision sensors is gaining traction in low-power vision applications. Spiking neural networks work well with the sparse nature of event-based data and suit deployment on low-power neuromorphic hardware. Being a nascent field, the sensitivity of spiking neural networks to potentially malicious adversarial attacks has received very little attention so far. In this work, we show how white-box adversarial attack algorithms can be adapted to the discrete and sparse nature of event-based visual data, and to the continuous-time setting of spiking neural networks. We test our methods on the N-MNIST and IBM Gestures neuromorphic vision datasets and show adversarial perturbations achieve a high success rate, by injecting a relatively small number of appropriately placed events. We also verify, for the first time, the effectiveness of these perturbations directly on neuromorphic hardware. Finally, we discuss the properties of the resulting perturbations and possible future directions. </details>
<details>	<summary>注释</summary>	16 pages, preprint, submitted to ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2021年10月07日</details>

# 243、快速精确递归神经网络的脉冲激励秩编码
- [ ] Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks 
时间：2021年10月06日                         第一作者：Alan Jeffares                       [链接](https://arxiv.org/abs/2110.02865).                     
## 摘要：生物脉冲神经网络（SNN）可以在其输出中对信息进行时间编码，例如按照神经元激发的顺序，而人工神经网络（ANN）通常不这样做。因此，在处理时间输入时，神经形态计算的SNN模型被认为可能比ANN更快速有效。另一方面，人工神经网络更易于训练，通常可获得优异的性能。在这里，我们表明，时态编码（如SNN启发的秩编码（RC））也可以应用于传统的人工神经网络（如LSTM），并导致计算节省和加速。在ANN的RC中，我们使用标准实值激活通过时间应用反向传播，但仅从每个顺序输入示例的策略性早期时间步开始，由阈值交叉事件决定。然后，学习自然也会在不改变模型或算法的情况下产生输出。通过跳过第一个事件后剩余的输入序列，向前和向后的训练过程都可以显著缩短。RC培训还显著缩短了推理过程中的洞察时间，但准确性的降低幅度最小。期望的速度-精度权衡可通过改变阈值或奖励输出熵的正则化参数进行调整。我们在序列分类的两个玩具问题中以及在时间编码的MNIST数据集中演示了这些问题，其中我们的RC模型在第一个输入时间步后达到99.19%的准确率，在使用SNN的时间编码以及Google语音命令的口语词分类方面优于最新技术，优于非RC训练的LSTM早期推理。
<details>	<summary>英文摘要</summary>	Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e.g. in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not. As a result, models of SNNs for neuromorphic computing are regarded as potentially more rapid and efficient than ANNs when dealing with temporal input. On the other hand, ANNs are simpler to train, and usually achieve superior performance. Here we show that temporal coding such as rank coding (RC) inspired by SNNs can also be applied to conventional ANNs such as LSTMs, and leads to computational savings and speedups. In our RC for ANNs, we apply backpropagation through time using the standard real-valued activations, but only from a strategically early time step of each sequential input example, decided by a threshold-crossing event. Learning then incorporates naturally also _when_ to produce an output, without other changes to the model or the algorithm. Both the forward and the backward training pass can be significantly shortened by skipping the remaining input sequence after that first event. RC-training also significantly reduces time-to-insight during inference, with a minimal decrease in accuracy. The desired speed-accuracy trade-off is tunable by varying the threshold or a regularization parameter that rewards output entropy. We demonstrate these in two toy problems of sequence classification, and in a temporally-encoded MNIST dataset where our RC model achieves 99.19% accuracy after the first input time-step, outperforming the state of the art in temporal coding with SNNs, as well as in spoken-word classification of Google Speech Commands, outperforming non-RC-trained early inference with LSTMs. </details>
<details>	<summary>邮件日期</summary>	2021年10月07日</details>

# 242、全脉冲变分自动编码器
- [ ] Fully Spiking Variational Autoencoder 
时间：2021年10月05日                         第一作者：Hiromichi Kamata                       [链接](https://arxiv.org/abs/2110.00375).                     
<details>	<summary>注释</summary>	https://github.com/kamata1729/FullySpikingVAE </details>
<details>	<summary>邮件日期</summary>	2021年10月06日</details>

# 241、基于生物神经网络的端到端语音识别
- [ ] Towards efficient end-to-end speech recognition with biologically-inspired neural networks 
时间：2021年10月04日                         第一作者：Thomas Bohnstingl                       [链接](https://arxiv.org/abs/2110.02743).                     
## 摘要：自动语音识别（ASR）是一种使程序能够将人类语音处理成书面形式的能力。人工智能（AI）的最新发展导致了基于深层神经网络的高精度ASR系统，如递归神经网络传感器（RNN-T）。然而，这些方法的核心组件和执行的操作与强大的生物对应物，即人脑不同。另一方面，目前基于脉冲神经网络（SNN）的生物启发ASR模型的发展在准确性方面落后，主要集中在小规模应用上。在这项工作中，我们回顾了将生物学上合理的模型纳入深度学习的过程，并从大脑中发现的各种神经和突触动力学中获得灵感，从而大大增强了这些模型的能力。特别是，我们介绍了模拟轴-体突触和轴-轴突触的神经连接概念。基于此，我们提出了具有丰富神经突触动力学的新型深度学习单元，并将其集成到RNN-T架构中。我们首次证明，与现有的深度学习模型相比，大规模ASR模型的生物现实实现可以产生具有竞争力的性能水平。具体地说，我们证明了这种实现具有一些优势，例如降低了计算成本和较低的延迟，这对于语音识别应用至关重要。
<details>	<summary>英文摘要</summary>	Automatic speech recognition (ASR) is a capability which enables a program to process human speech into a written form. Recent developments in artificial intelligence (AI) have led to high-accuracy ASR systems based on deep neural networks, such as the recurrent neural network transducer (RNN-T). However, the core components and the performed operations of these approaches depart from the powerful biological counterpart, i.e., the human brain. On the other hand, the current developments in biologically-inspired ASR models, based on spiking neural networks (SNNs), lag behind in terms of accuracy and focus primarily on small scale applications. In this work, we revisit the incorporation of biologically-plausible models into deep learning and we substantially enhance their capabilities, by taking inspiration from the diverse neural and synaptic dynamics found in the brain. In particular, we introduce neural connectivity concepts emulating the axo-somatic and the axo-axonic synapses. Based on this, we propose novel deep learning units with enriched neuro-synaptic dynamics and integrate them into the RNN-T architecture. We demonstrate for the first time, that a biologically realistic implementation of a large-scale ASR model can yield competitive performance levels compared to the existing deep learning models. Specifically, we show that such an implementation bears several advantages, such as a reduced computational cost and a lower latency, which are critical for speech recognition applications. </details>
<details>	<summary>邮件日期</summary>	2021年10月07日</details>

# 240、脉冲超维网络：结合记忆框架的神经形态模型
- [ ] Spiking Hyperdimensional Network: Neuromorphic Models Integrated with Memory-Inspired Framework 
时间：2021年10月01日                         第一作者：Zhuowen Zou                       [链接](https://arxiv.org/abs/2110.00214).                     
## 摘要：最近，受大脑启发的计算模型在鲁棒性和能源效率方面表现出了超越当今深度学习解决方案的巨大潜力。特别是，脉冲神经网络（SNN）和超维计算（HDC）在实现高效和稳健的认知学习方面显示了良好的结果。尽管取得了成功，但这两种大脑启发模型有着不同的优势。SNN模仿人脑的物理特性，而HDC则在更抽象和功能的层面上对大脑进行建模。他们的设计理念展示了激励他们组合的互补模式。借助于经典的记忆心理学模型，我们提出了SpikeHD，这是第一个从根本上结合了脉冲神经网络和超维计算的框架。SpikeHD生成了一个可扩展且强大的认知学习系统，可以更好地模拟大脑功能。SpikeHD利用脉冲神经网络通过保留原始事件脉冲数据的空间和时间相关性来提取低级特征。然后，通过将信号映射到高维空间，学习抽象信息，并对数据进行分类，利用HDC对SNN输出进行操作。我们对一组基准分类问题的广泛评估表明，与SNN体系结构相比，SpikeHD具有以下优点：（1）通过利用两阶段信息处理显著增强学习能力，（2）对噪声和故障具有较强的鲁棒性，以及（3）减少网络规模和学习复杂信息所需的参数。
<details>	<summary>英文摘要</summary>	Recently, brain-inspired computing models have shown great potential to outperform today's deep learning solutions in terms of robustness and energy efficiency. Particularly, Spiking Neural Networks (SNNs) and HyperDimensional Computing (HDC) have shown promising results in enabling efficient and robust cognitive learning. Despite the success, these two brain-inspired models have different strengths. While SNN mimics the physical properties of the human brain, HDC models the brain on a more abstract and functional level. Their design philosophies demonstrate complementary patterns that motivate their combination. With the help of the classical psychological model on memory, we propose SpikeHD, the first framework that fundamentally combines Spiking neural network and hyperdimensional computing. SpikeHD generates a scalable and strong cognitive learning system that better mimics brain functionality. SpikeHD exploits spiking neural networks to extract low-level features by preserving the spatial and temporal correlation of raw event-based spike data. Then, it utilizes HDC to operate over SNN output by mapping the signal into high-dimensional space, learning the abstract information, and classifying the data. Our extensive evaluation on a set of benchmark classification problems shows that SpikeHD provides the following benefit compared to SNN architecture: (1) significantly enhance learning capability by exploiting two-stage information processing, (2) enables substantial robustness to noise and failure, and (3) reduces the network size and required parameters to learn complex information. </details>
<details>	<summary>邮件日期</summary>	2021年10月04日</details>

# 239、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2021年10月01日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
<details>	<summary>邮件日期</summary>	2021年10月04日</details>

# 238、基于进化神经形态雷达的自主开源飞艇高度控制器
- [ ] Evolved neuromorphic radar-based altitude controller for an autonomous open-source blimp 
时间：2021年10月01日                         第一作者：Marina Gonz\'alez-\'Alvarez                       [链接](https://arxiv.org/abs/2110.00646).                     
## 摘要：机器人飞艇在安全性、机动性和延长飞行时间方面具有显著优势。然而，它们的高度限制性权重约束对执行所需控制任务的可用计算能力提出了重大挑战。脉冲神经网络（SNN）是解决这一问题的一个很有前途的研究方向。通过模仿神经元之间使用脉冲或脉冲传输信息的生物过程，它们允许低功耗和异步事件驱动处理。在本文中，我们提出了一种基于SNN的机器人飞艇进化高度控制器，该控制器完全依赖于机载雷达提供的传感器反馈。从轻量级、低成本、开源飞艇的设计出发，我们还提出了一种基于SNN的控制器体系结构，一种在模拟环境中训练网络的进化框架，以及一种改善与现实差距的控制方案。通过实际实验对系统的性能进行了评估，并与人工神经网络和线性控制器进行了比较，证明了该方法的优越性。结果表明，通过有效的控制，可以精确跟踪高度指令。
<details>	<summary>英文摘要</summary>	Robotic airships offer significant advantages in terms of safety, mobility, and extended flight times. However, their highly restrictive weight constraints pose a major challenge regarding the available computational power to perform the required control tasks. Spiking neural networks (SNNs) are a promising research direction for addressing this problem. By mimicking the biological process for transferring information between neurons using spikes or impulses, they allow for low power consumption and asynchronous event-driven processing. In this paper, we propose an evolved altitude controller based on a SNN for a robotic airship which relies solely on the sensory feedback provided by an airborne radar. Starting from the design of a lightweight, low-cost, open-source airship, we also present a SNN-based controller architecture, an evolutionary framework for training the network in a simulated environment, and a control scheme for ameliorating the gap with reality. The system's performance is evaluated through real-world experiments, demonstrating the advantages of our approach by comparing it with an artificial neural network and a linear controller. The results show an accurate tracking of the altitude command with an efficient control effort. </details>
<details>	<summary>邮件日期</summary>	2021年10月05日</details>

# 237、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年10月01日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
<details>	<summary>注释</summary>	12 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月05日</details>

# 236、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年09月30日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
## 摘要：我们训练了一个神经形态硬件芯片，通过变分能量最小化来近似量子自旋模型的基态。与使用马尔可夫链蒙特卡罗生成样本的变分人工神经网络相比，该方法的优点是神经形态设备以快速且固有的并行方式生成样本。我们开发了一种训练算法，并将其应用于横向场伊辛模型，在中等系统规模（$N\leq 10$）下表现出良好的性能。一项系统的超参数研究表明，对更大系统规模的可扩展性主要取决于样本质量，而样本质量受到模拟神经形态芯片上参数漂移的限制。学习性能显示了阈值行为作为ansatz变分参数数量的函数，大约$50$隐藏神经元足以表示高达$N=10$的临界基态。网络参数的6+1位分辨率不限制当前设置中可达到的近似质量。我们的工作为利用神经形态硬件的能力解决量子多体问题中的维度诅咒迈出了重要的一步。
<details>	<summary>英文摘要</summary>	We train a neuromorphic hardware chip to approximate the ground states of quantum spin models by variational energy minimization. Compared to variational artificial neural networks using Markov chain Monte Carlo for sample generation, this approach has the advantage that the neuromorphic device generates samples in a fast and inherently parallel fashion. We develop a training algorithm and apply it to the transverse field Ising model, showing good performance at moderate system sizes ($N\leq 10$). A systematic hyperparameter study shows that scalability to larger system sizes mainly depends on sample quality which is limited by parameter drifts on the analog neuromorphic chip. The learning performance shows a threshold behavior as a function of the number of variational parameters of the ansatz, with approximately $50$ hidden neurons being sufficient for representing critical ground states up to $N=10$. The 6+1-bit resolution of the network parameters does not limit the reachable approximation quality in the current setup. Our work provides an important step towards harnessing the capabilities of neuromorphic hardware for tackling the curse of dimensionality in quantum many-body problems. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月01日</details>

# 235、全脉冲变分自动编码器
- [ ] Fully Spiking Variational Autoencoder 
时间：2021年09月26日                         第一作者：Hiromichi Kamata                       [链接](https://arxiv.org/abs/2110.00375).                     
## 摘要：脉冲神经网络（SNN）由于其二进制和事件驱动的特性，可以在具有超高速和超低能耗的神经形态设备上运行。因此，SNN有望有各种应用，包括在边缘设备上运行生成模型，以创建高质量图像。在这项研究中，我们构建了一个带有SNN的变分自动编码器（VAE）来实现图像生成。VAE以其在生成模型中的稳定性而闻名；最近，它的质量提高了。在VAE中，潜在空间表示为正态分布，采样时需要进行浮点计算。然而，这在SNN中是不可能的，因为所有特征必须是二进制时间序列数据。因此，我们使用自回归SNN模型构建了潜在空间，并从其输出中随机选取样本对潜在变量进行采样。这允许潜在变量遵循伯努利过程，并允许变分学习。因此，我们构建了全脉冲变分自动编码器，其中所有模块都用SNN构造。据我们所知，我们是第一个仅使用SNN层构建VAE的公司。我们对几个数据集进行了实验，并确认它可以生成与传统人工神经网络相同或更好质量的图像。代码很快就会发布。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) can be run on neuromorphic devices with ultra-high speed and ultra-low energy consumption because of their binary and event-driven nature. Therefore, SNNs are expected to have various applications, including as generative models being running on edge devices to create high-quality images. In this study, we build a variational autoencoder (VAE) with SNN to enable image generation. VAE is known for its stability among generative models; recently, its quality advanced. In vanilla VAE, the latent space is represented as a normal distribution, and floating-point calculations are required in sampling. However, this is not possible in SNNs because all features must be binary time series data. Therefore, we constructed the latent space with an autoregressive SNN model, and randomly selected samples from its output to sample the latent variables. This allows the latent variables to follow the Bernoulli process and allows variational learning. Thus, we build the Fully Spiking Variational Autoencoder where all modules are constructed with SNN. To the best of our knowledge, we are the first to build a VAE only with SNN layers. We experimented with several datasets, and confirmed that it can generate images with the same or better quality compared to conventional ANNs. The code will be available soon. </details>
<details>	<summary>邮件日期</summary>	2021年10月04日</details>

# 234、平衡态隐式微分训练反馈脉冲神经网络
- [ ] Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State 
时间：2021年09月29日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2109.14247).                     
## 摘要：脉冲神经网络（SNN）是受大脑启发的模型，能够在神经形态硬件上实现节能。然而，由于脉冲神经元模型的不连续性，SNN的监督训练仍然是一个难题。大多数现有的方法模仿人工神经网络的反向传播框架和前馈结构，并使用替代导数或计算关于脉冲时间的梯度来处理该问题。这些方法要么累积近似误差，要么仅通过现有脉冲有限地传播信息，通常需要信息沿时间步传播，具有较大的内存开销和生物不可信性。在这项工作中，我们考虑反馈脉冲神经网络，这是更像大脑，并提出了一种新的训练方法，不依赖于准确的反向正向计算。首先，我们证明了具有反馈连接的SNN的平均放电速率会随着时间逐渐演化到一个平衡状态，这遵循一个不动点方程。然后，通过将反馈SNN的正向计算视为该方程的黑箱解算器，并利用方程上的隐式微分，我们可以计算参数的梯度，而无需考虑精确的正向过程。这样，前向和后向过程被解耦，从而避免了不可微脉冲函数的问题。我们还简要讨论了内隐分化的生物学合理性，它只需要计算另一个平衡。在MNIST、Fashion MNIST、N-MNIST、CIFAR-10和CIFAR-100上进行的大量实验表明，我们的方法对于在少量时间步长内具有较少神经元和参数的反馈模型具有优越的性能。我们的代码可在https://github.com/pkuxmq/IDE-FSNN.
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are brain-inspired models that enable energy-efficient implementation on neuromorphic hardware. However, the supervised training of SNNs remains a hard problem due to the discontinuity of the spiking neuron model. Most existing methods imitate the backpropagation framework and feedforward architectures for artificial neural networks, and use surrogate derivatives or compute gradients with respect to the spiking time to deal with the problem. These approaches either accumulate approximation errors or only propagate information limitedly through existing spikes, and usually require information propagation along time steps with large memory costs and biological implausibility. In this work, we consider feedback spiking neural networks, which are more brain-like, and propose a novel training method that does not rely on the exact reverse of the forward computation. First, we show that the average firing rates of SNNs with feedback connections would gradually evolve to an equilibrium state along time, which follows a fixed-point equation. Then by viewing the forward computation of feedback SNNs as a black-box solver for this equation, and leveraging the implicit differentiation on the equation, we can compute the gradient for parameters without considering the exact forward procedure. In this way, the forward and backward procedures are decoupled and therefore the problem of non-differentiable spiking functions is avoided. We also briefly discuss the biological plausibility of implicit differentiation, which only requires computing another equilibrium. Extensive experiments on MNIST, Fashion-MNIST, N-MNIST, CIFAR-10, and CIFAR-100 demonstrate the superior performance of our method for feedback models with fewer neurons and parameters in a small number of time steps. Our code is avaiable at https://github.com/pkuxmq/IDE-FSNN. </details>
<details>	<summary>注释</summary>	Accepted by NeurIPS 2021 (Spotlight) </details>
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 233、通过信息瓶颈学习脉冲神经网络的时间解码
- [ ] Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck 
时间：2021年09月29日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2106.01177).                     
<details>	<summary>注释</summary>	Accepted at NeuRIPS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 232、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2021年09月29日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 231、立体画：用脉冲神经网络进行深度学习
- [ ] StereoSpike: Depth Learning with a Spiking Neural Network 
时间：2021年09月28日                         第一作者：Ulysse Ran\c{c}on                       [链接](https://arxiv.org/abs/2109.13751).                     
## 摘要：深度估计是一项重要的计算机视觉任务，特别适用于自动驾驶车辆的导航或机器人的目标操纵。在这里，我们使用端到端的神经形态方法解决了这个问题，将两个基于事件的摄像头和一个脉冲神经网络（SNN）与一个稍加修改的类似U网络的编码器-解码器架构相结合，我们将其命名为立体派克。更具体地说，我们使用了多车辆立体事件摄影机数据集（MVSEC）。它提供了一个深度-地面真实值，用于使用代理梯度下降以有监督的方式训练立体派克。我们提出了一种新的读出模式，从解码器的峰值中获得密集的模拟预测——每个像素的深度。我们证明了该体系结构的通用性非常好，甚至比其非脉冲对应结构更好，从而实现了最先进的测试精度。据我们所知，这是第一次用完全脉冲网络解决如此大规模的回归问题。最后，我们证明了通过正则化可以获得低的发射率（<10%），并且在精度上的代价最小。这意味着可以在神经形态芯片上高效地实现立体派克，为低功耗和实时嵌入式系统打开了大门。
<details>	<summary>英文摘要</summary>	Depth estimation is an important computer vision task, useful in particular for navigation in autonomous vehicles, or for object manipulation in robotics. Here we solved it using an end-to-end neuromorphic approach, combining two event-based cameras and a Spiking Neural Network (SNN) with a slightly modified U-Net-like encoder-decoder architecture, that we named StereoSpike. More specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It provides a depth ground-truth, which was used to train StereoSpike in a supervised manner, using surrogate gradient descent. We propose a novel readout paradigm to obtain a dense analog prediction -- the depth of each pixel -- from the spikes of the decoder. We demonstrate that this architecture generalizes very well, even better than its non-spiking counterparts, leading to state-of-the-art test accuracy. To the best of our knowledge, it is the first time that such a large-scale regression problem is solved by a fully spiking network. Finally, we show that low firing rates (<10%) can be obtained via regularization, with a minimal cost in accuracy. This means that StereoSpike could be efficiently implemented on neuromorphic chips, opening the door for low power and real time embedded systems. </details>
<details>	<summary>邮件日期</summary>	2021年09月29日</details>

# 230、深相量网络：连接传统神经网络和脉冲神经网络
- [ ] Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks 
时间：2021年09月28日                         第一作者：Wilkie Olin-Ammentorp                       [链接](https://arxiv.org/abs/2106.11908).                     
<details>	<summary>注释</summary>	24 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月29日</details>

# 229、一种优化的无梯度深脉冲神经网络结构
- [ ] An optimised deep spiking neural network architecture without gradients 
时间：2021年09月27日                         第一作者：Yeshwanth Bethi                       [链接](https://arxiv.org/abs/2109.12813).                     
## 摘要：我们提出了一种端到端可训练的模块化事件驱动神经结构，该结构使用局部突触和阈值适应规则来执行任意时空脉冲模式之间的转换。该体系结构代表了现有脉冲神经网络（SNN）体系结构的高度抽象模型。所提出的优化深度事件驱动脉冲神经网络结构（ODSA）可以同时学习多个任意时间尺度的分层时空特征。ODSA在不使用误差反向传播或梯度计算的情况下执行在线学习。通过在每个节点上使用简单的局部自适应选择阈值，网络可以快速学习在每一层为任何给定问题适当分配其神经元资源，而无需使用实值误差度量。这些自适应选择阈值是ODSA的核心特征，确保了网络的稳定性和对噪声以及初始系统参数选择的显著鲁棒性。由于每一层的硬赢家通吃（WTA）约束，网络激活本质上是稀疏的。我们评估了现有时空数据集的体系结构，包括spike编码的IRIS和TIDIGITS数据集，以及基于我们创建的国际莫尔斯电码的一组新任务。这些测试证明了ODSA的分层时空学习能力。通过这些测试，我们证明了ODSA能够以尽可能少的计算节点数最优地解决实际且极具挑战性的分层时空学习任务。
<details>	<summary>英文摘要</summary>	We present an end-to-end trainable modular event-driven neural architecture that uses local synaptic and threshold adaptation rules to perform transformations between arbitrary spatio-temporal spike patterns. The architecture represents a highly abstracted model of existing Spiking Neural Network (SNN) architectures. The proposed Optimized Deep Event-driven Spiking neural network Architecture (ODESA) can simultaneously learn hierarchical spatio-temporal features at multiple arbitrary time scales. ODESA performs online learning without the use of error back-propagation or the calculation of gradients. Through the use of simple local adaptive selection thresholds at each node, the network rapidly learns to appropriately allocate its neuronal resources at each layer for any given problem without using a real-valued error measure. These adaptive selection thresholds are the central feature of ODESA, ensuring network stability and remarkable robustness to noise as well as to the selection of initial system parameters. Network activations are inherently sparse due to a hard Winner-Take-All (WTA) constraint at each layer. We evaluate the architecture on existing spatio-temporal datasets, including the spike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based on International Morse Code that we created. These tests demonstrate the hierarchical spatio-temporal learning capabilities of ODESA. Through these tests, we demonstrate ODESA can optimally solve practical and highly challenging hierarchical spatio-temporal learning tasks with the minimum possible number of computing nodes. </details>
<details>	<summary>注释</summary>	18 pages, 6 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 228、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2021年09月27日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
## 摘要：大脑是寻找灵感以开发更高效的神经网络的完美场所。我们的突触和神经元的内部运作让我们得以窥见深度学习的未来。本文展示了如何将几十年来在深度学习、梯度下降、反向传播和神经科学方面的研究成果应用于生物学上合理的脉冲神经网络。本文探讨了将数据编码为脉冲与学习过程之间微妙的相互作用；将梯度学习应用于脉冲神经网络的挑战和解决方案；时间反向传播和脉冲时间依赖性可塑性之间的微妙联系，以及深度学习如何向生物学上合理的在线学习发展。一些想法在神经形态工程界被广泛接受和使用，而另一些想法在这里首次被提出或证明是正确的。
<details>	<summary>英文摘要</summary>	The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper shows how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks. This paper explores the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks; the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 227、基于代理的脉冲神经网络训练
- [ ] Spiking neural networks trained via proxy 
时间：2021年09月27日                         第一作者：Saeed Reza Kheradpisheh                       [链接](https://arxiv.org/abs/2109.13208).                     
## 摘要：我们提出了一种新的学习算法来训练以传统人工神经网络（ANN）为代理的脉冲神经网络（SNN）。我们分别耦合了两个SNN和ANN网络，这两个网络由具有相同网络结构和共享突触权重的Integrated and fire（IF）和ReLU神经元组成。两个网络的前向传递是完全独立的。通过假设带有速率编码的IF神经元作为ReLU的近似值，我们在代理ANN中反向传播SNN的错误以更新共享权重，只需将ANN的最终输出替换为SNN的最终输出。我们将提出的代理学习应用于深度卷积SNN，并在Fahion MNIST和Cifar10两个基准数据集上对其进行评估，分类准确率分别为94.56%和93.11%。所提出的网络可以优于其他通过串联学习、替代梯度学习或从深度ANN转换而来的深度SNN。转换后的SNN需要很长的模拟时间才能达到合理的精度，而我们的代理学习可以使有效的SNN具有更短的模拟时间。
<details>	<summary>英文摘要</summary>	We propose a new learning algorithm to train spiking neural networks (SNN) using conventional artificial neural networks (ANN) as proxy. We couple two SNN and ANN networks, respectively, made of integrate-and-fire (IF) and ReLU neurons with the same network architectures and shared synaptic weights. The forward passes of the two networks are totally independent. By assuming IF neuron with rate-coding as an approximation of ReLU, we backpropagate the error of the SNN in the proxy ANN to update the shared weights, simply by replacing the ANN final output with that of the SNN. We applied the proposed proxy learning to deep convolutional SNNs and evaluated it on two benchmarked datasets of Fahion-MNIST and Cifar10 with 94.56% and 93.11% classification accuracy, respectively. The proposed networks could outperform other deep SNNs trained with tandem learning, surrogate gradient learning, or converted from deep ANNs. Converted SNNs require long simulation times to reach reasonable accuracies while our proxy learning leads to efficient SNNs with much shorter simulation times. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 226、Brian2Loihi：神经形态芯片Loihi的仿真器，使用脉冲神经网络仿真器Brian
- [ ] Brian2Loihi: An emulator for the neuromorphic chip Loihi using the spiking neural network simulator Brian 
时间：2021年09月25日                         第一作者：Carlo Michaelis                       [链接](https://arxiv.org/abs/2109.12308).                     
## 摘要：开发智能神经形态解决方案仍然是一项具有挑战性的工作。它需要对硬件的基本构建块有坚实的概念性理解。除此之外，可访问且用户友好的原型设计对于加快设计流程至关重要。我们开发了一个基于神经网络模拟器Brian的开源Loihi模拟器，该模拟器可以很容易地整合到现有的仿真工作流中。我们在软件中演示了单个神经元和循环连接的脉冲神经网络的无错误Loihi仿真。片上学习也进行了审查和实施，由于随机舍入，存在合理的差异。这项工作对Loihi的计算单元进行了连贯的介绍，并介绍了一个新的、易于使用的Loihi原型软件包，旨在帮助简化新算法的概念化和部署。
<details>	<summary>英文摘要</summary>	Developing intelligent neuromorphic solutions remains a challenging endeavour. It requires a solid conceptual understanding of the hardware's fundamental building blocks. Beyond this, accessible and user-friendly prototyping is crucial to speed up the design pipeline. We developed an open source Loihi emulator based on the neural network simulator Brian that can easily be incorporated into existing simulation workflows. We demonstrate errorless Loihi emulation in software for a single neuron and for a recurrently connected spiking neural network. On-chip learning is also reviewed and implemented, with reasonable discrepancy due to stochastic rounding. This work provides a coherent presentation of Loihi's computational unit and introduces a new, easy-to-use Loihi prototyping package with the aim to help streamline conceptualisation and deployment of new algorithms. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 225、具有相变记忆突触的脉冲递归神经网络的在线训练
- [ ] Online Training of Spiking Recurrent Neural Networks with Phase-Change Memory Synapses 
时间：2021年09月25日                         第一作者：Yigit Demirag                       [链接](https://arxiv.org/abs/2108.01804).                     
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 224、最大化相互信息的感知系统的生物学上合理的学习规则
- [ ] Biologically Plausible Learning Rules for Perceptual Systems that Maximize Mutual Information 
时间：2021年09月07日                         第一作者：Tao Liu                       [链接](https://arxiv.org/abs/2109.13102).                     
## 摘要：人们普遍认为，生物体的感知系统是针对其所处环境的特性而优化的。这一原理的一个具体例子称为Infomax原理，认为早期感知处理的目的是最大化神经编码和传入感觉信号之间的互信息。在本文中，我们展示了一个利用时空局部、基于脉冲和连续时间学习规则精确实现这一原理的模型。
<details>	<summary>英文摘要</summary>	It is widely believed that the perceptual system of an organism is optimized for the properties of the environment to which it is exposed. A specific instance of this principle known as the Infomax principle holds that the purpose of early perceptual processing is to maximize the mutual information between the neural coding and the incoming sensory signal. In this article, we show a model to implement this principle accurately with spatio-temporal local, spike-based, and continuous-time learning rules. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 223、通过正则化训练无神经元破裂或死亡的深脉冲自动编码器
- [ ] Training Deep Spiking Auto-encoders without Bursting or Dying Neurons through Regularization 
时间：2021年09月22日                         第一作者：Justus F. H\"ubotter                       [链接](https://arxiv.org/abs/2109.11045).                     
## 摘要：脉冲神经网络是计算神经科学中下一代大脑模型的一种很有前途的方法。此外，与经典的人工神经网络相比，它们可以在专门的神经形态硬件中实现快速计算，从而成为AI的节能部署。然而，训练深度脉冲神经网络，特别是以无监督的方式训练，是一项挑战，而且脉冲模型的性能受到死亡或爆裂神经元的显著阻碍。在这里，我们将基于膜电位的反向传播的端到端学习应用于具有多个可训练的漏积分和激发神经元层的脉冲卷积自动编码器。我们提出了仿生正则化方法来控制潜在表征中的脉冲密度。在实验中，我们发现对膜电位和脉冲输出应用正则化成功地避免了神经元死亡和破裂，并显著降低了脉冲自动编码器的重建误差。在MNIST数据集上训练正则化网络可产生与非脉冲基线模型（确定性和变分自动编码器）相当的图像重建质量，并表明比早期方法有所改进。重要的是，我们表明，与变分自动编码器不同，脉冲潜在表示显示与图像类相关的结构。
<details>	<summary>英文摘要</summary>	Spiking neural networks are a promising approach towards next-generation models of the brain in computational neuroscience. Moreover, compared to classic artificial neural networks, they could serve as an energy-efficient deployment of AI by enabling fast computation in specialized neuromorphic hardware. However, training deep spiking neural networks, especially in an unsupervised manner, is challenging and the performance of a spiking model is significantly hindered by dead or bursting neurons. Here, we apply end-to-end learning with membrane potential-based backpropagation to a spiking convolutional auto-encoder with multiple trainable layers of leaky integrate-and-fire neurons. We propose bio-inspired regularization methods to control the spike density in latent representations. In the experiments, we show that applying regularization on membrane potential and spiking output successfully avoids both dead and bursting neurons and significantly decreases the reconstruction error of the spiking auto-encoder. Training regularized networks on the MNIST dataset yields image reconstruction quality comparable to non-spiking baseline models (deterministic and variational auto-encoder) and indicates improvement upon earlier approaches. Importantly, we show that, unlike the variational auto-encoder, the spiking latent representations display structure associated with the image class. </details>
<details>	<summary>注释</summary>	Under review </details>
<details>	<summary>邮件日期</summary>	2021年09月24日</details>

# 222、在Intel的神经形态硬件Loihi上映射和验证点神经元模型
- [ ] Mapping and Validating a Point Neuron Model on Intel's Neuromorphic Hardware Loihi 
时间：2021年09月22日                         第一作者：Srijanie Dey                        [链接](https://arxiv.org/abs/2109.10835).                     
## 摘要：神经形态硬件基于模拟大脑的自然生物结构。由于其计算模型与标准神经模型相似，因此它可以作为神经科学和人工智能领域（包括生物医学应用）研究项目的计算加速工具。然而，为了开发这一新一代计算机芯片，必须对基于大脑的实验数据进行严格的模拟和随后的验证。在这项工作中，我们研究了英特尔第五代神经形态芯片“Loihi”的潜力，该芯片基于脉冲神经网络（SNN）模拟大脑神经元的新思想。这项工作是在模拟基于与丰富的解剖、生理和行为约束数据集相匹配的小鼠初级视觉皮层的泄漏集成和火灾（LIF）模型的背景下进行的。经典硬件上的仿真作为神经形态实现的验证平台。我们发现，Loihi非常有效地复制了经典模拟，并且随着网络变大，它在时间和能量性能方面的可扩展性显著提高。
<details>	<summary>英文摘要</summary>	Neuromorphic hardware is based on emulating the natural biological structure of the brain. Since its computational model is similar to standard neural models, it could serve as a computational acceleration for research projects in the field of neuroscience and artificial intelligence, including biomedical applications. However, in order to exploit this new generation of computer chips, rigorous simulation and consequent validation of brain-based experimental data is imperative. In this work, we investigate the potential of Intel's fifth generation neuromorphic chip - `Loihi', which is based on the novel idea of Spiking Neural Networks (SNNs) emulating the neurons in the brain. The work is implemented in context of simulating the Leaky Integrate and Fire (LIF) models based on the mouse primary visual cortex matched to a rich data set of anatomical, physiological and behavioral constraints. Simulations on the classical hardware serve as the validation platform for the neuromorphic implementation. We find that Loihi replicates classical simulations very efficiently and scales notably well in terms of both time and energy performance as the networks get larger. </details>
<details>	<summary>邮件日期</summary>	2021年09月23日</details>

# 221、通过结构学习：深入神经形态知识图嵌入
- [ ] Learning through structure: towards deep neuromorphic knowledge graph embeddings 
时间：2021年09月21日                         第一作者：Victor Caceres Chian                       [链接](https://arxiv.org/abs/2109.10376).                     
## 摘要：在从分子合成到社会网络分析和推荐系统的许多工业和学术应用中，计算图结构数据的潜在表示是一项普遍存在的学习任务。知识图是与语义Web相关的最流行和最广泛使用的数据表示形式之一。除了以机器可读的格式构造事实知识之外，知识图还充当许多人工智能应用程序的主干，并允许将上下文信息吸收到各种学习算法中。图神经网络试图通过相邻节点之间的消息传递启发式在低维向量空间中编码图结构。近年来，许多不同的图形神经网络结构在许多学习任务中表现出开创性的性能。在这项工作中，我们提出了一种策略，将用于知识图推理的深度图学习体系结构映射到神经形态体系结构。基于随机初始化和未训练（即冻结）图神经网络能够保持局部图结构的认识，我们构造了一个具有浅知识图嵌入模型的冻结神经网络。我们的实验表明，在传统的计算硬件上，这会导致显著的加速和内存减少，同时保持有竞争力的性能水平。此外，我们将冻结结构扩展到脉冲神经网络，引入了一种新的、基于事件的、高度稀疏的知识图嵌入算法，该算法适合在神经形态硬件中实现。
<details>	<summary>英文摘要</summary>	Computing latent representations for graph-structured data is an ubiquitous learning task in many industrial and academic applications ranging from molecule synthetization to social network analysis and recommender systems. Knowledge graphs are among the most popular and widely used data representations related to the Semantic Web. Next to structuring factual knowledge in a machine-readable format, knowledge graphs serve as the backbone of many artificial intelligence applications and allow the ingestion of context information into various learning algorithms. Graph neural networks attempt to encode graph structures in low-dimensional vector spaces via a message passing heuristic between neighboring nodes. Over the recent years, a multitude of different graph neural network architectures demonstrated ground-breaking performances in many learning tasks. In this work, we propose a strategy to map deep graph learning architectures for knowledge graph reasoning to neuromorphic architectures. Based on the insight that randomly initialized and untrained (i.e., frozen) graph neural networks are able to preserve local graph structures, we compose a frozen neural network with shallow knowledge graph embedding models. We experimentally show that already on conventional computing hardware, this leads to a significant speedup and memory reduction while maintaining a competitive performance level. Moreover, we extend the frozen architecture to spiking neural networks, introducing a novel, event-based and highly sparse knowledge graph embedding algorithm that is suitable for implementation in neuromorphic hardware. </details>
<details>	<summary>注释</summary>	Accepted for publication at the International Conference on Neuromorphic Computing (ICNC 2021) </details>
<details>	<summary>邮件日期</summary>	2021年09月23日</details>

# 220、基于神经形态处理器的微型飞行器机载高度控制简约神经形态PID的设计与实现
- [ ] Design and implementation of a parsimonious neuromorphic PID for onboard altitude control for MAVs using neuromorphic processors 
时间：2021年09月21日                         第一作者：Stein Stroobants                       [链接](https://arxiv.org/abs/2109.10199).                     
## 摘要：机器人的神经形态传感和处理技术的巨大潜力促使研究人员和工程师们研究用于自主机器人（导航、障碍物检测和回避等）鲁棒可靠控制的新模型，特别是在无人机竞赛和攻击性机动等挑战性环境中的四旋翼机器人。使用脉冲神经网络，这些模型可以在神经形态硬件上运行，从而受益于出色的更新率和高能效。然而，低级控制器往往被忽略，并停留在神经形态环路之外。设计低级神经形态控制器对于消除标准PID至关重要，因此可以受益于关闭神经形态回路的所有优点。在本文中，我们提出了一种简约可调的神经形态PID控制器，该控制器具有最少数量的93个稀疏连接的神经元，以实现配备Intel Loihi神经形态芯片的四旋翼的自主机载高度控制。我们在一组实验中成功地证明了我们提出的网络的鲁棒性，其中要求四旋翼从起飞时达到目标高度。我们的结果证实了这种低级神经形态控制器的适用性，最终具有非常高的更新频率。
<details>	<summary>英文摘要</summary>	The great promises of neuromorphic sensing and processing for robotics have led researchers and engineers to investigate novel models for robust and reliable control of autonomous robots (navigation, obstacle detection and avoidance, etc.), especially for quadrotors in challenging contexts such as drone racing and aggressive maneuvers. Using spiking neural networks, these models can be run on neuromorphic hardware to benefit from outstanding update rates and high energy efficiency. Yet, low-level controllers are often neglected and remain outside of the neuromorphic loop. Designing low-level neuromorphic controllers is crucial to remove the standard PID, and therefore benefit from all the advantages of closing the neuromorphic loop. In this paper, we propose a parsimonious and adjustable neuromorphic PID controller, endowed with a minimal number of 93 neurons sparsely connected to achieve autonomous, onboard altitude control of a quadrotor equipped with Intel's Loihi neuromorphic chip. We successfully demonstrate the robustness of our proposed network in a set of experiments where the quadrotor is requested to reach a target altitude from take-off. Our results confirm the suitability of such low-level neuromorphic controllers, ultimately with a very high update frequency. </details>
<details>	<summary>注释</summary>	7 pages, 9 figures, conference </details>
<details>	<summary>邮件日期</summary>	2021年09月22日</details>

# 219、面向节能安全的边缘人工智能：一种跨层框架
- [ ] Towards Energy-Efficient and Secure Edge AI: A Cross-Layer Framework 
时间：2021年09月20日                         第一作者：Muhammad Shafique                       [链接](https://arxiv.org/abs/2109.09829).                     
## 摘要：安全和隐私问题以及需要定期处理的数据量将处理推到了计算系统的边缘。由于严格的内存和功率/能量限制，在资源受限的边缘设备上部署高级神经网络（NN），如深度神经网络（DNN）和脉冲神经网络（SNN），提供最先进的结果是一项挑战。此外，这些系统需要在各种安全和可靠性威胁下保持正确的功能。本文首先讨论了在不同系统层，即硬件（HW）和软件（SW）解决能源效率、可靠性和安全问题的现有方法。然后，我们讨论了如何通过硬件/软件级优化（如修剪、量化和近似）进一步提高边缘人工智能系统的性能（延迟）和能效。为了解决可靠性威胁（如永久性和瞬时性故障），我们强调了成本效益高的缓解技术，如故障感知培训和映射。此外，我们还简要讨论了解决安全威胁（如模型和数据损坏）的有效检测和保护技术。最后，我们将讨论如何将这些技术结合到一个集成的跨层框架中，以实现健壮且节能的边缘人工智能系统。
<details>	<summary>英文摘要</summary>	The security and privacy concerns along with the amount of data that is required to be processed on regular basis has pushed processing to the edge of the computing systems. Deploying advanced Neural Networks (NN), such as deep neural networks (DNNs) and spiking neural networks (SNNs), that offer state-of-the-art results on resource-constrained edge devices is challenging due to the stringent memory and power/energy constraints. Moreover, these systems are required to maintain correct functionality under diverse security and reliability threats. This paper first discusses existing approaches to address energy efficiency, reliability, and security issues at different system layers, i.e., hardware (HW) and software (SW). Afterward, we discuss how to further improve the performance (latency) and the energy efficiency of Edge AI systems through HW/SW-level optimizations, such as pruning, quantization, and approximation. To address reliability threats (like permanent and transient faults), we highlight cost-effective mitigation techniques, like fault-aware training and mapping. Moreover, we briefly discuss effective detection and protection techniques to address security threats (like model and data corruption). Towards the end, we discuss how these techniques can be combined in an integrated cross-layer framework for realizing robust and energy-efficient Edge AI systems. </details>
<details>	<summary>注释</summary>	To appear at the 40th IEEE/ACM International Conference on Computer-Aided Design (ICCAD), November 2021, Virtual Event </details>
<details>	<summary>邮件日期</summary>	2021年09月22日</details>

# 218、具有易训练性和鲁棒性的时间编码深脉冲神经网络
- [ ] Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance 
时间：2021年09月17日                         第一作者：Shibo Zhou                       [链接](https://arxiv.org/abs/1909.10837).                     
<details>	<summary>邮件日期</summary>	2021年09月20日</details>

# 217、具有共振和激发神经元的深脉冲神经网络
- [ ] Deep Spiking Neural Networks with Resonate-and-Fire Neurons 
时间：2021年09月16日                         第一作者：Badr AlKhamissi                       [链接](https://arxiv.org/abs/2109.08234).                     
## 摘要：在这项工作中，我们探索了一种新的脉冲神经网络（SNN）公式，其中共振和激发（RAF）神经元（Izhikevich，2001）通过反向传播进行梯度下降训练。RAF-SNN虽然在生物学上更合理，但在不同的网络配置中，使用相似或更少的参数，其性能可与机器学习文献中的传统模型相比或更高。引人注目的是，RAF-SNN在静态和动态条件下，对测试/训练时产生的噪声具有鲁棒性。与MNIST上的CNN相比，我们在测试时显示N（0，0.2）诱导噪声的绝对准确度高出25%。与N-MNIST上的LSTM相比，我们在训练时显示了70%的绝对准确度和20%的诱导噪声。
<details>	<summary>英文摘要</summary>	In this work, we explore a new Spiking Neural Network (SNN) formulation with Resonate-and-Fire (RAF) neurons (Izhikevich, 2001) trained with gradient descent via back-propagation. The RAF-SNN, while more biologically plausible, achieves performance comparable to or higher than conventional models in the Machine Learning literature across different network configurations, using similar or fewer parameters. Strikingly, the RAF-SNN proves robust against noise induced at testing/training time, under both static and dynamic conditions. Against CNN on MNIST, we show 25% higher absolute accuracy with N(0, 0.2) induced noise at testing time. Against LSTM on N-MNIST, we show 70% higher absolute accuracy with 20% induced noise at training time. </details>
<details>	<summary>注释</summary>	Preprint </details>
<details>	<summary>邮件日期</summary>	2021年09月20日</details>

# 216、基于加权神经元分配的脉冲神经网络视觉位置识别
- [ ] Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments 
时间：2021年09月14日                         第一作者：Somayeh Hussaini                       [链接](https://arxiv.org/abs/2109.06452).                     
## 摘要：脉冲神经网络（SNN）既有令人信服的潜在优势，包括能源效率和低延迟，也有挑战，包括事件脉冲的不可微性。该领域的许多初步研究已经将深度神经网络转换为等效的SNN，但这种转换方法可能会否定从头开发的基于SNN的方法的一些潜在优势。高性能SNN的一个有希望的领域是模板匹配和图像识别。本研究介绍了第一个用于视觉位置识别（VPR）任务的高性能SNN：给定查询图像，SNN必须从参考图像列表中找到最接近的匹配。这一新系统的核心是一种新的分配方案，它通过对单个位置编码神经元进行上加权和对多个不同参考位置作出响应的“模糊”神经元进行下加权，实现了一种形式的模糊信息显著性。在具有挑战性的牛津RobotCar和Nordland数据集上的一系列实验中，我们表明，我们的SNN实现了与最先进和经典技术相当的VPR性能，并且随着参考位置数量的增加，性能逐渐下降。我们的研究结果为SNN提供了一个重要的里程碑，SNN可以提供健壮、节能和低延迟的机器人定位。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) offer both compelling potential advantages, including energy efficiency and low latencies, and challenges including the non-differentiable nature of event spikes. Much of the initial research in this area has converted deep neural networks to equivalent SNNs, but this conversion approach potentially negates some of the potential advantages of SNN-based approaches developed from scratch. One promising area for high performance SNNs is template matching and image recognition. This research introduces the first high performance SNN for the Visual Place Recognition (VPR) task: given a query image, the SNN has to find the closest match out of a list of reference images. At the core of this new system is a novel assignment scheme that implements a form of ambiguity-informed salience, by up-weighting single-place-encoding neurons and down-weighting "ambiguous" neurons that respond to multiple different reference places. In a range of experiments on the challenging Oxford RobotCar and Nordland datasets, we show that our SNN achieves comparable VPR performance to state-of-the-art and classical techniques, and degrades gracefully in performance with an increasing number of reference places. Our results provide a significant milestone towards SNNs that can provide robust, energy-efficient and low latency robot localization. </details>
<details>	<summary>注释</summary>	8 pages, 6 figures, under review </details>
<details>	<summary>邮件日期</summary>	2021年09月15日</details>

# 215、BioLCNet：报酬调制局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2021年09月12日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
## 摘要：最近的研究表明，卷积神经网络（CNN）并不是唯一可行的图像分类方法。此外，CNN中使用的权重共享和反向传播并不符合灵长类视觉系统中存在的机制。为了提出一个生物学上更合理的解决方案，我们设计了一个局部连接的脉冲神经网络（SNN），该网络使用脉冲时间依赖性可塑性（STDP）及其报酬调制变量（R-STDP）学习规则进行训练。通过使用脉冲神经元和局部连接以及强化学习（RL），我们为我们提出的架构命名了BioLCNet。我们的网络由速率编码输入层、本地连接的隐藏层和解码输出层组成。输出层采用基于脉冲总体的投票方案进行解码。我们使用MNIST数据集获得图像分类精度，并评估奖励系统对不同目标响应的鲁棒性。
<details>	<summary>英文摘要</summary>	Recent studies have shown that convolutional neural networks (CNNs) are not the only feasible solution for image classification. Furthermore, weight sharing and backpropagation used in CNNs do not correspond to the mechanisms present in the primate visual system. To propose a more biologically plausible solution, we designed a locally connected spiking neural network (SNN) trained using spike-timing-dependent plasticity (STDP) and its reward-modulated variant (R-STDP) learning rules. The use of spiking neurons and local connections along with reinforcement learning (RL) led us to the nomenclature BioLCNet for our proposed architecture. Our network consists of a rate-coded input layer followed by a locally connected hidden layer and a decoding output layer. A spike population-based voting scheme is adopted for decoding in the output layer. We used the MNIST dataset to obtain image classification accuracy and to assess the robustness of our rewarding system to varying target responses. </details>
<details>	<summary>注释</summary>	8 pages, 5 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年09月14日</details>

# 214、用于基于事件的光流估计的时空递归网络
- [ ] Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation 
时间：2021年09月10日                         第一作者：Ziluo Ding                       [链接](https://arxiv.org/abs/2109.04871).                     
## 摘要：事件摄像机为视觉感知提供了很有希望的替代方案，特别是在高速和高动态范围的场景中。最近，许多深度学习方法在为许多基于事件的问题（如光流估计）提供无模型解决方案方面取得了巨大成功。然而，现有的深度学习方法并没有从建筑设计的角度很好地解决时间信息的重要性，也不能有效地提取时空特征。另一个利用脉冲神经网络的研究领域存在更深层次架构的训练问题。为了解决这些问题，提出了一种新的输入表示法，用于捕获事件的时间分布以增强信号。此外，我们还介绍了一种用于基于事件的光流估计的时空递归编码-解码神经网络结构，该结构利用卷积选通递归单元从一系列事件图像中提取特征映射。此外，我们的架构允许合并一些传统的基于帧的核心模块，如相关层和迭代残差细化方案。该网络在多车辆立体事件摄像机数据集上通过自监督学习进行端到端训练。我们已经证明，它大大优于所有现有的最先进的方法。
<details>	<summary>英文摘要</summary>	Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes. Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation. However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features. Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture. To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement. Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images. Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated. The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms all the existing state-of-the-art methods by a large margin. </details>
<details>	<summary>邮件日期</summary>	2021年09月13日</details>

# 213、HSMD：一种基于混合脉冲神经网络结构的目标运动检测算法
- [ ] HSMD: An object motion detection algorithm using a Hybrid Spiking Neural Network Architecture 
时间：2021年09月09日                         第一作者：Pedro Machado                       [链接](https://arxiv.org/abs/2109.04119).                     
## 摘要：运动物体的检测是脊椎动物视网膜执行的一项琐碎任务，但也是一项复杂的计算机视觉任务。物体运动敏感神经节细胞（OMS-GC）是视网膜中感知运动物体的特殊细胞。OMS-GC以连续信号作为输入，产生棘波模式作为输出，通过视神经传输到视皮层。本文提出的混合敏感运动检测器（HSMD）算法通过定制的3层脉冲神经网络（SNN）增强了GSOC动态背景减法（DBS）算法，该网络输出类似于OMS-GC的脉冲响应。该算法与OpenCV库中现有的背景减法（BS）方法进行了比较，特别是2012年变化检测（CDnet2012）和2014年变化检测（CDnet2014）基准数据集。结果表明，HSMD在所有竞争方法中总体排名第一，并且在所有八个测试指标中的四个类别上的性能优于所有其他算法。此外，本文提出的HSMD是第一个使用SNN来增强现有最先进的DBS（GSOC）算法的HSMD，结果表明SNN在实际应用中提供了接近实时的性能。
<details>	<summary>英文摘要</summary>	The detection of moving objects is a trivial task performed by vertebrate retinas, yet a complex computer vision task. Object-motion-sensitive ganglion cells (OMS-GC) are specialised cells in the retina that sense moving objects. OMS-GC take as input continuous signals and produce spike patterns as output, that are transmitted to the Visual Cortex via the optic nerve. The Hybrid Sensitive Motion Detector (HSMD) algorithm proposed in this work enhances the GSOC dynamic background subtraction (DBS) algorithm with a customised 3-layer spiking neural network (SNN) that outputs spiking responses akin to the OMS-GC. The algorithm was compared against existing background subtraction (BS) approaches, available on the OpenCV library, specifically on the 2012 change detection (CDnet2012) and the 2014 change detection (CDnet2014) benchmark datasets. The results show that the HSMD was ranked overall first among the competing approaches and has performed better than all the other algorithms on four of the categories across all the eight test metrics. Furthermore, the HSMD proposed in this paper is the first to use an SNN to enhance an existing state of the art DBS (GSOC) algorithm and the results demonstrate that the SNN provides near real-time performance in realistic applications. </details>
<details>	<summary>邮件日期</summary>	2021年09月10日</details>

# 212、Nengo上使用脉冲神经网络的稀疏分布记忆
- [ ] Sparse Distributed Memory using Spiking Neural Networks on Nengo 
时间：2021年09月07日                         第一作者：Rohan Deepak Ajwani                       [链接](https://arxiv.org/abs/2109.03111).                     
## 摘要：我们提出了一种基于脉冲神经网络（SNN）的稀疏分布内存（SDM）实现的Nengo框架。我们的工作基于Furber等人2004年以前的工作，使用N-of-M代码实现SDM。作为SDM设计的一个组成部分，我们在Nengo上使用SNN实现了相关矩阵存储器（CMM）。我们的SNN实现在Nengo上使用泄漏集成和火灾（LIF）脉冲神经元模型。我们的目标是了解基于SNN的SDM与传统SDM相比的性能。为此，我们在Nengo上模拟了传统和基于SNN的SDM和CMM。我们观察到，基于SNN的模型的性能与传统模型类似。为了评估不同SNN的性能，我们使用自适应LIF、脉冲校正线性单元和Izhikevich模型重复了实验，并获得了类似的结果。我们的结论是，使用脉冲神经元开发某些类型的联想记忆确实是可行的，这些神经元的记忆容量和其他特征与没有SNN时的性能相似。最后，我们实现了一个应用程序，其中使用N-of-M代码编码的MNIST图像与其标签关联并存储在基于SNN的SDM中。
<details>	<summary>英文摘要</summary>	We present a Spiking Neural Network (SNN) based Sparse Distributed Memory (SDM) implemented on the Nengo framework. We have based our work on previous work by Furber et al, 2004, implementing SDM using N-of-M codes. As an integral part of the SDM design, we have implemented Correlation Matrix Memory (CMM) using SNN on Nengo. Our SNN implementation uses Leaky Integrate and Fire (LIF) spiking neuron models on Nengo. Our objective is to understand how well SNN-based SDMs perform in comparison to conventional SDMs. Towards this, we have simulated both conventional and SNN-based SDM and CMM on Nengo. We observe that SNN-based models perform similarly as the conventional ones. In order to evaluate the performance of different SNNs, we repeated the experiment using Adaptive-LIF, Spiking Rectified Linear Unit, and Izhikevich models and obtained similar results. We conclude that it is indeed feasible to develop some types of associative memories using spiking neurons whose memory capacity and other features are similar to the performance without SNNs. Finally we have implemented an application where MNIST images, encoded with N-of-M codes, are associated with their labels and stored in the SNN-based SDM. </details>
<details>	<summary>注释</summary>	8 pages, 11 figures, accepted as poster in Bernstein Conference 2021 ACM-class: H.3.2; I.5.5 </details>
<details>	<summary>邮件日期</summary>	2021年09月08日</details>

# 211、具有易训练性和鲁棒性的时间编码深脉冲神经网络
- [ ] Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance 
时间：2021年09月06日                         第一作者：Shibo Zhou                       [链接](https://arxiv.org/abs/1909.10837).                     
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 210、脉冲神经网络集成
- [ ] Ensembles of Spiking Neural Networks 
时间：2021年09月06日                         第一作者：Georgiana Neculae                       [链接](https://arxiv.org/abs/2010.14619).                     
<details>	<summary>注释</summary>	16 pages, 3 tables, 5 figures MSC-class: 68T07, 62J12 </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 209、用于序列学习的具有改进固有递归动力学的脉冲神经网络
- [ ] Spiking Neural Networks with Improved Inherent Recurrence Dynamics for Sequential Learning 
时间：2021年09月04日                         第一作者：Wachirawit Ponghiran                        [链接](https://arxiv.org/abs/2109.01905).                     
## 摘要：具有泄漏集成和激发（LIF）神经元的脉冲神经网络（SNN）可以以事件驱动的方式运行，并具有随时间保留信息的内部状态，为节能的神经形态计算提供了机会，特别是在边缘设备上。然而，值得注意的是，许多关于SNN的有代表性的工作并没有充分证明其固有的重复性（保留过去信息的膜电位）对于顺序学习的有用性。大多数工作训练SNN识别静态图像通过人工扩展输入表示时间通过率编码。我们证明了SNN可以被训练用于序列任务，并提出了对LIF神经元网络的修改，使内部状态能够学习长序列，并使其固有的重现性能够适应消失梯度问题。然后，我们开发了一个训练方案，用改进的固有递归动力学来训练所提出的SNN。我们的训练方案允许脉冲神经元产生多位输出（与二进制脉冲相反），这有助于缓解脉冲神经元激活函数的导数与用于克服脉冲神经元不可微性的替代导数之间的失配。我们的实验结果表明，在TIMIT和LibriSpeech 100h数据集上提出的SNN体系结构的精度与LSTMs相当（分别在1.10%和0.36%范围内），但参数比LSTMs少2倍。在TIMIT和LibriSpeech 100h数据集上，与GRU相比，稀疏SNN输出还导致乘法运算节省10.13倍和11.14倍，GRU通常被视为LSTM的轻量级替代品。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) with leaky integrate and fire (LIF) neurons, can be operated in an event-driven manner and have internal states to retain information over time, providing opportunities for energy-efficient neuromorphic computing, especially on edge devices. Note, however, many representative works on SNNs do not fully demonstrate the usefulness of their inherent recurrence (membrane potentials retaining information about the past) for sequential learning. Most of the works train SNNs to recognize static images by artificially expanded input representation in time through rate coding. We show that SNNs can be trained for sequential tasks and propose modifications to a network of LIF neurons that enable internal states to learn long sequences and make their inherent recurrence resilient to the vanishing gradient problem. We then develop a training scheme to train the proposed SNNs with improved inherent recurrence dynamics. Our training scheme allows spiking neurons to produce multi-bit outputs (as opposed to binary spikes) which help mitigate the mismatch between a derivative of spiking neurons' activation function and a surrogate derivative used to overcome spiking neurons' non-differentiability. Our experimental results indicate that the proposed SNN architecture on TIMIT and LibriSpeech 100h dataset yields accuracy comparable to that of LSTMs (within 1.10% and 0.36%, respectively), but with 2x fewer parameters than LSTMs. The sparse SNN outputs also lead to 10.13x and 11.14x savings in multiplication operations compared to GRUs, which is generally con-sidered as a lightweight alternative to LSTMs, on TIMIT and LibriSpeech 100h datasets, respectively. </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 208、卷积脉冲神经网络中基于脉冲时间位移的误差反向传播
- [ ] Spike time displacement based error backpropagation in convolutional spiking neural networks 
时间：2021年08月31日                         第一作者：Maryam Mirsadeghi                       [链接](https://arxiv.org/abs/2108.13621).                     
## 摘要：我们最近提出了STiDi BP算法，该算法避免了向后递归梯度计算，用于训练具有单脉冲时间编码的多层脉冲神经网络（SNN）。该算法采用线性近似来计算脉冲潜伏期相对于膜电位的导数，并使用具有分段线性突触后电位的脉冲神经元来降低计算成本和神经处理的复杂性。在本文中，我们扩展了STiDi-BP算法，将其应用于更深层次的卷积结构。基于两个流行基准MNIST和Fashion MNIST数据集的图像分类任务的评估结果表明，该算法的准确率分别为99.2%和92.8%，证实了该算法在深度SNN中的适用性。我们考虑的另一个问题是内存存储和计算成本的减少。要做到这一点，我们考虑卷积SNN（CSNN）与两组权重：实值权重，更新在后向传递和它们的符号，二进制权重，在前馈过程中使用。我们在两个数据集MNIST和Fashion MNIST上对二进制CSNN进行了评估，并获得了可接受的性能，与实值权重相关的精度下降可以忽略不计（分别约为$0.6%$和$0.8%$）。
<details>	<summary>英文摘要</summary>	We recently proposed the STiDi-BP algorithm, which avoids backward recursive gradient computation, for training multi-layer spiking neural networks (SNNs) with single-spike-based temporal coding. The algorithm employs a linear approximation to compute the derivative of the spike latency with respect to the membrane potential and it uses spiking neurons with piecewise linear postsynaptic potential to reduce the computational cost and the complexity of neural processing. In this paper, we extend the STiDi-BP algorithm to employ it in deeper and convolutional architectures. The evaluation results on the image classification task based on two popular benchmarks, MNIST and Fashion-MNIST datasets with the accuracies of respectively 99.2% and 92.8%, confirm that this algorithm has been applicable in deep SNNs. Another issue we consider is the reduction of memory storage and computational cost. To do so, we consider a convolutional SNN (CSNN) with two sets of weights: real-valued weights that are updated in the backward pass and their signs, binary weights, that are employed in the feedforward process. We evaluate the binary CSNN on two datasets of MNIST and Fashion-MNIST and obtain acceptable performance with a negligible accuracy drop with respect to real-valued weights (about $0.6%$ and $0.8%$ drops, respectively). </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 207、星形胶质细胞介导多层神经元-星形胶质细胞网络中的类似记忆
- [ ] Astrocytes mediate analogous memory in a multi-layer neuron-astrocytic network 
时间：2021年08月31日                         第一作者：Yuliya Tsybina                       [链接](https://arxiv.org/abs/2108.13414).                     
## 摘要：短期工作记忆的神经过程建模一直是神经科学许多理论研究的重点。在这里，我们提出了一个脉冲神经元网络（SNN）的数学模型，演示了一段信息如何作为一种健壮的活动模式保持几秒钟，然后在没有其他刺激的情况下完全消失。由于伴随SNN的星形胶质细胞的激活，这种短期记忆痕迹得以保留。星形胶质细胞以秒的时间尺度呈现钙瞬变。这些瞬变进一步调节突触传递的效率，从而通过释放胶质传递素在不同时间尺度上调节相邻神经元的放电频率。我们展示了这种瞬变如何持续编码神经元放电的频率，并提供了类似信息的可靠短期存储。这种短期记忆可以将操作信息保留几秒钟，然后完全忘记，以避免与即将出现的模式重叠。SNN通过局部细胞间扩散连接与星形细胞层相互连接。星形胶质细胞只有在相邻神经元同步激发时才被激活，例如当信息模式被加载时。为了举例说明，我们拍摄了人们脸部的灰度照片，其中的灰度编码了刺激神经元的外加电流水平。星形胶质细胞反馈通过改变神经元放电频率来调节（促进）突触传递。我们展示了如何加载任意模式，然后将其存储一定的时间间隔，并在将适当的线索模式应用于输入时进行检索。
<details>	<summary>英文摘要</summary>	Modeling the neuronal processes underlying short-term working memory remains the focus of many theoretical studies in neuroscience. Here we propose a mathematical model of spiking neuron network (SNN) demonstrating how a piece of information can be maintained as a robust activity pattern for several seconds then completely disappear if no other stimuli come. Such short-term memory traces are preserved due to the activation of astrocytes accompanying the SNN. The astrocytes exhibit calcium transients at a time scale of seconds. These transients further modulate the efficiency of synaptic transmission and, hence, the firing rate of neighboring neurons at diverse timescales through gliotransmitter release. We show how such transients continuously encode frequencies of neuronal discharges and provide robust short-term storage of analogous information. This kind of short-term memory can keep operative information for seconds, then completely forget it to avoid overlapping with forthcoming patterns. The SNN is inter-connected with the astrocytic layer by local inter-cellular diffusive connections. The astrocytes are activated only when the neighboring neurons fire quite synchronously, e.g. when an information pattern is loaded. For illustration, we took greyscale photos of people's faces where the grey level encoded the level of applied current stimulating the neurons. The astrocyte feedback modulates (facilitates) synaptic transmission by varying the frequency of neuronal firing. We show how arbitrary patterns can be loaded, then stored for a certain interval of time, and retrieved if the appropriate clue pattern is applied to the input. </details>
<details>	<summary>注释</summary>	18 pages, 6 figures, 1 table, Appendix </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 206、基于旋转不变卷积的大规模点云位置识别
- [ ] Attentive Rotation Invariant Convolution for Point Cloud-based Large Scale Place Recognition 
时间：2021年08月29日                         第一作者：Zhaoxin Fan                       [链接](https://arxiv.org/abs/2108.12790).                     
## 摘要：自动驾驶和同步定位与地图（SLAM）在现实世界中变得越来越重要，基于点云的大规模地点识别是其中的一个亮点。以往的位置识别方法都将该任务视为一个点云检索问题，取得了良好的性能。然而，它们都有一个共同的缺陷：它们无法处理旋转点云时的情况，这是常见的，例如，当视点或摩托车类型发生变化时。为了解决这个问题，本文提出了一种注意旋转不变卷积（ARIConv）。ARIConv在其结构中采用了三种旋转不变特征（RIF）：球形信号（SS）、单个局部旋转不变特征（ILRIF）和组局部旋转不变特征（GLRIF）来学习旋转不变卷积核，这些特征对旋转不变点云特征的学习具有鲁棒性。更重要的是，为了突出关键的RIF，我们在ARIConv中注入了一个关注模块，在学习内核时赋予不同的RIF不同的重要性。最后，利用ARIConv，我们构建了一个类似DenseNet的网络结构来学习用于检索的旋转不敏感全局描述符。我们的实验表明，当旋转点云扫描时，我们的模型可以在大规模位置识别任务中实现最先进的性能，并且可以在原始非旋转数据集上实现与大多数现有方法相当的结果。
<details>	<summary>英文摘要</summary>	Autonomous Driving and Simultaneous Localization and Mapping(SLAM) are becoming increasingly important in real world, where point cloud-based large scale place recognition is the spike of them. Previous place recognition methods have achieved acceptable performances by regarding the task as a point cloud retrieval problem. However, all of them are suffered from a common defect: they can't handle the situation when the point clouds are rotated, which is common, e.g, when viewpoints or motorcycle types are changed. To tackle this issue, we propose an Attentive Rotation Invariant Convolution (ARIConv) in this paper. The ARIConv adopts three kind of Rotation Invariant Features (RIFs): Spherical Signals (SS), Individual-Local Rotation Invariant Features (ILRIF) and Group-Local Rotation Invariant features (GLRIF) in its structure to learn rotation invariant convolutional kernels, which are robust for learning rotation invariant point cloud features. What's more, to highlight pivotal RIFs, we inject an attentive module in ARIConv to give different RIFs different importance when learning kernels. Finally, utilizing ARIConv, we build a DenseNet-like network architecture to learn rotation-insensitive global descriptors used for retrieving. We experimentally demonstrate that our model can achieve state-of-the-art performance on large scale place recognition task when the point cloud scans are rotated and can achieve comparable results with most of existing methods on the original non-rotated datasets. </details>
<details>	<summary>注释</summary>	10 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月31日</details>

# 205、热动力学介导的随机磁性隧道结的内在脉冲时间依赖塑性
- [ ] Intrinsic Spike Timing Dependent Plasticity in Stochastic Magnetic Tunnel Junctions Mediated by Heat Dynamics 
时间：2021年08月28日                         第一作者：Humberto Inzunza Velarde                       [链接](https://arxiv.org/abs/2108.12684).                     
## 摘要：对高效认知计算的追求引起了神经形态计算领域的广泛研究兴趣。神经形态计算旨在利用固态设备和电路模拟生物神经元和突触的行为。在各种方法中，新兴的非易失性记忆技术对模拟神经突触行为特别感兴趣。这些设备可以将生物神经元和突触的丰富动态映射到其固有的设备物理上。在这封信中，我们重点研究了生物突触的脉冲时间依赖性可塑性（STDP）行为，并提出了一种在磁隧道结（MTJ）器件中实现STDP行为的方法。具体而言，我们利用与时间相关的热动力学和MTJ对瞬时温度的响应来模拟STDP行为。我们的模拟基于磁化动力学的宏观自旋模型，结果表明，通过将简单的电压波形作为MTJ器件前后神经元的脉冲响应，可以在随机磁性隧道结中模拟STDP。
<details>	<summary>英文摘要</summary>	The quest for highly efficient cognitive computing has led to extensive research interest for the field of neuromorphic computing. Neuromorphic computing aims to mimic the behavior of biological neurons and synapses using solid-state devices and circuits. Among various approaches, emerging non-volatile memory technologies are of special interest for mimicking neuro-synaptic behavior. These devices allow the mapping of the rich dynamics of biological neurons and synapses onto their intrinsic device physics. In this letter, we focus on Spike Timing Dependent Plasticity (STDP) behavior of biological synapses and propose a method to implement the STDP behavior in Magnetic Tunnel Junction (MTJ) devices. Specifically, we exploit the time-dependent heat dynamics and the response of an MTJ to the instantaneous temperature to imitate the STDP behavior. Our simulations, based on a macro-spin model for magnetization dynamics, show that, STDP can be imitated in stochastic magnetic tunnel junctions by applying simple voltage waveforms as the spiking response of pre- and post-neurons across an MTJ device. </details>
<details>	<summary>邮件日期</summary>	2021年08月31日</details>

# 204、一种将脉冲神经网络映射到多核神经形态硬件的设计流程
- [ ] A Design Flow for Mapping Spiking Neural Networks to Many-Core Neuromorphic Hardware 
时间：2021年08月27日                         第一作者：Shihao Song                       [链接](https://arxiv.org/abs/2108.12444).                     
## 摘要：许多核心神经形态硬件的设计正变得越来越复杂，因为这些系统预计将执行大型机器学习模型。为了解决设计的复杂性，需要一个可预测的设计流程来保证实时性能，例如延迟和吞吐量，而不会显著增加计算核心的缓冲区需求。同步数据流图（SDFG）用于流应用程序到多处理器系统的可预测映射。我们提出了一种基于SDFG的设计流程，用于将脉冲神经网络（SNN）映射到多个核心神经形态硬件，目的是探索吞吐量和缓冲区大小之间的权衡。提出的设计流程集成了一种基于Kernighan-Lin图划分启发式的迭代划分方法，创建SNN集群，使每个集群都可以映射到硬件的核心。分区方法最小化了集群间的脉冲通信，从而提高了硬件共享互连的延迟。接下来，设计流程使用粒子群优化（PSO）的一个实例（一种进化算法）将集群映射到核心，探索吞吐量和缓冲区大小的设计空间。Pareto最优映射保留在设计流中，允许系统设计者选择满足设计的吞吐量和缓冲区大小要求的Pareto映射。我们使用五个大型卷积神经网络（CNN）模型评估了设计流程。结果表明，与最先进的基于数据流的映射解决方案相比，最大吞吐量提高63%，缓冲区大小要求降低10%。
<details>	<summary>英文摘要</summary>	The design of many-core neuromorphic hardware is getting more and more complex as these systems are expected to execute large machine learning models. To deal with the design complexity, a predictable design flow is needed to guarantee real-time performance such as latency and throughput without significantly increasing the buffer requirement of computing cores. Synchronous Data Flow Graphs (SDFGs) are used for predictable mapping of streaming applications to multiprocessor systems. We propose an SDFG-based design flow for mapping spiking neural networks (SNNs) to many-core neuromorphic hardware with the objective of exploring the tradeoff between throughput and buffer size. The proposed design flow integrates an iterative partitioning approach, based on Kernighan-Lin graph partitioning heuristic, creating SNN clusters such that each cluster can be mapped to a core of the hardware. The partitioning approach minimizes the inter-cluster spike communication, which improves latency on the shared interconnect of the hardware. Next, the design flow maps clusters to cores using an instance of the Particle Swarm Optimization (PSO), an evolutionary algorithm, exploring the design space of throughput and buffer size. Pareto optimal mappings are retained from the design flow, allowing system designers to select a Pareto mapping that satisfies throughput and buffer size requirements of the design. We evaluated the design flow using five large-scale convolutional neural network (CNN) models. Results demonstrate 63% higher maximum throughput and 10% lower buffer size requirement compared to state-of-the-art dataflow-based mapping solutions. </details>
<details>	<summary>注释</summary>	To appear in ICCAD 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月31日</details>

# 203、反向传播算法在Spiking神经形态硬件上的实现
- [ ] The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware 
时间：2021年08月26日                         第一作者：Alpha Renner                       [链接](https://arxiv.org/abs/2106.07030).                     
<details>	<summary>注释</summary>	21 pages, 5 figures, Changes v1->v2: minor changes of text and formatting, correction of total power in supplementary Table III Report-no: LA-UR-21-24457 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年08月27日</details>

# 202、脉冲神经网络的Hessian感知量化
- [ ] Hessian Aware Quantization of Spiking Neural Networks 
时间：2021年08月23日                         第一作者：Hin Wai Lui                        [链接](https://arxiv.org/abs/2104.14117).                     
<details>	<summary>邮件日期</summary>	2021年08月25日</details>

# 201、ReSpawn：考虑不可靠记忆的脉冲神经网络的节能容错
- [ ] ReSpawn: Energy-Efficient Fault-Tolerance for Spiking Neural Networks considering Unreliable Memories 
时间：2021年08月23日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2108.10271).                     
## 摘要：由于其受生物启发的计算能力，脉冲神经网络（SNN）具有低能量和无监督学习能力的潜力。然而，如果在存储器中存在硬件引起的故障（可能来自制造缺陷或电压引起的近似误差）的情况下执行它们的处理，则它们可能会受到精度降低的影响。由于最近的工作仍然集中于SNN中的故障建模和随机故障注入，因此SNN硬件架构中的内存故障对准确性的影响以及相应的故障缓解技术没有得到彻底的探讨。为此，我们提出了ReSpawn，这是一个新的框架，用于缓解弹性和节能SNN的片外和片内存储器中故障的负面影响。ReSpawn的关键机制是：（1）分析SNNs的容错性；（2）通过（a）存储器中的故障感知映射（FAM）和（b）故障感知训练和映射（FATM）提高SNN容错性。如果训练数据集不完全可用，则通过有效的位洗牌技术使用FAM，将有效位放在非故障存储单元上，将不重要位放在故障存储单元上，同时最小化内存访问能量。同时，如果训练数据集完全可用，则在数据映射和训练过程中，通过考虑故障存储单元，采用FATM。实验结果表明，与没有故障缓解技术的基线SNN相比，使用故障感知映射方案的ReSpawn在没有重新训练的情况下，对于包含900个神经元的网络，准确率提高了70%。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have shown a potential for having low energy with unsupervised learning capabilities due to their biologically-inspired computation. However, they may suffer from accuracy degradation if their processing is performed under the presence of hardware-induced faults in memories, which can come from manufacturing defects or voltage-induced approximation errors. Since recent works still focus on the fault-modeling and random fault injection in SNNs, the impact of memory faults in SNN hardware architectures on accuracy and the respective fault-mitigation techniques are not thoroughly explored. Toward this, we propose ReSpawn, a novel framework for mitigating the negative impacts of faults in both the off-chip and on-chip memories for resilient and energy-efficient SNNs. The key mechanisms of ReSpawn are: (1) analyzing the fault tolerance of SNNs; and (2) improving the SNN fault tolerance through (a) fault-aware mapping (FAM) in memories, and (b) fault-aware training-and-mapping (FATM). If the training dataset is not fully available, FAM is employed through efficient bit-shuffling techniques that place the significant bits on the non-faulty memory cells and the insignificant bits on the faulty ones, while minimizing the memory access energy. Meanwhile, if the training dataset is fully available, FATM is employed by considering the faulty memory cells in the data mapping and training processes. The experimental results show that, compared to the baseline SNN without fault-mitigation techniques, ReSpawn with a fault-aware mapping scheme improves the accuracy by up to 70% for a network with 900 neurons without retraining. </details>
<details>	<summary>注释</summary>	To appear at the 40th IEEE/ACM International Conference on Computer-Aided Design (ICCAD), November 2021, Virtual Event </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 200、结合可学习膜时间常数增强脉冲神经网络的学习
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2021年08月17日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>注释</summary>	Accepted by International Conference on Computer Vision (ICCV) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 199、用脉冲神经网络进行类比推理和关系推理
- [ ] Analogical and Relational Reasoning with Spiking Neural Networks 
时间：2021年08月17日                         第一作者：Rollin Omari                       [链接](https://arxiv.org/abs/2010.06746).                     
<details>	<summary>注释</summary>	Problems were discovered with the details of the experiments involving the LSM neural network, the results reported were not correct. A new version of the paper is in progress with corrected results ACM-class: F.2.2; I.2.6; I.2.10; I.5.1; I.5.3 </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 198、脉冲神经元的线性约束学习
- [ ] Linear Constraints Learning for Spiking Neurons 
时间：2021年08月11日                         第一作者：Huy Le Nguyen                       [链接](https://arxiv.org/abs/2103.12564).                     
<details>	<summary>注释</summary>	35 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月12日</details>

# 197、鲁棒视觉目标跟踪的多域协同特征表示
- [ ] Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking 
时间：2021年08月10日                         第一作者：Jiqing Zhang                        [链接](https://arxiv.org/abs/2108.04521).                     
## 摘要：联合利用多个不同但互补的领域信息已被证明是执行鲁棒目标跟踪的有效方法。本文致力于在挑战场景中有效地表示和利用帧域和事件域的互补特征来提高目标跟踪性能。具体来说，我们提出了公共特征提取器（CFE）来从RGB域和事件域学习潜在的公共表示。为了学习这两个领域的独特特征，我们利用一种基于脉冲神经网络的独特事件提取器（UEE）来提取事件领域中在某些挑战性条件下RGB可能遗漏的边缘线索，基于深度卷积神经网络的RGB（UER）抽取器用于提取RGB域的纹理和语义信息。在标准RGB基准和真实事件跟踪数据集上的大量实验证明了该方法的有效性。我们展示了我们的方法优于所有比较先进的跟踪算法，并验证了基于事件的数据是在具有挑战性的场景中进行跟踪的有力线索。
<details>	<summary>英文摘要</summary>	Jointly exploiting multiple different yet complementary domain information has been proven to be an effective way to perform robust object tracking. This paper focuses on effectively representing and utilizing complementary features from the frame domain and event domain for boosting object tracking performance in challenge scenarios. Specifically, we propose Common Features Extractor (CFE) to learn potential common representations from the RGB domain and event domain. For learning the unique features of the two domains, we utilize a Unique Extractor for Event (UEE) based on Spiking Neural Networks to extract edge cues in the event domain which may be missed in RGB in some challenging conditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional Neural Networks to extract texture and semantic information in RGB domain. Extensive experiments on standard RGB benchmark and real event tracking dataset demonstrate the effectiveness of the proposed approach. We show our approach outperforms all compared state-of-the-art tracking algorithms and verify event-based data is a powerful cue for tracking in challenging scenes. </details>
<details>	<summary>邮件日期</summary>	2021年08月11日</details>

# 196、脉冲时间依赖的可塑性训练脉冲神经网络的泛化特性
- [ ] Characterization of Generalizability of Spike Timing Dependent Plasticity trained Spiking Neural Networks 
时间：2021年08月09日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2105.14677).                     
<details>	<summary>注释</summary>	23 pages, submitted to Frontiers in Neuroscience. arXiv admin note: text overlap with arXiv:2010.08195, arXiv:2006.09313 by other authors </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 195、基于神经形态芯片的无人机事件驱动视觉与控制
- [ ] Event-driven Vision and Control for UAVs on a Neuromorphic Chip 
时间：2021年08月08日                         第一作者：Antonio Vitale                       [链接](https://arxiv.org/abs/2108.03694).                     
## 摘要：与传统图像传感器相比，基于事件的视觉传感器在无人机高速控制中实现了高达三个数量级的速度与功耗权衡。基于事件的摄像头产生稀疏的事件流，可以比图像更有效地处理，延迟更低，从而实现超快速的视觉驱动控制。在这里，我们探讨如何将基于事件的视觉算法实现为神经形态芯片上的脉冲神经元网络，并将其用于无人机控制器。我们展示了基于事件的感知在芯片上的无缝集成如何导致更快的控制率和更低的延迟。此外，我们还演示了如何使用片上学习实现SNN控制器的在线自适应。我们的芯片上脉冲神经元网络是基于神经形态视觉的控制器解决高速无人机控制任务的第一个例子。神经形态硬件中出色的处理可扩展性为将来解决更具挑战性的视觉任务和将视觉感知集成到快速控制回路中提供了可能。
<details>	<summary>英文摘要</summary>	Event-based vision sensors achieve up to three orders of magnitude better speed vs. power consumption trade off in high-speed control of UAVs compared to conventional image sensors. Event-based cameras produce a sparse stream of events that can be processed more efficiently and with a lower latency than images, enabling ultra-fast vision-driven control. Here, we explore how an event-based vision algorithm can be implemented as a spiking neuronal network on a neuromorphic chip and used in a drone controller. We show how seamless integration of event-based perception on chip leads to even faster control rates and lower latency. In addition, we demonstrate how online adaptation of the SNN controller can be realised using on-chip learning. Our spiking neuronal network on chip is the first example of a neuromorphic vision-based controller solving a high-speed UAV control task. The excellent scalability of processing in neuromorphic hardware opens the possibility to solve more challenging visual tasks in the future and integrate visual perception in fast control loops. </details>
<details>	<summary>注释</summary>	7 pages, 7 figures, 1 table </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 194、强化学习剂中高温提取神经元峰电位
- [ ] Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents 
时间：2021年08月05日                         第一作者：Ling Zhang                       [链接](https://arxiv.org/abs/2108.10078).                     
## 摘要：与深度神经网络（DNN）相比，脉冲神经网络（SNN）具有更快的处理速度、更低的能耗和更高的生物可解释性，有望接近强人工智能。强化学习类似于生物学学习。研究SNN与RL的结合具有重要意义。提出了基于STBP的脉冲蒸馏网络（SDN）强化学习方法。该方法采用蒸馏法，有效地避免了STBP算法的缺点，在分类上可以达到SOTA性能，并且可以得到更小、更快收敛和更低功耗的SNN强化学习模型。实验表明，该方法比传统的SNN强化学习和DNN强化学习方法收敛速度快，约快1000个历元，获得的SNN比DNN小200倍。我们还将SDN部署到PKU nc64c芯片上，证明SDN的功耗比DNN低，在大规模设备上SDN的功耗比DNN低600多倍。SDN提供了一种新的SNN强化学习方式，并能实现SOTA性能，证明了SNN强化学习进一步发展的可能性。
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN), compared with depth neural network (DNN), has faster processing speed, lower energy consumption and more biological interpretability, which is expected to approach Strong AI. Reinforcement learning is similar to learning in biology. It is of great significance to study the combination of SNN and RL. We propose the reinforcement learning method of spike distillation network (SDN) with STBP. This method uses distillation to effectively avoid the weakness of STBP, which can achieve SOTA performance in classification, and can obtain a smaller, faster convergence and lower power consumption SNN reinforcement learning model. Experiments show that our method can converge faster than traditional SNN reinforcement learning and DNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN 200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which proves that SDN has lower power consumption than DNN, and the power consumption of SDN is more than 600 times lower than DNN on large-scale devices. SDN provides a new way of SNN reinforcement learning, and can achieve SOTA performance, which proves the possibility of further development of SNN reinforcement learning. </details>
<details>	<summary>注释</summary>	7 pages, 5 figures, conference </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 193、使用局部递归模体构建递归脉冲神经网络和降低风险的结构优化
- [ ] Composing Recurrent Spiking Neural Networks using Locally-Recurrent Motifs and Risk-Mitigating Architectural Optimization 
时间：2021年08月04日                         第一作者：Wenrui Zhang                       [链接](https://arxiv.org/abs/2108.01793).                     
## 摘要：在神经电路中，循环连通性对网络的功能和稳定性起着至关重要的作用。然而，现有的递归脉冲神经网络（RSNN）通常是由随机连接构造的，没有经过优化。尽管RSNN可以产生对记忆形成和学习至关重要的丰富动态，但RSNN的系统架构优化仍然是一个开放性挑战。我们的目标是通过新的可扩展RSNN体系结构和自动化体系结构优化，实现大型RSNN的系统设计。我们基于一种称为稀疏连接的递归模体层（SC-ML）的层结构构建RSNN，该层结构由多个通过稀疏横向连接连接在一起的小递归模体组成。基序的小尺寸和稀疏的基序间连接导致RSNN架构可扩展到大网络尺寸。我们进一步提出了一种称为混合风险缓解架构搜索（HRMAS）的方法，以系统地优化所提出的循环模体和SC-ML层架构的拓扑结构。HRMAS是一个交替的两步优化过程，通过引入一种新的生物启发的“自我修复”机制，通过内在可塑性，我们减轻了由体系结构变化引起的网络不稳定和性能下降的风险。内在可塑性被引入每个HRMAS迭代的第二步，并在RSNN架构“进化”过程中，作为对第一步引入的结构和突触重量修改的无监督快速自适应。据作者所知，这是第一个对RSNNs进行系统架构优化的工作。使用一个语音和三个神经形态数据集，我们展示了所提出的自动化架构优化比现有手动设计的RSNN带来的显著性能改进。
<details>	<summary>英文摘要</summary>	In neural circuits, recurrent connectivity plays a crucial role in network function and stability. However, existing recurrent spiking neural networks (RSNNs) are often constructed by random connections without optimization. While RSNNs can produce rich dynamics that are critical for memory formation and learning, systemic architectural optimization of RSNNs is still an opening challenge. We aim to enable systemic design of large RSNNs via a new scalable RSNN architecture and automated architectural optimization. We compose RSNNs based on a layer architecture called Sparsely-Connected Recurrent Motif Layer (SC-ML) that consists of multiple small recurrent motifs wired together by sparse lateral connections. The small size of the motifs and sparse inter-motif connectivity leads to an RSNN architecture scalable to large network sizes. We further propose a method called Hybrid Risk-Mitigating Architectural Search (HRMAS) to systematically optimize the topology of the proposed recurrent motifs and SC-ML layer architecture. HRMAS is an alternating two-step optimization process by which we mitigate the risk of network instability and performance degradation caused by architectural change by introducing a novel biologically-inspired "self-repairing" mechanism through intrinsic plasticity. The intrinsic plasticity is introduced to the second step of each HRMAS iteration and acts as unsupervised fast self-adaption to structural and synaptic weight modifications introduced by the first step during the RSNN architectural "evolution". To the best of the authors' knowledge, this is the first work that performs systematic architectural optimization of RSNNs. Using one speech and three neuromorphic datasets, we demonstrate the significant performance improvement brought by the proposed automated architecture optimization over existing manually-designed RSNNs. </details>
<details>	<summary>注释</summary>	20 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月05日</details>

# 192、具有相变记忆突触的脉冲递归神经网络的在线训练
- [ ] Online Training of Spiking Recurrent Neural Networks with Phase-Change Memory Synapses 
时间：2021年08月04日                         第一作者：Yigit Demirag                       [链接](https://arxiv.org/abs/2108.01804).                     
## 摘要：脉冲递归神经网络（RNN）由于其丰富的时间动态性和稀疏的处理能力，在解决各种复杂的认知和运动任务方面是一种很有前途的工具。然而，在专用神经形态硬件上训练脉冲RNN仍然是一个开放的挑战。这主要是由于缺乏本地、硬件友好的学习机制，即使在权重分辨率有限的情况下，也无法解决暂时的信用分配问题并确保稳定的网络动态。如果人们利用记忆器件进行内存计算来解决冯·诺依曼瓶颈问题，而牺牲了脉冲RNN的计算和工作内存的可变性，那么这些挑战将进一步加剧。为了解决这些挑战，并在记忆性神经形态RNN中实现在线学习，我们提出了一个基于精确和全面的相变记忆（PCM）器件模型的差分结构交叉阵列仿真框架。我们使用最近提出的e-prop学习规则训练了一个脉冲RNN，其权值在所提出的仿真框架中进行了仿真。虽然e-prop局部近似于理想的突触更新，但由于大量PCM非理想性，很难在记忆基底上实现更新。我们比较了几种广泛采用的权值更新方案，这些方案主要是为了应对这些设备的非理想性，并证明累积梯度可以在线有效地训练记忆基底上的脉冲RNN。
<details>	<summary>英文摘要</summary>	Spiking recurrent neural networks (RNNs) are a promising tool for solving a wide variety of complex cognitive and motor tasks, due to their rich temporal dynamics and sparse processing. However training spiking RNNs on dedicated neuromorphic hardware is still an open challenge. This is due mainly to the lack of local, hardware-friendly learning mechanisms that can solve the temporal credit assignment problem and ensure stable network dynamics, even when the weight resolution is limited. These challenges are further accentuated, if one resorts to using memristive devices for in-memory computing to resolve the von-Neumann bottleneck problem, at the expense of a substantial increase in variability in both the computation and the working memory of the spiking RNNs. To address these challenges and enable online learning in memristive neuromorphic RNNs, we present a simulation framework of differential-architecture crossbar arrays based on an accurate and comprehensive Phase-Change Memory (PCM) device model. We train a spiking RNN whose weights are emulated in the presented simulation framework, using a recently proposed e-prop learning rule. Although e-prop locally approximates the ideal synaptic updates, it is difficult to implement the updates on the memristive substrate due to substantial PCM non-idealities. We compare several widely adapted weight update schemes that primarily aim to cope with these device non-idealities and demonstrate that accumulating gradients can enable online and efficient training of spiking RNN on memristive substrates. </details>
<details>	<summary>邮件日期</summary>	2021年08月05日</details>

# 191、DFSynthesizer：基于数据流的神经网络到神经形态硬件的合成
- [ ] DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware 
时间：2021年08月04日                         第一作者：Shihao Song                       [链接](https://arxiv.org/abs/2108.02023).                     
## 摘要：脉冲神经网络（SNN）是一种新兴的计算模型，它使用事件驱动激活和仿生学习算法。基于SNN的机器学习程序通常在基于tile的神经形态硬件平台上执行，其中每个tile由一个称为crossbar的计算单元组成，该计算单元映射程序的神经元和突触。然而，在现成的神经形态硬件上合成这样的程序是一个挑战。这是因为硬件固有的资源和延迟限制，这会影响模型性能（例如精度）和硬件性能（例如吞吐量）。我们提出了DFSynthesizer，这是一个端到端的框架，用于将基于SNN的机器学习程序合成到神经形态硬件。拟议的框架分为四个步骤。首先，它分析一个机器学习程序，并使用代表性数据生成SNN工作负载。其次，它对SNN工作负载进行分区，并生成适合目标神经形态硬件交叉条的集群。第三，它利用同步数据流图（SDFG）丰富的语义来表示集群SNN程序，允许根据关键硬件约束（如交叉杆的数量、每个交叉杆的尺寸、磁贴上的缓冲区空间和磁贴通信带宽）进行性能分析。最后，它使用一种新的调度算法在硬件的横杆上执行集群，从而保证硬件性能。我们用10个常用的机器学习程序来评估DFSynthesizer。我们的结果表明，与当前的映射方法相比，DFSynthesizer提供了更严格的性能保证。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) are an emerging computation model, which uses event-driven activation and bio-inspired learning algorithms. SNN-based machine-learning programs are typically executed on tile- based neuromorphic hardware platforms, where each tile consists of a computation unit called crossbar, which maps neurons and synapses of the program. However, synthesizing such programs on an off-the-shelf neuromorphic hardware is challenging. This is because of the inherent resource and latency limitations of the hardware, which impact both model performance, e.g., accuracy, and hardware performance, e.g., throughput. We propose DFSynthesizer, an end-to-end framework for synthesizing SNN-based machine learning programs to neuromorphic hardware. The proposed framework works in four steps. First, it analyzes a machine-learning program and generates SNN workload using representative data. Second, it partitions the SNN workload and generates clusters that fit on crossbars of the target neuromorphic hardware. Third, it exploits the rich semantics of Synchronous Dataflow Graph (SDFG) to represent a clustered SNN program, allowing for performance analysis in terms of key hardware constraints such as number of crossbars, dimension of each crossbar, buffer space on tiles, and tile communication bandwidth. Finally, it uses a novel scheduling algorithm to execute clusters on crossbars of the hardware, guaranteeing hardware performance. We evaluate DFSynthesizer with 10 commonly used machine-learning programs. Our results demonstrate that DFSynthesizer provides much tighter performance guarantee compared to current mapping approaches. </details>
<details>	<summary>注释</summary>	Accepted for publication at ACM Transactions on Embedded Computing </details>
<details>	<summary>邮件日期</summary>	2021年08月05日</details>

# 190、脉冲神经形态芯片学习纠缠量子态
- [ ] Spiking neuromorphic chip learns entangled quantum states 
时间：2021年08月03日                         第一作者：Stefanie Czischek                       [链接](https://arxiv.org/abs/2008.01039).                     
<details>	<summary>注释</summary>	22 pages, 6 figures Submission to SciPost </details>
<details>	<summary>邮件日期</summary>	2021年08月04日</details>

# 189、形成具有迭代优胜者的单元组件需要所有计算和激发抑制平衡
- [ ] Formation of cell assemblies with iterative winners-take-all computation and excitation-inhibition balance 
时间：2021年08月02日                         第一作者：Viacheslav Osaulenko                        [链接](https://arxiv.org/abs/2108.00706).                     
## 摘要：本文针对将信息编码为二进制单元程序集的问题。脉冲神经网络和k-winners-take-all模型是两种常见的方法，但第一种方法难以用于信息处理，第二种方法过于简单，缺乏第一种方法的重要特征。我们提出了一个中间模型，该模型与kWTA的计算简便性相同，并且具有更灵活和更丰富的动力学特性。它使用显式抑制神经元通过迭代程序平衡和塑造兴奋。这导致抑制性和兴奋性神经元之间的反复交互作用，更好地适应输入分布，并执行习惯化、去相关和聚类等计算。为了证明这些，我们研究了类Hebbian学习规则，并提出了一种新的具有多种稳定机制的二元权重学习规则。我们的源代码是公开的。
<details>	<summary>英文摘要</summary>	This paper targets the problem of encoding information into binary cell assemblies. Spiking neural networks and k-winners-take-all models are two common approaches, but the first is hard to use for information processing and the second is too simple and lacks important features of the first. We present an intermediate model that shares the computational ease of kWTA and has more flexible and richer dynamics. It uses explicit inhibitory neurons to balance and shape excitation through an iterative procedure. This leads to a recurrent interaction between inhibitory and excitatory neurons that better adapts to the input distribution and performs such computations as habituation, decorrelation, and clustering. To show these, we investigate Hebbian-like learning rules and propose a new learning rule for binary weights with multiple stabilization mechanisms. Our source code is publicly available. </details>
<details>	<summary>注释</summary>	14 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 188、通过乘法突触的脉冲神经网络的非线性计算
- [ ] Nonlinear computations in spiking neural networks through multiplicative synapses 
时间：2021年08月02日                         第一作者：Michele Nardin                       [链接](https://arxiv.org/abs/2009.03857).                     
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 187、基于片上学习的深脉冲神经网络连接剪枝
- [ ] Connection Pruning for Deep Spiking Neural Networks with On-Chip Learning 
时间：2021年07月31日                         第一作者：Thao N.N. Nguyen                       [链接](https://arxiv.org/abs/2010.04351).                     
<details>	<summary>注释</summary>	8 pages, 9 figures This paper has been accepted for publication in the International Conference on Neuromorphic Systems (ICONS) 2021 Journal-ref: International Conference on Neuromorphic Systems 2021 DOI: 10.1145/3477145.3477157 </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 186、脉冲神经网络无监督模式识别的权值发散易化原理
- [ ] The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks 
时间：2021年07月31日                         第一作者：Oleg Nikitin                       [链接](https://arxiv.org/abs/2104.09943).                     
<details>	<summary>注释</summary>	9 pages, 5 figures, submitted to the conference ICANN 2021 MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 185、HYPER-SNN：面向高光谱图像分类的节能量化深脉冲神经网络
- [ ] HYPER-SNN: Towards Energy-efficient Quantized Deep Spiking Neural Networks for Hyperspectral Image Classification 
时间：2021年07月28日                         第一作者：Gourav Datta                       [链接](https://arxiv.org/abs/2107.11979).                     
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 184、节能随机游动计算的神经形态标度优势
- [ ] Neuromorphic scaling advantages for energy-efficient random walk computation 
时间：2021年07月27日                         第一作者：J. Darby Smith                       [链接](https://arxiv.org/abs/2107.13057).                     
## 摘要：受大脑难以置信的效率和能力的启发，神经形态计算（NMC）方法将从根本上改进计算。大多数NMC研究的目标是在人造硬件中复制大脑的计算结构和架构，其重点是人工智能；然而，很少有人探究这种大脑启发的硬件是否能提供超越认知任务的价值。我们证明了脉冲神经形态结构的高度并行性和可配置性使得它们非常适合通过离散时间马尔可夫链实现随机游动。这种随机游动在蒙特卡罗方法中非常有用，蒙特卡罗方法是解决广泛数值计算任务的基本计算工具。此外，我们还展示了涉及一类随机微分方程的概率解的数学基础如何利用这些模拟为一系列广泛适用的计算任务提供解决方案。尽管处于早期开发阶段，我们发现NMC平台在足够大的规模下可以大幅降低高性能计算（HPC）平台的能源需求。
<details>	<summary>英文摘要</summary>	Computing stands to be radically improved by neuromorphic computing (NMC) approaches inspired by the brain's incredible efficiency and capabilities. Most NMC research, which aims to replicate the brain's computational structure and architecture in man-made hardware, has focused on artificial intelligence; however, less explored is whether this brain-inspired hardware can provide value beyond cognitive tasks. We demonstrate that high-degree parallelism and configurability of spiking neuromorphic architectures makes them well-suited to implement random walks via discrete time Markov chains. Such random walks are useful in Monte Carlo methods, which represent a fundamental computational tool for solving a wide range of numerical computing tasks. Additionally, we show how the mathematical basis for a probabilistic solution involving a class of stochastic differential equations can leverage those simulations to provide solutions for a range of broadly applicable computational tasks. Despite being in an early development stage, we find that NMC platforms, at a sufficient scale, can drastically reduce the energy demands of high-performance computing (HPC) platforms. </details>
<details>	<summary>注释</summary>	Paper, figures, supplement Report-no: SAND2021-9085 O </details>
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 183、用单脉冲混合输入编码训练能量有效的深脉冲神经网络
- [ ] Training Energy-Efficient Deep Spiking Neural Networks with Single-Spike Hybrid Input Encoding 
时间：2021年07月26日                         第一作者：Gourav Datta                       [链接](https://arxiv.org/abs/2107.12374).                     
## 摘要：脉冲神经网络（SNN）已经成为传统深度学习框架的一个有吸引力的替代方案，因为它们在事件驱动的神经形态硬件中提供了更高的计算效率。然而，由于输入编码和训练技术的效率低下，最先进的（SOTA）SNN存在较高的推理延迟。最广泛使用的输入编码方案，例如基于泊松的速率编码，没有利用snn的时间学习能力。本文提出了一种低延迟节能SNN的训练框架，该框架在输入层使用混合编码方案，在第一个时间步中直接应用图像的模拟像素值，在随后的时间步中使用一种新的脉冲时间编码变体。特别是，每个隐藏层中的神经元被限制在每张图像中最多发射一次，这增加了激活稀疏性。为了训练这些混合编码的SNN，我们提出了一种基于梯度下降的脉冲时间相关反向传播（STDB）机制，该机制使用了一种新的基于输出神经元脉冲时间和膜电位的交叉熵损失函数。由此产生的SNN降低了延迟和高激活稀疏性，显著提高了计算效率。特别是，我们评估了在几种VGG体系结构上针对来自CIFAR-10和CIFAR-100数据集的图像分类任务提出的培训方案。我们在CIFAR-100数据集上以$5$timesteps获得了$66.46$\%的顶级精度，计算能量比同等标准ANN少${\sim}125 \倍。此外，与其他最先进的速率或时间编码的SNN模型相比，我们提出的SNN推理速度快5$-$300\倍。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have emerged as an attractive alternative to traditional deep learning frameworks, since they provide higher computational efficiency in event driven neuromorphic hardware. However, the state-of-the-art (SOTA) SNNs suffer from high inference latency, resulting from inefficient input encoding and training techniques. The most widely used input coding schemes, such as Poisson based rate-coding, do not leverage the temporal learning capabilities of SNNs. This paper presents a training framework for low-latency energy-efficient SNNs that uses a hybrid encoding scheme at the input layer in which the analog pixel values of an image are directly applied during the first timestep and a novel variant of spike temporal coding is used during subsequent timesteps. In particular, neurons in every hidden layer are restricted to fire at most once per image which increases activation sparsity. To train these hybrid-encoded SNNs, we propose a variant of the gradient descent based spike timing dependent back propagation (STDB) mechanism using a novel cross entropy loss function based on both the output neurons' spike time and membrane potential. The resulting SNNs have reduced latency and high activation sparsity, yielding significant improvements in computational efficiency. In particular, we evaluate our proposed training scheme on image classification tasks from CIFAR-10 and CIFAR-100 datasets on several VGG architectures. We achieve top-1 accuracy of $66.46$\% with $5$ timesteps on the CIFAR-100 dataset with ${\sim}125\times$ less compute energy than an equivalent standard ANN. Additionally, our proposed SNN performs $5$-$300\times$ faster inference compared to other state-of-the-art rate or temporally coded SNN models. </details>
<details>	<summary>注释</summary>	arXiv admin note: text overlap with arXiv:2107.11979 </details>
<details>	<summary>邮件日期</summary>	2021年07月28日</details>

# 182、面向高光谱图像分类的能量高效量化深棘波神经网络
- [ ] Towards Energy-efficient Quantized Deep SpikingNeural Networks for Hyperspectral Image Classification 
时间：2021年07月26日                         第一作者：Gourav Datta                       [链接](https://arxiv.org/abs/2107.11979).                     
## 摘要：超光谱图像（HSI）在一系列连续的光谱带中提供丰富的光谱和空间信息。然而，精确处理波段之间的光谱和空间相关性需要使用能量昂贵的三维卷积神经网络（CNN）。为了应对这一挑战，我们建议使用由iso体系结构CNN生成并通过量化感知梯度下降进行训练的脉冲神经网络（SNN），以优化其权重、膜泄漏和触发阈值。在训练和推断期间，HSI的模拟像素值直接应用于SNN的输入层，而无需转换为脉冲序列。我们的训练技术降低了延迟，并结合了高激活稀疏性，从而显著提高了计算效率。我们使用3-D和3-D/2-D混合卷积结构上的三个HSI数据集来评估我们的方案。我们在Indian Pines数据集上通过5个时间步（推断延迟）和6位权重量化分别实现了98.68%、98.34%和98.20%的总体准确度、平均准确度和kappa系数。特别是，我们的模型实现了与最新技术（SOTA）相似的精度，在三个HSI数据集上的计算能量平均分别比iso体系结构全精度和6位量化CNN少560.6倍和44.8倍。
<details>	<summary>英文摘要</summary>	Hyper spectral images (HSI) provide rich spectral and spatial information across a series of contiguous spectral bands. However, the accurate processing of the spectral and spatial correlation between the bands requires the use of energy-expensive 3-D Convolutional Neural Networks (CNNs). To address this challenge, we propose the use of Spiking Neural Networks (SNNs) that are generated from iso-architecture CNNs and trained with quantization-aware gradient descent to optimize their weights, membrane leak, and firing thresholds. During both training and inference, the analog pixel values of a HSI are directly applied to the input layer of the SNN without the need to convert to a spike-train. The reduced latency of our training technique combined with high activation sparsity yields significant improvements in computational efficiency. We evaluate our proposal using three HSI datasets on a 3-D and a 3-D / 2-D hybrid convolutional architecture. We achieve overall accuracy, average accuracy, and kappa coefficient of 98.68%, 98.34%, and 98.20% respectively with 5 time steps (inference latency) and 6-bit weight quantization on the Indian Pines dataset. In particular, our models achieved accuracies similar to state-of-the-art (SOTA) with 560.6 and 44.8 times less compute energy on average over three HSI datasets than an iso-architecture full-precision and 6-bit quantized CNN, respectively. </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 181、用于事件流分类的时间型注意脉冲神经网络
- [ ] Temporal-wise Attention Spiking Neural Networks for Event Streams Classification 
时间：2021年07月25日                         第一作者：Man Yao                       [链接](https://arxiv.org/abs/2107.11711).                     
## 摘要：如何有效地处理事件稀疏、不均匀、时间分辨率为微秒的时空事件流具有重要的应用价值。脉冲神经网络（SNN）作为一种基于大脑的事件触发计算模型，具有从事件流中提取有效时空特征的潜力。然而，当将单个事件聚合为具有新的更高时间分辨率的帧时，现有的SNN模型并不重视序列帧具有不同的信噪比，因为事件流是稀疏和非均匀的。这种情况会干扰现有SNN的性能。在这项工作中，我们提出了一个基于时间的注意SNN（TA-SNN）模型来学习基于帧的表示方法来处理事件流。具体来说，我们将注意概念扩展到时间输入，在训练阶段判断框架对最终决策的重要性，在推理阶段丢弃不相关的框架。我们证明了TA-SNN模型提高了事件流分类任务的准确性。我们还研究了多尺度时间分辨率对基于帧表示的影响。我们的方法在三个不同的分类任务上进行了测试：手势识别、图像分类和语音数字识别。我们报告了关于这些任务的最新结果，并在仅60毫秒的时间内就基本提高了手势识别的准确率（几乎19%）。
<details>	<summary>英文摘要</summary>	How to effectively and efficiently deal with spatio-temporal event streams, where the events are generally sparse and non-uniform and have the microsecond temporal resolution, is of great value and has various real-life applications. Spiking neural network (SNN), as one of the brain-inspired event-triggered computing models, has the potential to extract effective spatio-temporal features from the event streams. However, when aggregating individual events into frames with a new higher temporal resolution, existing SNN models do not attach importance to that the serial frames have different signal-to-noise ratios since event streams are sparse and non-uniform. This situation interferes with the performance of existing SNNs. In this work, we propose a temporal-wise attention SNN (TA-SNN) model to learn frame-based representation for processing event streams. Concretely, we extend the attention concept to temporal-wise input to judge the significance of frames for the final decision at the training stage, and discard the irrelevant frames at the inference stage. We demonstrate that TA-SNN models improve the accuracy of event streams classification tasks. We also study the impact of multiple-scale temporal resolutions for frame-based representation. Our approach is tested on three different classification tasks: gesture recognition, image classification, and spoken digit recognition. We report the state-of-the-art results on these tasks, and get the essential improvement of accuracy (almost 19\%) for gesture recognition with only 60 ms. </details>
<details>	<summary>注释</summary>	Accepted by ICCV 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 180、H2Learn：用于高精度脉冲神经网络的高效学习加速器
- [ ] H2Learn: High-Efficiency Learning Accelerator for High-Accuracy Spiking Neural Networks 
时间：2021年07月25日                         第一作者：Ling Liang                       [链接](https://arxiv.org/abs/2107.11746).                     
## 摘要：虽然脉冲神经网络（spiking neural networks，SNNs）得益于生物似然神经模型，但在常见的局部突触可塑性学习规则下，其学习精度较低，限制了其在许多实际任务中的应用。最近，人工神经网络（ANN）领域中一种新兴的基于时间反向传播（BPTT）的SNN监督学习算法成功地提高了SNN的准确性，并有助于提高SNN的实用性。然而，当前的通用处理器在对SNN执行BPTT时，由于ANN定制的优化，效率较低。另一方面，目前的神经形态芯片不能支持BPTT，因为它们主要采用局部突触可塑性规则来简化实现。在这项工作中，我们提出了H2Learn，一种新的体系结构，可以实现基于BPTT的SNN学习的高效率，从而确保SNN的高精度。首先，我们描述了基于BPTT的SNN学习行为。得益于前向传递中基于二进制脉冲的计算和权重更新，我们首先在前向引擎和权重更新引擎中设计基于查找表（LUT）的处理元素，使累加隐式化，并融合多个输入点的计算。其次，得益于后向过程中丰富的稀疏性，我们设计了一个双稀疏感知后向引擎，该引擎利用了输入和输出的稀疏性。最后，我们应用不同引擎之间的管道优化来构建基于BPTT的SNN学习的端到端解决方案。与现代NVIDIA V100 GPU相比，H2Learn在多个基准数据集上实现了7.38倍的面积节约、5.74-10.20倍的加速比和5.25-7.12倍的节能。
<details>	<summary>英文摘要</summary>	Although spiking neural networks (SNNs) take benefits from the bio-plausible neural modeling, the low accuracy under the common local synaptic plasticity learning rules limits their application in many practical tasks. Recently, an emerging SNN supervised learning algorithm inspired by backpropagation through time (BPTT) from the domain of artificial neural networks (ANNs) has successfully boosted the accuracy of SNNs and helped improve the practicability of SNNs. However, current general-purpose processors suffer from low efficiency when performing BPTT for SNNs due to the ANN-tailored optimization. On the other hand, current neuromorphic chips cannot support BPTT because they mainly adopt local synaptic plasticity rules for simplified implementation. In this work, we propose H2Learn, a novel architecture that can achieve high efficiency for BPTT-based SNN learning which ensures high accuracy of SNNs. At the beginning, we characterized the behaviors of BPTT-based SNN learning. Benefited from the binary spike-based computation in the forward pass and the weight update, we first design lookup table (LUT) based processing elements in Forward Engine and Weight Update Engine to make accumulations implicit and to fuse the computations of multiple input points. Second, benefited from the rich sparsity in the backward pass, we design a dual-sparsity-aware Backward Engine which exploits both input and output sparsity. Finally, we apply a pipeline optimization between different engines to build an end-to-end solution for the BPTT-based SNN learning. Compared with the modern NVIDIA V100 GPU, H2Learn achieves 7.38x area saving, 5.74-10.20x speedup, and 5.25-7.12x energy saving on several benchmark datasets. </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 179、一种用于节能目标检测的全脉冲混合神经网络
- [ ] A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection 
时间：2021年07月24日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2104.10719).                     
<details>	<summary>注释</summary>	10 pages, Submitted Manuscript </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 178、通过注意引导压缩实现低延迟节能的深度SNN
- [ ] Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided Compression 
时间：2021年07月16日                         第一作者：Souvik Kundu                       [链接](https://arxiv.org/abs/2107.12445).                     
## 摘要：深脉冲神经网络（SNN）已成为传统深度学习框架的潜在替代方案，因为它们有望在事件驱动的神经形态硬件上提供更高的计算效率。然而，为了在复杂的视觉应用中表现良好，大多数SNN训练框架都会产生较大的推理延迟，这会导致脉冲活动增加和能量效率降低。因此，最小化平均脉冲活动同时保持EP SNN的准确性仍然是一个重大的挑战和机遇。本文提出了一种非迭代SNN训练技术，该技术在保持高推理精度的同时实现超高压缩，同时降低脉冲活动。特别是，我们的框架首先使用未压缩元模型的注意图来生成压缩的ANN。这一步可以调整为支持不规则和结构化通道修剪，以利用各种平台上的计算优势。然后，该框架使用直接输入执行基于稀疏学习的监督SNN训练。在训练过程中，它联合优化SNN权重、阈值和泄漏参数，以在保持压缩的同时大幅减少所需的时间步数。为了评估我们的方法的优点，我们在CIFAR-10和CIFAR-100上对VGG和ResNet的变体进行了实验，并在Tiny-ImageNet上对VGG16进行了实验。通过建议的技术生成的SNN模型产生的SOTA压缩比高达33.4x，与基线未运行的对应模型相比，精度没有显著下降。与现有的SNN修剪方法相比，我们实现了高达8.3倍的压缩，并提高了精度。
<details>	<summary>英文摘要</summary>	Deep spiking neural networks (SNNs) have emerged as a potential alternative to traditional deep learning frameworks, due to their promise to provide increased compute efficiency on event-driven neuromorphic hardware. However, to perform well on complex vision applications, most SNN training frameworks yield large inference latency which translates to increased spike activity and reduced energy efficiency. Hence,minimizing average spike activity while preserving accuracy indeep SNNs remains a significant challenge and opportunity.This paper presents a non-iterative SNN training technique thatachieves ultra-high compression with reduced spiking activitywhile maintaining high inference accuracy. In particular, our framework first uses the attention-maps of an un compressed meta-model to yield compressed ANNs. This step can be tuned to support both irregular and structured channel pruning to leverage computational benefits over a broad range of platforms. The framework then performs sparse-learning-based supervised SNN training using direct inputs. During the training, it jointly optimizes the SNN weight, threshold, and leak parameters to drastically minimize the number of time steps required while retaining compression. To evaluate the merits of our approach, we performed experiments with variants of VGG and ResNet, on both CIFAR-10 and CIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the proposed technique yield SOTA compression ratios of up to 33.4x with no significant drops in accuracy compared to baseline unpruned counterparts. Compared to existing SNN pruning methods, we achieve up to 8.3x higher compression with improved accuracy. </details>
<details>	<summary>注释</summary>	10 Pages, 8 Figures, 5 Tables </details>
<details>	<summary>邮件日期</summary>	2021年07月28日</details>

# 177、用于神经形态信息处理的共振隧道二极管纳米光电脉冲节点
- [ ] Resonant tunnelling diode nano-optoelectronic spiking nodes for neuromorphic information processing 
时间：2021年07月21日                         第一作者：Mat\v{e}j Hejda                       [链接](https://arxiv.org/abs/2107.06721).                     
<details>	<summary>注释</summary>	Changed manuscript title to lower-case </details>
<details>	<summary>邮件日期</summary>	2021年07月22日</details>

# 176、深度神经网络中时间稀疏性的训练，在视频处理中的应用
- [ ] Training for temporal sparsity in deep neural networks, application in video processing 
时间：2021年07月15日                         第一作者：Amirreza Yousefzadeh                       [链接](https://arxiv.org/abs/2107.07305).                     
## 摘要：激活稀疏性提高了稀疏感知神经网络加速器的计算效率和资源利用率。由于DNNs中的主要操作是使用权重计算内积的激活的乘法累加（MAC），跳过两个操作数中至少有一个为零的操作可以使推断在延迟和功率方面更有效。激活的空间稀疏化是DNN文献中的一个热门话题，已经建立了几种方法来偏向DNN。另一方面，时间稀疏性是仿生脉冲神经网络（SNNs）的固有特性，神经形态处理利用SNNs提高硬件效率。引入和利用时空稀疏性，是DNN文献中很少探讨的话题，但与DNN的发展趋势完全一致，从静态信号处理转向更多的流信号处理。为了实现这一目标，本文引入了一种新的DNN层（称为Delta激活层），其唯一目的是提高训练过程中激活的时间稀疏性。Delta激活层将时间稀疏性转化为空间稀疏性，以便在硬件中执行稀疏张量乘法时加以利用。通过在训练过程中使用delta推理和“通常的”空间稀疏启发式，所得到的模型不仅学习利用空间而且还学习利用时间激活稀疏性（对于给定的输入数据分布）。人们可以在普通训练或细化阶段使用δ激活层。我们实现了Delta激活层作为标准Tensoflow Keras库的扩展，并将其应用于人体动作识别（UCF101）数据集的深层神经网络训练。我们报告了激活稀疏度几乎提高了3倍，在长时间的训练后，模型精度的可恢复性损失。
<details>	<summary>英文摘要</summary>	Activation sparsity improves compute efficiency and resource utilization in sparsity-aware neural network accelerators. As the predominant operation in DNNs is multiply-accumulate (MAC) of activations with weights to compute inner products, skipping operations where (at least) one of the two operands is zero can make inference more efficient in terms of latency and power. Spatial sparsification of activations is a popular topic in DNN literature and several methods have already been established to bias a DNN for it. On the other hand, temporal sparsity is an inherent feature of bio-inspired spiking neural networks (SNNs), which neuromorphic processing exploits for hardware efficiency. Introducing and exploiting spatio-temporal sparsity, is a topic much less explored in DNN literature, but in perfect resonance with the trend in DNN, to shift from static signal processing to more streaming signal processing. Towards this goal, in this paper we introduce a new DNN layer (called Delta Activation Layer), whose sole purpose is to promote temporal sparsity of activations during training. A Delta Activation Layer casts temporal sparsity into spatial activation sparsity to be exploited when performing sparse tensor multiplications in hardware. By employing delta inference and ``the usual'' spatial sparsification heuristics during training, the resulting model learns to exploit not only spatial but also temporal activation sparsity (for a given input data distribution). One may use the Delta Activation Layer either during vanilla training or during a refinement phase. We have implemented Delta Activation Layer as an extension of the standard Tensoflow-Keras library, and applied it to train deep neural networks on the Human Action Recognition (UCF101) dataset. We report an almost 3x improvement of activation sparsity, with recoverable loss of model accuracy after longer training. </details>
<details>	<summary>邮件日期</summary>	2021年07月16日</details>

# 175、用于神经形态信息处理的共振隧道二极管纳米光电脉冲节点
- [ ] Resonant Tunnelling Diode Nano-Optoelectronic Spiking Nodes For Neuromorphic Information Processing 
时间：2021年07月14日                         第一作者：Mat\v{e}j Hejda                       [链接](https://arxiv.org/abs/2107.06721).                     
## 摘要：在这项工作中，我们介绍了一种光电脉冲人工神经元，它能够以超快的速率（$\大约$100 ps/光脉冲）和低能耗（$<$pJ/脉冲）工作。所提出的系统结合了一个表现出负微分电导的可激发共振隧道二极管（RTD）元件，该元件耦合到纳米级光源（形成主节点）或光电探测器（形成接收节点）。我们数值研究了一个互连的主-接收RTD节点系统的脉冲动态响应和信息传播功能。利用脉冲阈值化和积分的关键功能，我们利用单个节点对序列脉冲模式进行分类，并对图像特征（边缘）进行卷积识别。我们还展示了一个光学互连的脉冲神经网络模型，用于处理超过10gbps的时空数据，具有很高的推理精度。最后，我们展示了一种利用脉冲时间相关可塑性的片外监督学习方法，用于支持RTD的光子脉冲神经网络。这些结果证明了RTD脉冲节点在低占地面积、低能耗、高速光电实现神经形态硬件方面的潜力和可行性。
<details>	<summary>英文摘要</summary>	In this work, we introduce an optoelectronic spiking artificial neuron capable of operating at ultrafast rates ($\approx$ 100 ps/optical spike) and with low energy consumption ($<$ pJ/spike). The proposed system combines an excitable resonant tunnelling diode (RTD) element exhibiting negative differential conductance, coupled to a nanoscale light source (forming a master node) or a photodetector (forming a receiver node). We study numerically the spiking dynamical responses and information propagation functionality of an interconnected master-receiver RTD node system. Using the key functionality of pulse thresholding and integration, we utilize a single node to classify sequential pulse patterns and perform convolutional functionality for image feature (edge) recognition. We also demonstrate an optically-interconnected spiking neural network model for processing of spatiotemporal data at over 10 Gbps with high inference accuracy. Finally, we demonstrate an off-chip supervised learning approach utilizing spike-timing dependent plasticity for the RTD-enabled photonic spiking neural network. These results demonstrate the potential and viability of RTD spiking nodes for low footprint, low energy, high-speed optoelectronic realization of neuromorphic hardware. </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 174、利用图形学习中时空特征归一化的脉冲动态特性
- [ ] Exploiting Spiking Dynamics with Spatial-temporal Feature Normalization in Graph Learning 
时间：2021年06月30日                         第一作者：Mingkun Xu                       [链接](https://arxiv.org/abs/2107.06865).                     
## 摘要：具有内在动力学的生物脉冲神经元是大脑在复杂环境中处理多模态信息的强大表征和学习能力的基础。尽管最近在处理欧几里德空间任务的脉冲神经网络（SNNs）方面取得了巨大的进展，但利用SNNs处理以图形数据为代表的非欧几里德空间数据仍然是一个挑战，这主要是由于缺乏有效的建模框架和有用的训练技术。在这里，我们提出了一个通用的基于脉冲的建模框架，可以直接训练snn进行图形学习。通过对节点特征的脉冲数据流进行时空展开，将图卷积滤波器引入到脉冲动力学中，形成了一种协同学习范式。针对SNN特有的脉冲表示和脉冲动态特性，提出了一种适用于SNN的时空特征归一化（STFN）技术，以加速SNN的收敛。我们将我们的方法实例化为两个脉冲图模型，包括图卷积SNNs和图注意SNNs，并在Cora、citeser和Pubmed三个节点分类基准上验证了它们的性能。我们的模型可以以较低的计算成本达到与最先进的图形神经网络（GNN）模型相当的性能，显示了在神经形态硬件上执行的巨大优势，并促进了神经形态在图形场景中的应用。
<details>	<summary>英文摘要</summary>	Biological spiking neurons with intrinsic dynamics underlie the powerful representation and learning capabilities of the brain for processing multimodal information in complex environments. Despite recent tremendous progress in spiking neural networks (SNNs) for handling Euclidean-space tasks, it still remains challenging to exploit SNNs in processing non-Euclidean-space data represented by graph data, mainly due to the lack of effective modeling framework and useful training techniques. Here we present a general spike-based modeling framework that enables the direct training of SNNs for graph learning. Through spatial-temporal unfolding for spiking data flows of node features, we incorporate graph convolution filters into spiking dynamics and formalize a synergistic learning paradigm. Considering the unique features of spike representation and spiking dynamics, we propose a spatial-temporal feature normalization (STFN) technique suitable for SNN to accelerate convergence. We instantiate our methods into two spiking graph models, including graph convolution SNNs and graph attention SNNs, and validate their performance on three node-classification benchmarks, including Cora, Citeseer, and Pubmed. Our model can achieve comparable performance with the state-of-the-art graph neural network (GNN) models with much lower computation costs, demonstrating great benefits for the execution on neuromorphic hardware and prompting neuromorphic applications in graphical scenarios. </details>
<details>	<summary>注释</summary>	Accepted by IJCAI-21 </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 173、反向传播邻域聚合用于脉冲神经网络的精确训练
- [ ] Backpropagated Neighborhood Aggregation for Accurate Training of Spiking Neural Networks 
时间：2021年06月22日                         第一作者：Yukun Yang                       [链接](https://arxiv.org/abs/2107.06861).                     
## 摘要：虽然反向传播（BP）已被应用于脉冲神经网络（SNNs）取得了令人鼓舞的结果，但一个关键的挑战是将连续值损失反向传播到具有不连续全部或无放电活动的脉冲神经元层上。现有的方法通过引入具有自身局限性的折衷方案来解决这一难题，从而导致潜在的性能下降。我们提出了一种新的类似BP的方法，称为邻域聚合（NA），它计算精确的误差梯度来指导权重更新，这可能导致不连续的修改射击活动。NA利用一个新的膜电位距离函数，通过在每个神经元的当前膜电位附近聚集多个扰动膜电位波形损失的有限差分来实现这一目标。实验表明，该算法在多个数据集的SNN训练中具有良好的性能。
<details>	<summary>英文摘要</summary>	While backpropagation (BP) has been applied to spiking neural networks (SNNs) achieving encouraging results, a key challenge involved is to backpropagate a continuous-valued loss over layers of spiking neurons exhibiting discontinuous all-or-none firing activities. Existing methods deal with this difficulty by introducing compromises that come with their own limitations, leading to potential performance degradation. We propose a novel BP-like method, called neighborhood aggregation (NA), which computes accurate error gradients guiding weight updates that may lead to discontinuous modifications of firing activities. NA achieves this goal by aggregating finite differences of the loss over multiple perturbed membrane potential waveforms in the neighborhood of the present membrane potential of each neuron while utilizing a new membrane potential distance function. Our experiments show that the proposed NA algorithm delivers the state-of-the-art performance for SNN training on several datasets. </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 172、活性树突树可以减轻超导神经元扇入的限制
- [ ] An active dendritic tree can mitigate fan-in limitations in superconducting neurons 
时间：2021年07月12日                         第一作者：Bryce A. Primavera                        [链接](https://arxiv.org/abs/2107.05777).                     
## 摘要：超导电子电路在神经形态硬件方面有很多可提供的。超导量子干涉器件（SQUIDs）可以作为神经元胞体阈值操作的有源元件。然而，SQUID在应用信号中具有周期性的响应函数。我们从理论上证明，如果一个人限制对鱿鱼的总输入以维持一个单调递增的反应，那么一大部分突触必须是活跃的，以驱动神经元达到阈值。然后我们证明了一个活跃的树突树（也基于SQUIDs）可以显著减少突触的比例，这些突触必须活跃才能驱动神经元达到阈值。在这种情况下，树突树的加入提供了增强每个神经元的计算能力和允许神经元以稀疏的输入活动脉冲的双重好处。
<details>	<summary>英文摘要</summary>	Superconducting electronic circuits have much to offer with regard to neuromorphic hardware. Superconducting quantum interference devices (SQUIDs) can serve as an active element to perform the thresholding operation of a neuron's soma. However, a SQUID has a response function that is periodic in the applied signal. We show theoretically that if one restricts the total input to a SQUID to maintain a monotonically increasing response, a large fraction of synapses must be active to drive a neuron to threshold. We then demonstrate that an active dendritic tree (also based on SQUIDs) can significantly reduce the fraction of synapses that must be active to drive the neuron to threshold. In this context, the inclusion of a dendritic tree provides the dual benefits of enhancing the computational abilities of each neuron and allowing the neuron to spike with sparse input activity. </details>
<details>	<summary>注释</summary>	8 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年07月14日</details>

# 171、更快速的SNN模拟，具有惰性+事件驱动的可塑性和共享原子
- [ ] Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared Atomics 
时间：2021年07月08日                         第一作者：Dennis Bautembach                       [链接](https://arxiv.org/abs/2107.04092).                     
## 摘要：我们提出了两种新的优化方法来加速基于时钟的脉冲神经网络（SNN）模拟器。第一个目标是脉冲时间依赖性可塑性（STDP）。它结合了懒惰和事件驱动的可塑性，并有效地促进了使用位场和整数内部函数计算突触前和突触后峰值。它提供了更高的带宽比事件驱动塑性单独实现了1.5倍-2倍的加速比我们最接近的竞争对手。第二个优化目标是脉冲交货。我们以一种限制在任何给定时间需要更新的神经元数量的方式来划分我们的图表示，这允许我们在共享内存而不是全局内存中执行所述更新。这比我们最接近的竞争对手快2-2.5倍。这两种优化都代表了STDP和Spice（我们最先进的SNN模拟器）内部峰值交付多年迭代的最终进化阶段。所提出的优化不仅限于我们的图形表示或管道，而且适用于多种模拟器设计。我们在三个成熟的模型上评估我们的性能，并将我们自己与其他三个最先进的模拟器进行比较。
<details>	<summary>英文摘要</summary>	We present two novel optimizations that accelerate clock-based spiking neural network (SNN) simulators. The first one targets spike timing dependent plasticity (STDP). It combines lazy- with event-driven plasticity and efficiently facilitates the computation of pre- and post-synaptic spikes using bitfields and integer intrinsics. It offers higher bandwidth than event-driven plasticity alone and achieves a 1.5x-2x speedup over our closest competitor. The second optimization targets spike delivery. We partition our graph representation in a way that bounds the number of neurons that need be updated at any given time which allows us to perform said update in shared memory instead of global memory. This is 2x-2.5x faster than our closest competitor. Both optimizations represent the final evolutionary stages of years of iteration on STDP and spike delivery inside "Spice" (/spaIk/), our state of the art SNN simulator. The proposed optimizations are not exclusive to our graph representation or pipeline but are applicable to a multitude of simulator designs. We evaluate our performance on three well-established models and compare ourselves against three other state of the art simulators. </details>
<details>	<summary>注释</summary>	Submitted to IEEE-HPEC 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月12日</details>

# 170、一种用于人工智能的长-短期记忆算法在脉冲神经元硬件中的应用
- [ ] A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware 
时间：2021年07月08日                         第一作者：Philipp Plank                       [链接](https://arxiv.org/abs/2107.03992).                     
## 摘要：尽管付出了大量的努力，但目前采用深度神经网络（DNNs）的人工智能（AI）方法在多大程度上可以在基于spike的神经形态硬件上更有效地实现仍然是一个悬而未决的问题。这尤其适用于解决序列处理任务的人工智能方法，序列处理任务是基于峰值的神经形态硬件的主要应用目标。一个困难是，用于此类任务的dnn通常采用长-短期记忆（LSTM）单元。然而，在基于峰值的硬件中对这些单元的有效仿真却一直缺失。我们提出了一个生物启发的解决方案，解决了这个问题。该解决方案使我们能够实现一类主要的dnn，用于序列处理任务，如时间序列分类和问答，并在神经形态硬件上节省大量的能量。事实上，我们用来回答问题的用于推理对象之间关系的关系网络是大型DNN的第一个例子，该DNN在神经形态硬件上执行序列处理任务，具有显著的节能效果。
<details>	<summary>英文摘要</summary>	In spite of intensive efforts it has remained an open problem to what extent current Artificial Intelligence (AI) methods that employ Deep Neural Networks (DNNs) can be implemented more energy-efficiently on spike-based neuromorphic hardware. This holds in particular for AI methods that solve sequence processing tasks, a primary application target for spike-based neuromorphic hardware. One difficulty is that DNNs for such tasks typically employ Long Short-Term Memory (LSTM) units. Yet an efficient emulation of these units in spike-based hardware has been missing. We present a biologically inspired solution that solves this problem. This solution enables us to implement a major class of DNNs for sequence processing tasks such as time series classification and question answering with substantial energy savings on neuromorphic hardware. In fact, the Relational Network for reasoning about relations between objects that we use for question answering is the first example of a large DNN that carries out a sequence processing task with substantial energy-saving on neuromorphic hardware. </details>
<details>	<summary>注释</summary>	Philipp Plank and Arjun Rao have contributed equally to this work as first authors </details>
<details>	<summary>邮件日期</summary>	2021年07月09日</details>

# 169、Q-SpiNN：一种量化脉冲神经网络的框架
- [ ] Q-SpiNN: A Framework for Quantizing Spiking Neural Networks 
时间：2021年07月05日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2107.01807).                     
## 摘要：量化是在不显著降低准确度的情况下减少脉冲神经网络（SNNs）内存占用的一项重要技术。然而，最新的技术仅集中于从特定量化方案直接采用权重量化，即训练后量化（PTQ）或训练量化（ITQ），并且不考虑（1）量化其他SNN参数（例如，神经元膜电位），（2）探索量化方法的不同组合（即量化方案、精度水平和舍入方案），以及（3）在最后选择具有良好记忆精度权衡的SNN模型。因此，由这些最新技术提供的用于满足目标精度的存储器节省是有限的，从而妨碍在资源受限的系统（例如，物联网边缘设备）上处理snn。针对这一点，我们提出了Q-SpiNN，一个新的量化框架的内存效率SNNs。Q-SpiNN的关键机制是：（1）根据不同SNN参数对精度的重要性对其进行量化；（2）探索量化方案、精度水平和舍入方案的不同组合，以找到有效的SNN模型候选，（3）开发一种算法，该算法量化了候选人获得的记忆精度折衷的好处，并选择帕累托最优的。实验结果表明，对于无监督网络，Q-SpiNN减少了约4倍的内存占用，同时在MNIST数据集上保持了1%的准确率。对于有监督的网络，Q-SpiNN减少了大约2倍的内存，同时将精度保持在DVS手势数据集基线的2%以内。
<details>	<summary>英文摘要</summary>	A prominent technique for reducing the memory footprint of Spiking Neural Networks (SNNs) without decreasing the accuracy significantly is quantization. However, the state-of-the-art only focus on employing the weight quantization directly from a specific quantization scheme, i.e., either the post-training quantization (PTQ) or the in-training quantization (ITQ), and do not consider (1) quantizing other SNN parameters (e.g., neuron membrane potential), (2) exploring different combinations of quantization approaches (i.e., quantization schemes, precision levels, and rounding schemes), and (3) selecting the SNN model with a good memory-accuracy trade-off at the end. Therefore, the memory saving offered by these state-of-the-art to meet the targeted accuracy is limited, thereby hindering processing SNNs on the resource-constrained systems (e.g., the IoT-Edge devices). Towards this, we propose Q-SpiNN, a novel quantization framework for memory-efficient SNNs. The key mechanisms of the Q-SpiNN are: (1) employing quantization for different SNN parameters based on their significance to the accuracy, (2) exploring different combinations of quantization schemes, precision levels, and rounding schemes to find efficient SNN model candidates, and (3) developing an algorithm that quantifies the benefit of the memory-accuracy trade-off obtained by the candidates, and selects the Pareto-optimal one. The experimental results show that, for the unsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while maintaining the accuracy within 1% from the baseline on the MNIST dataset. For the supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping the accuracy within 2% from the baseline on the DVS-Gesture dataset. </details>
<details>	<summary>注释</summary>	Accepted for publication at the 2021 International Joint Conference on Neural Networks (IJCNN), July 2021, Virtual Event </details>
<details>	<summary>邮件日期</summary>	2021年07月06日</details>

# 168、DVS攻击：对动态视觉传感器的攻击
- [ ] DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks 
时间：2021年07月01日                         第一作者：Alberto Marchisio                        [链接](https://arxiv.org/abs/2107.00415).                     
## 摘要：脉冲神经网络（SNN）尽管在神经形态硬件上实现时具有能量效率，并且与基于事件的动态视觉传感器（DV）相结合，但是容易受到安全威胁，例如对抗性攻击，即，为诱导误分类而添加到输入中的小扰动。为此，我们提出了DVS攻击，这是一套隐蔽而有效的对抗性攻击方法，旨在干扰构成snn输入的事件序列。首先，我们证明了DVS的噪声滤波器可以作为对抗攻击的防御机制。然后，我们实现了几种攻击，并在DVS摄像机的两种噪声滤波器的情况下进行了测试。实验结果表明，所设计的滤波器只能部分地抵抗我们提出的DVS攻击。使用最佳的噪声滤波器设置，我们提出的基于掩码滤波器的破折号攻击在DVS手势数据集和MNIST数据集上的准确率分别比原始干净帧降低了20%和65%以上。所有建议的DVS攻击和噪声滤波器的源代码发布于https://github.com/albertomarchisio/DVS-Attacks.
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs), despite being energy-efficient when implemented on neuromorphic hardware and coupled with event-based Dynamic Vision Sensors (DVS), are vulnerable to security threats, such as adversarial attacks, i.e., small perturbations added to the input for inducing a misclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet efficient adversarial attack methodologies targeted to perturb the event sequences that compose the input of the SNNs. First, we show that noise filters for DVS can be used as defense mechanisms against adversarial attacks. Afterwards, we implement several attacks and test them in the presence of two types of noise filters for DVS cameras. The experimental results show that the filters can only partially defend the SNNs against our proposed DVS-Attacks. Using the best settings for the noise filters, our proposed Mask Filter-Aware Dash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset and by more than 65% on the MNIST dataset, compared to the original clean frames. The source code of all the proposed DVS-Attacks and noise filters is released at https://github.com/albertomarchisio/DVS-Attacks. </details>
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月02日</details>

# 167、CarSNN：一种基于Loihi神经形态研究处理器的基于事件的自主汽车的高效脉冲神经网络
- [ ] CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor 
时间：2021年07月01日                         第一作者：Alberto Viale                        [链接](https://arxiv.org/abs/2107.00401).                     
## 摘要：与自动驾驶（AD）相关的功能提供了新的移动性形式，这也有利于其他类型的智能和自动系统，如机器人、智能交通和智能工业。对于这些应用程序，需要快速、实时地做出决策。此外，在寻求电动移动性的过程中，这项任务必须遵循低功耗策略，而不影响交通工具或机器人的自主性。这两个挑战可以利用新兴的脉冲神经网络（SNNs）来解决。当部署在专门的神经形态硬件上时，SNNs可以以低延迟和低功耗实现高性能。在本文中，我们使用一个连接到基于事件的摄像机的SNN来解决AD的一个关键问题，即汽车和其他物体之间的分类。为了比传统的基于帧的相机消耗更少的能量，我们使用了动态视觉传感器（DVS）。实验遵循离线监督学习规则进行，然后将学习到的SNN模型映射到Intel-Loihi神经形态研究芯片上。我们的最佳实验实现了86%的离线实现准确率，当它被移植到Loihi芯片上时，准确率下降到83%。神经形态硬件实现每个样本的最大延迟为0.72ms，仅消耗310mw。据我们所知，这项工作是第一个在神经形态芯片上实现基于事件的汽车分类器。
<details>	<summary>英文摘要</summary>	Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip. </details>
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月02日</details>

# 166、基于脉冲神经网络的三维趋化算法
- [ ] Algorithm For 3D-Chemotaxis Using Spiking Neural Network 
时间：2021年06月30日                         第一作者：Jayesh Choudhary                       [链接](https://arxiv.org/abs/2106.16215).                     
## 摘要：在这项工作中，我们的目标是设计一个端到端的脉冲实现在三维媒体的轮廓跟踪启发趋化，其中蠕虫到达的地区，有给定的集合浓度。对于刨床介质，有效的轮廓跟踪算法已经被设计出来，但是新的自由度有相当多的挑战。在这里，我们设计了一个基于klinokinesis的算法——蠕虫的运动是对刺激的响应，但与刺激不成正比。因此，所遵循的路径不是最短的，但我们可以成功地跟踪设定浓度。考虑到在神经形态计算硬件上实现的可行性，我们使用简单的LIF神经元来实现神经网络。
<details>	<summary>英文摘要</summary>	In this work, we aim to devise an end-to-end spiking implementation for contour tracking in 3D media inspired by chemotaxis, where the worm reaches the region which has the given set concentration. For a planer medium, efficient contour tracking algorithms have already been devised, but a new degree of freedom has quite a few challenges. Here we devise an algorithm based on klinokinesis - where the motion of the worm is in response to the stimuli but not proportional to it. Thus the path followed is not the shortest, but we can track the set concentration successfully. We are using simple LIF neurons for the neural network implementation, considering the feasibility of its implementation in the neuromorphic computing hardware. </details>
<details>	<summary>注释</summary>	12 pages, 8 figures, accepted for the '30th International Conference on Artificial Neural Networks, ICANN2021' </details>
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 165、Spiking-GAN：一种使用时间到第一个Spike编码的生成性攻击网络
- [ ] Spiking-GAN: A Spiking Generative Adversarial Network Using Time-To-First-Spike Coding 
时间：2021年06月29日                         第一作者：Vineet Kotariya                       [链接](https://arxiv.org/abs/2106.15420).                     
## 摘要：脉冲神经网络（SNNs）在解决深度学习问题中显示出巨大的潜力。然而，它们仍然局限于简单的分类任务。在本文中，我们提出了第一个基于脉冲的生成性对抗网络（GAN）。它采用了一种称为时间到第一峰值编码的时态编码方案。我们使用时域近似反向传播来训练它。我们使用简单的集成和火灾（如果）神经元非常高的不应期为我们的网络，这确保了一个神经元的最大峰值。这使得该模型比基于峰值速率的系统要稀疏得多。我们改进的时间损失函数称为“攻击性TTFS”，与以前的工作相比，网络的推理时间提高了33%以上，网络中的脉冲数减少了11%以上。实验表明，利用该方法在MNIST数据集上训练网络，可以生成高质量的样本。从而证明了该框架在解决脉冲域中的此类问题方面的潜力。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have shown great potential in solving deep learning problems in an energy-efficient manner. However, they are still limited to simple classification tasks. In this paper, we propose Spiking-GAN, the first spike-based Generative Adversarial Network (GAN). It employs a kind of temporal coding scheme called time-to-first-spike coding. We train it using approximate backpropagation in the temporal domain. We use simple integrate-and-fire (IF) neurons with very high refractory period for our network which ensures a maximum of one spike per neuron. This makes the model much sparser than a spike rate-based system. Our modified temporal loss function called 'Aggressive TTFS' improves the inference time of the network by over 33% and reduces the number of spikes in the network by more than 11% compared to previous works. Our experiments show that on training the network on the MNIST dataset using this approach, we can generate high quality samples. Thereby demonstrating the potential of this framework for solving such problems in the spiking domain. </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 164、分叉脉冲神经网络
- [ ] Bifurcation Spiking Neural Network 
时间：2021年06月25日                         第一作者：Shao-Qun Zhang                        [链接](https://arxiv.org/abs/1909.08341).                     
<details>	<summary>注释</summary>	18 pages </details>
<details>	<summary>邮件日期</summary>	2021年06月28日</details>

# 163、群体编码和动态神经元改进的脉冲参与者网络用于强化学习
- [ ] Population-coding and Dynamic-neurons improved Spiking Actor Network for Reinforcement Learning 
时间：2021年06月23日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2106.07854).                     
<details>	<summary>注释</summary>	27 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月24日</details>

# 162、深度脉冲神经网络的梯度重排剪枝
- [ ] Pruning of Deep Spiking Neural Networks through Gradient Rewiring 
时间：2021年06月22日                         第一作者：Yanqi Chen                       [链接](https://arxiv.org/abs/2105.04916).                     
<details>	<summary>注释</summary>	9 pages, 7 figures, 4 tables. To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021) </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 161、约束塑性储备作为神经网络频率和权值控制的一种自然方法
- [ ] Constrained plasticity reserve as a natural way to control frequency and weights in spiking neural networks 
时间：2021年06月20日                         第一作者：Oleg Nikitin                        [链接](https://arxiv.org/abs/2103.08143).                     
<details>	<summary>注释</summary>	24 pages, 10 figures MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 160、SiamSNN：用于节能目标跟踪的siames脉冲神经网络
- [ ] SiamSNN: Siamese Spiking Neural Networks for Energy-Efficient Object Tracking 
时间：2021年06月19日                         第一作者：Yihao Luo                       [链接](https://arxiv.org/abs/2003.07584).                     
<details>	<summary>注释</summary>	Accepted by ICANN2021, 12 pages, 5figures </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 159、VLSI电路约束对时序编码多层脉冲神经网络的影响
- [ ] Effects of VLSI Circuit Constraints on Temporal-Coding Multilayer Spiking Neural Networks 
时间：2021年06月18日                         第一作者：Yusuke Sakemi                       [链接](https://arxiv.org/abs/2106.10382).                     
## 摘要：脉冲神经网络（spiking neural network，SNN）不仅作为大脑的一种数学模型，而且作为一种能量有效的信息处理模型，在现实世界中得到了广泛的应用。特别地，基于时态编码的snn被期望比基于速率编码的snn更有效，因为前者需要更少的峰值来执行任务。由于snn是连续状态和连续时间模型，用模拟VLSI电路实现snn是非常有利的。然而，当系统规模很大时，用连续时间模拟电路构建整个系统是不可行的。因此，必须采用混合信号电路，对突触权值进行时间离散化和量化。此外，SNNs的模拟VLSI实现具有非理想性，例如噪声和器件失配的影响，以及模拟电路操作引起的其他限制。在这项研究中，我们研究了时间离散化和/或权重量化对SNNs性能的影响。此外，我们还阐明了膜电位下限和放电阈值的时间波动的影响。最后，我们提出了一种将数学SNN模型映射到离散时间模拟电路的优化方法。
<details>	<summary>英文摘要</summary>	The spiking neural network (SNN) has been attracting considerable attention not only as a mathematical model for the brain, but also as an energy-efficient information processing model for real-world applications. In particular, SNNs based on temporal coding are expected to be much more efficient than those based on rate coding, because the former requires substantially fewer spikes to carry out tasks. As SNNs are continuous-state and continuous-time models, it is favorable to implement them with analog VLSI circuits. However, the construction of the entire system with continuous-time analog circuits would be infeasible when the system size is very large. Therefore, mixed-signal circuits must be employed, and the time discretization and quantization of the synaptic weights are necessary. Moreover, the analog VLSI implementation of SNNs exhibits non-idealities, such as the effects of noise and device mismatches, as well as other constraints arising from the analog circuit operation. In this study, we investigated the effects of the time discretization and/or weight quantization on the performance of SNNs. Furthermore, we elucidated the effects the lower bound of the membrane potentials and the temporal fluctuation of the firing threshold. Finally, we propose an optimal approach for the mapping of mathematical SNN models to analog circuits with discretized time. </details>
<details>	<summary>注释</summary>	corrected typos </details>
<details>	<summary>邮件日期</summary>	2021年06月28日</details>

# 158、基于模糊神经遗传算法的水电站恢复力优化规划
- [ ] HydroPower Plant Planning for Resilience Improvement of Power Systems using Fuzzy-Neural based Genetic Algorithm 
时间：2021年06月16日                         第一作者：Akbal Rain                       [链接](https://arxiv.org/abs/2106.12042).                     
## 摘要：本文提出了一种基于负荷频率控制（LFC）的小水电站优化设计新方法，该方法采用自校正模糊比例微分（PD）方法对规划进行估计和预测。由于频率不受任何甩负荷或其它因素的控制，因此该电站处于动态频率变化下，采用PD控制器进行模糊规则优化，再结合神经深度学习技术和遗传算法进行优化。这项工作的主要目的是使小水电站的频率保持在额定值。因此，本文提出的模糊PD遗传优化控制器将应用于小规模水电系统的线性调频控制。该方案可用于小水电和柴油发电机组的不同设计。也可以在水电系统中使用柴油发电机，当用户需求高于发电量时，柴油发电机可以关闭。仿真将在MATLAB/Simulink中进行，以表示和评估这种控制方案在动态频率变化下的性能。以脉冲神经网络（SNN）为主要的深度学习技术，对这种负载频率控制进行优化，形成深度脉冲神经网络（DSNN）。结果表明，与其他方法相比，该方案具有鲁棒性强、性能高的频率控制性能。
<details>	<summary>英文摘要</summary>	This paper will propose a novel technique for optimize hydropower plant in small scale based on load frequency control (LFC) which use self-tuning fuzzy Proportional- Derivative (PD) method for estimation and prediction of planning. Due to frequency is not controlled by any dump load or something else, so this power plant is under dynamic frequency variations that will use PD controller which optimize by fuzzy rules and then with neural deep learning techniques and Genetic Algorithm optimization. The main purpose of this work is because to maintain frequency in small-hydropower plant at nominal value. So, proposed controller means Fuzzy PD optimization with Genetic Algorithm will be used for LFC in small scale of hydropower system. The proposed schema can be used in different designation of both diesel generator and mini-hydropower system at low stream flow. It is also possible to use diesel generator at the hydropower system which can be turn off when Consumer demand is higher than electricity generation. The simulation will be done in MATLAB/Simulink to represent and evaluate the performance of this control schema under dynamic frequency variations. Spiking Neural Network (SNN) used as the main deep learning techniques to optimizing this load frequency control which turns into Deep Spiking Neural Network (DSNN). Obtained results represented that the proposed schema has robust and high-performance frequency control in comparison to other methods. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月24日</details>

# 157、深相量网络：连接传统和脉冲神经网络
- [ ] Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks 
时间：2021年06月15日                         第一作者：Wilkie Olin-Ammentorp                       [链接](https://arxiv.org/abs/2106.11908).                     
## 摘要：在这项工作中，我们扩展了标准的神经网络，假设神经元的激活与单位圆上的复数的角度相对应，即“相量”。这样的网络中的每一层通过对前一层的相位进行加权叠加并计算新的相位值来产生新的激活。这种广义体系结构允许模型达到高精度，并具有独特的优点，即可以在考虑或不考虑时间变量的情况下执行网络的数学等效版本。重要的是，时域中相位角的值可以稀疏地由一系列周期性重复的δ函数或“脉冲”表示。我们展示了在标准深度学习任务上相量网络的非时域训练，并且证明了这些网络可以在传统的非时域或脉冲时域中执行，而不需要转换步骤。这为构建深度网络提供了一个新的基础，该网络通过适合于神经形态计算硬件的基于时间脉冲的计算进行操作。
<details>	<summary>英文摘要</summary>	In this work, we extend standard neural networks by building upon an assumption that neuronal activations correspond to the angle of a complex number lying on the unit circle, or 'phasor.' Each layer in such a network produces new activations by taking a weighted superposition of the previous layer's phases and calculating the new phase value. This generalized architecture allows models to reach high accuracy and carries the singular advantage that mathematically equivalent versions of the network can be executed with or without regard to a temporal variable. Importantly, the value of a phase angle in the temporal domain can be sparsely represented by a periodically repeating series of delta functions or 'spikes'. We demonstrate the atemporal training of a phasor network on standard deep learning tasks and show that these networks can then be executed in either the traditional atemporal domain or spiking temporal domain with no conversion step needed. This provides a novel basis for constructing deep networkswhich operate via temporal, spike-based calculations suitable for neuromorphic computing hardware. </details>
<details>	<summary>注释</summary>	18 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 156、群体编码和动态神经元改进的脉冲参与者网络用于强化学习
- [ ] Population-coding and Dynamic-neurons improved Spiking Actor Network for Reinforcement Learning 
时间：2021年06月15日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2106.07854).                     
## 摘要：由于深度神经网络（DNNs）是一种功能强大的函数逼近器，深度强化学习（DRL）在机器人控制任务中得到了很好的应用。与普通人工神经元的dnn相比，具有生物合理性的脉冲神经元网络（SNN）包含了不同数量的脉冲神经元，使得它在时空信息的状态表示上具有很强的自然能力。基于一个混合学习框架，即脉冲-参与者网络从状态中推断行为，而深度批评网络评估参与者，我们提出了一个群体编码和动态神经元改进的脉冲-参与者网络（PDSAN），用于从两个不同的标度（输入编码和神经元编码）进行有效的状态表示。对于输入编码，我们采用具有动态感受野的总体编码来直接编码每个输入状态分量。对于神经元编码，我们提出了不同类型的动态神经元（包括一阶和二阶神经元动力学）来描述更复杂的神经元动力学。最后，使用双延迟深度确定性策略梯度算法（TD3-PDSAN）结合深度批评网络对PDSAN进行训练。大量的实验结果表明，我们的TD3-PDSAN模型在四个OpenAI健身房基准任务上取得了比现有模型更好的性能。利用SNN改进RL，使其向满足生物合理性的有效计算方向发展，是一个重要的尝试。
<details>	<summary>英文摘要</summary>	With the Deep Neural Networks (DNNs) as a powerful function approximator, Deep Reinforcement Learning (DRL) has been excellently demonstrated on robotic control tasks. Compared to DNNs with vanilla artificial neurons, the biologically plausible Spiking Neural Network (SNN) contains a diverse population of spiking neurons, making it naturally powerful on state representation with spatial and temporal information. Based on a hybrid learning framework, where a spike actor-network infers actions from states and a deep critic network evaluates the actor, we propose a Population-coding and Dynamic-neurons improved Spiking Actor Network (PDSAN) for efficient state representation from two different scales: input coding and neuronal coding. For input coding, we apply population coding with dynamically receptive fields to directly encode each input state component. For neuronal coding, we propose different types of dynamic-neurons (containing 1st-order and 2nd-order neuronal dynamics) to describe much more complex neuronal dynamics. Finally, the PDSAN is trained in conjunction with deep critic networks using the Twin Delayed Deep Deterministic policy gradient algorithm (TD3-PDSAN). Extensive experimental results show that our TD3-PDSAN model achieves better performance than state-of-the-art models on four OpenAI gym benchmark tasks. It is an important attempt to improve RL with SNN towards the effective computation satisfying biological plausibility. </details>
<details>	<summary>注释</summary>	27 pages, 11 figures, accepted by Journal of Neural Networks </details>
<details>	<summary>邮件日期</summary>	2021年06月16日</details>

# 155、基于脉冲时变塑性和梯度下降的脉冲神经网络SAR图像分类
- [ ] SAR Image Classification Based on Spiking Neural Network through Spike-Time Dependent Plasticity and Gradient Descent 
时间：2021年06月15日                         第一作者：Jiankun Chen                       [链接](https://arxiv.org/abs/2106.08005).                     
## 摘要：目前，基于卷积神经网络（CNN）的合成孔径雷达（SAR）图像分类方法存在抗噪性差、泛化能力差等问题。脉冲神经网络是类脑智能的核心组成部分之一，具有良好的应用前景。本文利用具有复杂时空信息的脉冲序列，基于SNN的无监督和有监督学习，构造了一个完整的SAR图像分类器。本文首先阐述了脉冲神经元模型、SNN的感受野以及脉冲序列的构建。提出了一种基于STDP的无监督学习算法和一种基于梯度下降的有监督学习算法。在MSTAR数据集的三类图像中，单层和双层无监督学习SNN的平均分类准确率分别为80.8%和85.1%。此外，无监督学习的收敛输出脉冲序列可以作为教学信号。基于TensorFlow框架，自下而上构建了单层监督学习SNN，分类准确率达到90.05%。通过比较SNN和cnn的抗噪性能和模型参数，验证了SNN的有效性和突出的优点。复制我们实验的代码可从\url获得{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}.
<details>	<summary>英文摘要</summary>	At present, the Synthetic Aperture Radar (SAR) image classification method based on convolution neural network (CNN) has faced some problems such as poor noise resistance and generalization ability. Spiking neural network (SNN) is one of the core components of brain-like intelligence and has good application prospects. This article constructs a complete SAR image classifier based on unsupervised and supervised learning of SNN by using spike sequences with complex spatio-temporal information. We firstly expound the spiking neuron model, the receptive field of SNN, and the construction of spike sequence. Then we put forward an unsupervised learning algorithm based on STDP and a supervised learning algorithm based on gradient descent. The average classification accuracy of single layer and bilayer unsupervised learning SNN in three categories images on MSTAR dataset is 80.8\% and 85.1\%, respectively. Furthermore, the convergent output spike sequences of unsupervised learning can be used as teaching signals. Based on the TensorFlow framework, a single layer supervised learning SNN is built from the bottom, and the classification accuracy reaches 90.05\%. By comparing noise resistance and model parameters between SNNs and CNNs, the effectiveness and outstanding advantages of SNN are verified. Code to reproduce our experiments is available at \url{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}. </details>
<details>	<summary>邮件日期</summary>	2021年06月16日</details>

# 154、脉冲神经网络的能量有效知识提取
- [ ] Energy-efficient Knowledge Distillation for Spiking Neural Networks 
时间：2021年06月14日                         第一作者：Dongjin Lee                       [链接](https://arxiv.org/abs/2106.07172).                     
## 摘要：脉冲神经网络（SNNs）作为传统人工神经网络（ANNs）的一种高效节能的替代方法，由于其事件驱动的计算能力而受到广泛关注。考虑到SNN模型在约束神经形态设备中的应用前景，许多研究将最初用于ANN模型压缩的技术，如网络量化、剪枝和知识提取等，应用到SNN中。其中，已有的关于知识提炼的研究报道了student-SNN模型的精度改进。然而，对SNN的一个重要特征&能量效率的分析却很少。在本文中，我们从准确性和能源效率两个方面深入分析了提取的SNN模型的性能。在这个过程中，我们观察到，当使用传统的知识蒸馏方法时，脉冲数量大幅增加，导致能源效率低下。在此基础上，提出了一种基于非均匀温度参数的知识提取方法。我们在两个不同的数据集上对我们的方法进行了评估，结果表明SNN学生既满足准确性的提高，也满足脉冲数目的减少。在MNIST数据集上，我们提出的学生SNN与传统知识提取方法训练的学生SNN相比，准确率提高了0.09%，峰值减少了65%。我们还将结果与其他SNN压缩技术和训练方法进行了比较。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have been gaining interest as energy-efficient alternatives of conventional artificial neural networks (ANNs) due to their event-driven computation. Considering the future deployment of SNN models to constrained neuromorphic devices, many studies have applied techniques originally used for ANN model compression, such as network quantization, pruning, and knowledge distillation, to SNNs. Among them, existing works on knowledge distillation reported accuracy improvements of student SNN model. However, analysis on energy efficiency, which is also an important feature of SNN, was absent. In this paper, we thoroughly analyze the performance of the distilled SNN model in terms of accuracy and energy efficiency. In the process, we observe a substantial increase in the number of spikes, leading to energy inefficiency, when using the conventional knowledge distillation methods. Based on this analysis, to achieve energy efficiency, we propose a novel knowledge distillation method with heterogeneous temperature parameters. We evaluate our method on two different datasets and show that the resulting SNN student satisfies both accuracy improvement and reduction of the number of spikes. On MNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy and produces 65% less spikes compared to the student SNN trained with conventional knowledge distillation method. We also compare the results with other SNN compression techniques and training methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 153、反向传播算法在脉冲神经元硬件上的实现
- [ ] The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware 
时间：2021年06月13日                         第一作者：Alpha Renner                       [链接](https://arxiv.org/abs/2106.07030).                     
## 摘要：自然神经系统的能力激发了新一代机器学习算法以及能够快速、低功耗信息处理的神经形态超大规模集成（VLSI）电路。然而，大多数现代机器学习算法在神经生理学上并不合理，因此不能直接在神经形态硬件中实现。特别是，现代深度学习的主力，反向传播算法，已经证明很难转化为神经形态的硬件。在这项研究中，我们提出了一个基于脉冲门控动态信息协调和处理的神经形态，脉冲反向传播算法，实现在英特尔的Loihi神经形态研究处理器。我们展示了一个原理证明三层电路，学习从MNIST数据集分类数字。这个实现展示了在现代深度学习应用程序中使用大规模并行、低功耗、低延迟的神经形态处理器的途径。
<details>	<summary>英文摘要</summary>	The capabilities of natural neural systems have inspired new generations of machine learning algorithms as well as neuromorphic very large-scale integrated (VLSI) circuits capable of fast, low-power information processing. However, most modern machine learning algorithms are not neurophysiologically plausible and thus are not directly implementable in neuromorphic hardware. In particular, the workhorse of modern deep learning, the backpropagation algorithm, has proven difficult to translate to neuromorphic hardware. In this study, we present a neuromorphic, spiking backpropagation algorithm based on pulse-gated dynamical information coordination and processing, implemented on Intel's Loihi neuromorphic research processor. We demonstrate a proof-of-principle three-layer circuit that learns to classify digits from the MNIST dataset. This implementation shows a path for using massively parallel, low-power, low-latency neuromorphic processors in modern deep learning applications. </details>
<details>	<summary>注释</summary>	20 pages, 5 figures Report-no: LA-UR-21-24457 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 152、基于脉冲神经网络的联合学习
- [ ] Federated Learning with Spiking Neural Networks 
时间：2021年06月11日                         第一作者：Yeshwanth Venkatesha                       [链接](https://arxiv.org/abs/2106.06579).                     
## 摘要：随着神经网络在资源受限的嵌入式设备中的广泛应用，对低功耗神经系统的需求也越来越大。脉冲神经网络（SNNs）是一种新兴的能源效率的替代传统的人工神经网络（ANNs）是众所周知的计算密集型。从应用的角度来看，由于联合学习涉及多个能量受限的设备，因此利用SNNs提供的能量效率有很大的空间。尽管snn非常重要，但是在像联邦学习这样的大规模分布式系统上训练snn的研究却很少。在本文中，我们将SNNs引入到一个更现实的联邦学习场景中。具体来说，我们提出了一个联邦学习框架，分散和隐私保护培训的SNN。为了验证所提出的联邦学习框架，我们使用CIFAR10和CIFAR100基准测试评估了SNNs在联邦学习各个方面的优势。我们观察到，当数据分布在联邦中的大量客户机上时，SNNs在总体准确率方面优于ANNs，超过15%，同时提供高达5.3倍的能效。除了效率之外，我们还分析了所提出的联邦SNN框架对客户端数据分布、散乱和梯度噪声的敏感性，并与人工神经网络进行了综合比较。
<details>	<summary>英文摘要</summary>	As neural networks get widespread adoption in resource-constrained embedded devices, there is a growing need for low-power neural systems. Spiking Neural Networks (SNNs)are emerging to be an energy-efficient alternative to the traditional Artificial Neural Networks (ANNs) which are known to be computationally intensive. From an application perspective, as federated learning involves multiple energy-constrained devices, there is a huge scope to leverage energy efficiency provided by SNNs. Despite its importance, there has been little attention on training SNNs on a large-scale distributed system like federated learning. In this paper, we bring SNNs to a more realistic federated learning scenario. Specifically, we propose a federated learning framework for decentralized and privacy-preserving training of SNNs. To validate the proposed federated learning framework, we experimentally evaluate the advantages of SNNs on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks. We observe that SNNs outperform ANNs in terms of overall accuracy by over 15% when the data is distributed across a large number of clients in the federation while providing up to5.3x energy efficiency. In addition to efficiency, we also analyze the sensitivity of the proposed federated SNN framework to data distribution among the clients, stragglers, and gradient noise and perform a comprehensive comparison with ANNs. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 151、具有平衡突触的单混合信号神经元的时空棘波模式选择性
- [ ] Spatiotemporal Spike-Pattern Selectivity in Single Mixed-Signal Neurons with Balanced Synapses 
时间：2021年06月10日                         第一作者：Mattias Nilsson                       [链接](https://arxiv.org/abs/2106.05686).                     
## 摘要：实现混合信号神经形态处理器超低功耗推理和学习的潜力，需要有效地利用其非均匀模拟电路以及稀疏的、基于时间的信息编码和处理。在这里，我们研究了时空相关器（STC）网络中输出神经元的基于脉冲时间的时空感受野，我们使用兴奋-抑制平衡的突触前输入代替专门的轴突或神经元延迟。我们提出了一个混合信号DYNAP-SE神经形态处理器的半实物实验，其中硬件神经元的五维感受野通过随机抽样均匀分布的输入脉冲模式来映射。我们发现，当平衡的突触前成分被随机编程时，一些神经元显示出不同的感受野。此外，我们还演示了如何通过激活不同的非均匀模拟突触电路子集，调整神经元以检测特定的时空特征，而神经元最初是非选择性的。与以前基于延迟的神经形态硬件实现相比，平衡的突触元件的能量耗散比每侧连接低一个数量级（0.65nj对9.3nj）。因此，我们展示了如何利用不均匀的突触电路来实现STC网络层的资源有效性，使突触地址重编程成为一种离散的特征调整机制。
<details>	<summary>英文摘要</summary>	Realizing the potential of mixed-signal neuromorphic processors for ultra-low-power inference and learning requires efficient use of their inhomogeneous analog circuitry as well as sparse, time-based information encoding and processing. Here, we investigate spike-timing-based spatiotemporal receptive fields of output-neurons in the Spatiotemporal Correlator (STC) network, for which we used excitatory-inhibitory balanced disynaptic inputs instead of dedicated axonal or neuronal delays. We present hardware-in-the-loop experiments with a mixed-signal DYNAP-SE neuromorphic processor, in which five-dimensional receptive fields of hardware neurons were mapped by randomly sampling input spike-patterns from a uniform distribution. We find that, when the balanced disynaptic elements are randomly programmed, some of the neurons display distinct receptive fields. Furthermore, we demonstrate how a neuron was tuned to detect a particular spatiotemporal feature, to which it initially was non-selective, by activating a different subset of the inhomogeneous analog synaptic circuits. The energy dissipation of the balanced synaptic elements is one order of magnitude lower per lateral connection (0.65 nJ vs 9.3 nJ per spike) than former delay-based neuromorphic hardware implementations. Thus, we show how the inhomogeneous synaptic circuits could be utilized for resource-efficient implementation of STC network layers, in a way that enables synapse-address reprogramming as a discrete mechanism for feature tuning. </details>
<details>	<summary>注释</summary>	This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible </details>
<details>	<summary>邮件日期</summary>	2021年06月11日</details>

# 150、基于反向传播的深脉冲神经网络时间脉冲序列学习
- [ ] Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks 
时间：2021年06月07日                         第一作者：Wenrui Zhang                       [链接](https://arxiv.org/abs/2002.10085).                     
<details>	<summary>注释</summary>	Accepted for spotlight presentation of NeurIPS (Neural Information Processing System) 2020: https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 149、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年06月05日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 148、SpikePropamine：脉冲神经网络的可微可塑性
- [ ] SpikePropamine: Differentiable Plasticity in Spiking Neural Networks 
时间：2021年06月04日                         第一作者：Samuel Schmidgall                       [链接](https://arxiv.org/abs/2106.02681).                     
## 摘要：在生物神经网络的学习过程中，突触效能的适应性变化已经被证明起着关键的作用。尽管有这样的灵感来源，许多使用脉冲神经网络（SNNs）的以学习为中心的应用程序仍然保持静态的突触连接，阻止了初始训练期后的额外学习。在这里，我们介绍了一个框架，同时学习潜在的固定权重和规则的动态突触可塑性和神经调节突触可塑性在SNNs通过梯度下降。我们进一步展示了这个框架在一系列具有挑战性的基准上的能力，学习了几个可塑性规则的参数，包括BCM、Oja以及它们各自的一组神经调节变体。实验结果表明，具有可微可塑性的SNN足以解决一组传统SNN无法解决的具有挑战性的时间学习任务，即使在存在显著噪声的情况下也是如此。这些网络也被证明能够在高维机器人学习任务中产生运动，在初始训练期间没有出现的新情况下，可以观察到几乎最小的性能退化。
<details>	<summary>英文摘要</summary>	The adaptive changes in synaptic efficacy that occur between spiking neurons have been demonstrated to play a critical role in learning for biological neural networks. Despite this source of inspiration, many learning focused applications using Spiking Neural Networks (SNNs) retain static synaptic connections, preventing additional learning after the initial training period. Here, we introduce a framework for simultaneously learning the underlying fixed-weights and the rules governing the dynamics of synaptic plasticity and neuromodulated synaptic plasticity in SNNs through gradient descent. We further demonstrate the capabilities of this framework on a series of challenging benchmarks, learning the parameters of several plasticity rules including BCM, Oja's, and their respective set of neuromodulatory variants. The experimental results display that SNNs augmented with differentiable plasticity are sufficient for solving a set of challenging temporal learning tasks that a traditional SNN fails to solve, even in the presence of significant noise. These networks are also shown to be capable of producing locomotion on a high-dimensional robotic learning task, where near-minimal degradation in performance is observed in the presence of novel conditions not seen during the initial training period. </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 147、基于时间到第一脉冲编码的能量有效的深脉冲神经网络训练
- [ ] Training Energy-Efficient Deep Spiking Neural Networks with Time-to-First-Spike Coding 
时间：2021年06月04日                         第一作者：Seongsik Park                       [链接](https://arxiv.org/abs/2106.02568).                     
## 摘要：深度神经网络（DNNs）的巨大能量消耗已经成为深度学习中的一个严重问题。脉冲神经网络（Spiking neural networks，SNNs）是一种模拟人脑操作的高效节能神经网络。由于snn的事件驱动和时空稀疏操作，使得snn具有高效节能处理的可能性。为了释放它们的潜能，深部snn采用了时间编码，如TTFS（time-To-first spike）编码，它代表了神经元之间在第一个峰值时间的信息。通过TTFS编码，每个神经元最多产生一个脉冲，从而显著提高了能量效率。已有一些研究成功地将TTFS编码引入深度snn，但由于缺乏对训练效率的考虑，其效率的提高受到限制。针对上述问题，本文提出了基于TTFS编码的能量有效的深度snn训练方法。我们引入了一个代理DNN模型来在可行的时间内训练深度SNN，并分析了时态核对训练性能和效率的影响。在此基础上，我们提出了随机松弛激活和基于初值的时间核参数正则化方法。此外，为了进一步减少峰值的数量，我们提出了时态核感知的批处理规范化。利用所提出的方法，我们可以在显著减少脉冲的情况下获得可比的训练结果，从而产生节能的深SNN。
<details>	<summary>英文摘要</summary>	The tremendous energy consumption of deep neural networks (DNNs) has become a serious problem in deep learning. Spiking neural networks (SNNs), which mimic the operations in the human brain, have been studied as prominent energy-efficient neural networks. Due to their event-driven and spatiotemporally sparse operations, SNNs show possibilities for energy-efficient processing. To unlock their potential, deep SNNs have adopted temporal coding such as time-to-first-spike (TTFS)coding, which represents the information between neurons by the first spike time. With TTFS coding, each neuron generates one spike at most, which leads to a significant improvement in energy efficiency. Several studies have successfully introduced TTFS coding in deep SNNs, but they showed restricted efficiency improvement owing to the lack of consideration for efficiency during training. To address the aforementioned issue, this paper presents training methods for energy-efficient deep SNNs with TTFS coding. We introduce a surrogate DNN model to train the deep SNN in a feasible time and analyze the effect of the temporal kernel on training performance and efficiency. Based on the investigation, we propose stochastically relaxed activation and initial value-based regularization for the temporal kernel parameters. In addition, to reduce the number of spikes even further, we present temporal kernel-aware batch normalization. With the proposed methods, we could achieve comparable training results with significantly reduced spikes, which could lead to energy-efficient deep SNNs. </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 146、基于事件的脉冲神经网络光流自监督学习
- [ ] Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks 
时间：2021年06月03日                         第一作者：Federico Paredes-Vall\'es                       [链接](https://arxiv.org/abs/2106.01862).                     
## 摘要：神经形态传感和计算为高能量效率和高带宽的传感器处理带来了希望。神经形态计算面临的一个主要挑战是，传统人工神经网络（ANNs）的学习算法由于离散脉冲和更复杂的神经元动力学特性而不能直接转换为脉冲神经网络（SNNs）。因此，snn尚未成功地应用于复杂的大规模任务。本文主要研究基于事件的摄像机输入光流估计的自监督学习问题，并研究了最新的人工神经网络训练流水线所需的变化，以便成功地用SNNs解决这一问题。更具体地说，我们首先修改输入事件表示，用最少的显式时间信息编码更小的时间片。因此，我们使网络的神经元动力学和循环连接负责整合信息随着时间的推移。此外，我们重新构造了基于事件的光流的自监督损失函数，以改善其凸性。我们使用所提出的管道对各种类型的递归神经网络和snn进行了实验。关于SNNs，我们研究了参数初始化和优化、替代梯度形状和自适应神经元机制等因素的影响。我们发现初始化和替代梯度宽度在稀疏输入的学习中起着关键作用，而自适应性和可学习神经元参数的加入可以提高学习性能。结果表明，所提出的人工神经网络和神经网络的性能与目前最先进的自监督训练人工神经网络相当。
<details>	<summary>英文摘要</summary>	Neuromorphic sensing and computing hold a promise for highly energy-efficient and high-bandwidth-sensor processing. A major challenge for neuromorphic computing is that learning algorithms for traditional artificial neural networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due to the discrete spikes and more complex neuronal dynamics. As a consequence, SNNs have not yet been successfully applied to complex, large-scale tasks. In this article, we focus on the self-supervised learning problem of optical flow estimation from event-based camera inputs, and investigate the changes that are necessary to the state-of-the-art ANN training pipeline in order to successfully tackle it with SNNs. More specifically, we first modify the input event representation to encode a much smaller time slice with minimal explicit temporal information. Consequently, we make the network's neuronal dynamics and recurrent connections responsible for integrating information over time. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner. </details>
<details>	<summary>邮件日期</summary>	2021年06月04日</details>

# 145、可微点过程及其在脉冲神经网络中的应用
- [ ] A Differentiable Point Process with Its Application to Spiking Neural Networks 
时间：2021年06月03日                         第一作者：Hiroshi Kajino                       [链接](https://arxiv.org/abs/2106.00901).                     
<details>	<summary>注释</summary>	Accepted to ICML 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月04日</details>

# 144、可微点过程及其在脉冲神经网络中的应用
- [ ] A Differentiable Point Process with Its Application to Spiking Neural Networks 
时间：2021年06月02日                         第一作者：Hiroshi Kajino                       [链接](https://arxiv.org/abs/2106.00901).                     
## 摘要：本文研究了一种脉冲神经网络概率模型的学习算法。Jimenez-Rezende和Gerstner（2014）提出了一种随机变分推理算法来训练具有隐藏神经元的snn。该算法利用分数函数梯度估计更新变分分布，其高方差往往阻碍整个学习算法。提出了一种基于路径梯度估计的snn梯度估计方法。主要的技术难点是缺乏一种通用的方法来区分任意点过程的实现，这是导出路径梯度估计器所必需的。我们发展了一个可微点过程，这是本文的技术亮点，并将其应用于推导SNNs的路径梯度估计。通过数值模拟研究了梯度估计的有效性。
<details>	<summary>英文摘要</summary>	This paper is concerned about a learning algorithm for a probabilistic model of spiking neural networks (SNNs). Jimenez Rezende & Gerstner (2014) proposed a stochastic variational inference algorithm to train SNNs with hidden neurons. The algorithm updates the variational distribution using the score function gradient estimator, whose high variance often impedes the whole learning algorithm. This paper presents an alternative gradient estimator for SNNs based on the path-wise gradient estimator. The main technical difficulty is a lack of a general method to differentiate a realization of an arbitrary point process, which is necessary to derive the path-wise gradient estimator. We develop a differentiable point process, which is the technical highlight of this paper, and apply it to derive the path-wise gradient estimator for SNNs. We investigate the effectiveness of our gradient estimator through numerical simulation. </details>
<details>	<summary>注释</summary>	Accepted to ICML 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 143、通过信息瓶颈学习脉冲神经网络的时间译码
- [ ] Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck 
时间：2021年06月02日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2106.01177).                     
## 摘要：训练脉冲神经网络（SNNs）的一个关键挑战是，目标输出通常以自然信号的形式出现，例如用于分类的标签或用于生成模型的图像，并且需要编码成脉冲。这是通过手工制作目标脉冲信号来实现的，这又隐含地修复了用于将脉冲解码为自然信号的机制，例如速率解码。目标信号和解码规则的任意选择通常会削弱SNN在脉冲定时中编码和处理信息的能力。为了解决这一问题，本文提出了一种混合变分自动编码器结构，由编码SNN和译码人工神经网络（ANN）组成。解码神经网络的作用是学习如何将SNN输出的脉冲信号最佳地转换为目标自然信号。提出了一种新的端到端学习规则，通过代理梯度优化有向信息瓶颈训练准则。我们证明了该技术在各种任务的实验环境中的适用性，包括真实的数据集。
<details>	<summary>英文摘要</summary>	One of the key challenges in training Spiking Neural Networks (SNNs) is that target outputs typically come in the form of natural signals, such as labels for classification or images for generative models, and need to be encoded into spikes. This is done by handcrafting target spiking signals, which in turn implicitly fixes the mechanisms used to decode spikes into natural signals, e.g., rate decoding. The arbitrary choice of target signals and decoding rule generally impairs the capacity of the SNN to encode and process information in the timing of spikes. To address this problem, this work introduces a hybrid variational autoencoder architecture, consisting of an encoding SNN and a decoding Artificial Neural Network (ANN). The role of the decoding ANN is to learn how to best convert the spiking signals output by the SNN into the target natural signal. A novel end-to-end learning rule is introduced that optimizes a directed information bottleneck training criterion via surrogate gradients. We demonstrate the applicability of the technique in an experimental settings on various tasks, including real-life datasets. </details>
<details>	<summary>注释</summary>	Under review for conference publication </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 142、自下而上和自上而下的神经处理系统设计：自然智能和人工智能融合的神经形态智能
- [ ] Bottom-Up and Top-Down Neural Processing Systems Design: Neuromorphic Intelligence as the Convergence of Natural and Artificial Intelligence 
时间：2021年06月02日                         第一作者：Charlotte Frenkel                       [链接](https://arxiv.org/abs/2106.01288).                     
## 摘要：虽然摩尔定律推动了人们对计算能力的指数预期，但它的接近尾声呼唤着改善系统整体性能的新途径。这些途径之一是探索新的替代大脑启发的计算架构，承诺实现生物神经处理系统的灵活性和计算效率。在这一背景下，神经形态智能代表了一个范式的转变，在计算的基础上实现了脉冲神经网络结构紧密地协同定位处理和记忆。在本文中，我们提供了该领域的全面概述，强调了现有硅实现中存在的不同粒度级别，比较了旨在复制自然智能（自下而上）和旨在解决实际人工智能应用（自上而下）的方法，并评估用于实现这些目标的不同电路设计风格的好处。首先，我们介绍了模拟、混合信号和数字电路的设计风格，通过时间复用、内存计算和新型器件来确定处理和内存之间的边界。接下来，我们将重点介绍自底向上和自顶向下方法的关键权衡，调查它们的硅实现，并进行详细的比较分析以提取设计准则。最后，我们确定了实现神经形态边缘计算相对于传统机器学习加速器的竞争优势所需的必要协同作用和缺失元素，并概述了神经形态智能框架的关键元素。
<details>	<summary>英文摘要</summary>	While Moore's law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of new alternative brain-inspired computing architectures that promise to achieve the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic intelligence represents a paradigm shift in computing based on the implementation of spiking neural network architectures tightly co-locating processing and memory. In this paper, we provide a comprehensive overview of the field, highlighting the different levels of granularity present in existing silicon implementations, comparing approaches that aim at replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down), and assessing the benefits of the different circuit design styles used to achieve these goals. First, we present the analog, mixed-signal and digital circuit design styles, identifying the boundary between processing and memory through time multiplexing, in-memory computation and novel devices. Next, we highlight the key tradeoffs for each of the bottom-up and top-down approaches, survey their silicon implementations, and carry out detailed comparative analyses to extract design guidelines. Finally, we identify both necessary synergies and missing elements required to achieve a competitive advantage for neuromorphic edge computing over conventional machine-learning accelerators, and outline the key elements for a framework toward neuromorphic intelligence. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 141、基于平衡脉冲神经网络的振动异常在线检测
- [ ] Online Detection of Vibration Anomalies Using Balanced Spiking Neural Networks 
时间：2021年06月01日                         第一作者：Nik Dennler                       [链接](https://arxiv.org/abs/2106.00687).                     
## 摘要：振动模式产生了关于运行机器健康状态的有价值的信息，这通常用于大型工业系统的预测性维护任务。然而，在尺寸、复杂性和功率预算方面，利用这些信息的经典方法所需的开销对于较小规模的应用（如自动汽车、无人机或机器人）通常是禁止的。在这里，我们提出了一种神经形态的方法来执行振动分析使用脉冲神经网络，可以应用于广泛的场景。我们提出了一种基于脉冲的端到端管道，能够使用与模拟-数字神经形态电路兼容的构建块从振动数据中检测系统异常。该管道以在线无监督方式运行，依赖于耳蜗模型、反馈自适应和平衡脉冲神经网络。我们证明了所提出的方法在两个公开的数据集上达到了最先进的性能或更好的性能。此外，我们还演示了在异步神经形态处理器设备上实现的工作概念证明。这项工作是朝着设计和实现用于在线振动监测的自主低功耗边缘计算设备迈出的重要一步。
<details>	<summary>英文摘要</summary>	Vibration patterns yield valuable information about the health state of a running machine, which is commonly exploited in predictive maintenance tasks for large industrial systems. However, the overhead, in terms of size, complexity and power budget, required by classical methods to exploit this information is often prohibitive for smaller-scale applications such as autonomous cars, drones or robotics. Here we propose a neuromorphic approach to perform vibration analysis using spiking neural networks that can be applied to a wide range of scenarios. We present a spike-based end-to-end pipeline able to detect system anomalies from vibration data, using building blocks that are compatible with analog-digital neuromorphic circuits. This pipeline operates in an online unsupervised fashion, and relies on a cochlea model, on feedback adaptation and on a balanced spiking neural network. We show that the proposed method achieves state-of-the-art performance or better against two publicly available data sets. Further, we demonstrate a working proof-of-concept implemented on an asynchronous neuromorphic processor device. This work represents a significant step towards the design and implementation of autonomous low-power edge-computing devices for online vibration monitoring. </details>
<details>	<summary>注释</summary>	This work is presented at the 2021 IEEE AICAS </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 140、基于事件的反向传播可以精确计算脉冲神经网络的梯度
- [ ] Event-Based Backpropagation can compute Exact Gradients for Spiking Neural Networks 
时间：2021年05月31日                         第一作者：Timo C. Wunderlich                       [链接](https://arxiv.org/abs/2009.08378).                     
<details>	<summary>邮件日期</summary>	2021年06月02日</details>

# 139、STDP训练的脉冲神经网络预处理对时空动作识别的影响研究
- [ ] A Study On the Effects of Pre-processing On Spatio-temporal Action Recognition Using Spiking Neural Networks Trained with STDP 
时间：2021年05月31日                         第一作者：El-Assal Mireille                        [链接](https://arxiv.org/abs/2105.14740).                     
## 摘要：近年来，脉冲神经网络受到越来越多的关注。snn被视为解决ann在模式识别中的瓶颈问题（如能源效率）的假设性解决方案。但是目前的方法，如ANN-to-SNN转换和反向传播等，并没有充分利用这些网络，无监督方法还没有达到与先进的人工神经网络相媲美的成功。研究非监督学习方法训练的snn在视频分类任务中的行为非常重要，例如脉冲时间依赖可塑性（STDP），包括利用脉冲模拟运动信息的机制，因为这些信息对于视频理解至关重要。本文提出了多种方法将时间信息转换成静态格式，然后使用延迟编码将视觉信息转换成脉冲。这些方法与早期和晚期两种时间融合方法相结合，用于帮助脉冲神经网络捕获视频中的时空特征。本文利用STDP训练的卷积脉冲神经网络的网络结构，测试了该网络在动作识别任务中的性能。了解脉冲神经网络如何响应不同的运动提取和表示方法，有助于缩小snn和ann之间的性能差距。本文利用脉冲神经网络研究了动作形状和速度的相似性对动作识别的影响，并与其它方法进行了比较。
<details>	<summary>英文摘要</summary>	There has been an increasing interest in spiking neural networks in recent years. SNNs are seen as hypothetical solutions for the bottlenecks of ANNs in pattern recognition, such as energy efficiency. But current methods such as ANN-to-SNN conversion and back-propagation do not take full advantage of these networks, and unsupervised methods have not yet reached a success comparable to advanced artificial neural networks. It is important to study the behavior of SNNs trained with unsupervised learning methods such as spike-timing dependent plasticity (STDP) on video classification tasks, including mechanisms to model motion information using spikes, as this information is critical for video understanding. This paper presents multiple methods of transposing temporal information into a static format, and then transforming the visual information into spikes using latency coding. These methods are paired with two types of temporal fusion known as early and late fusion, and are used to help the spiking neural network in capturing the spatio-temporal features from videos. In this paper, we rely on the network architecture of a convolutional spiking neural network trained with STDP, and we test the performance of this network when challenged with action recognition tasks. Understanding how a spiking neural network responds to different methods of movement extraction and representation can help reduce the performance gap between SNNs and ANNs. In this paper we show the effect of the similarity in the shape and speed of certain actions on action recognition with spiking neural networks, we also highlight the effectiveness of some methods compared to others. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 138、基于脉冲神经网络的硅视网膜生物视觉注意模式分类
- [ ] Bio-inspired visual attention for silicon retinas based on spiking neural networks applied to pattern classification 
时间：2021年05月31日                         第一作者：Am\'elie Gruel                        [链接](https://arxiv.org/abs/2105.14753).                     
## 摘要：视觉注意可以定义为一种行为和认知过程，即有选择地集中在感官线索的一个离散方面，而忽略其他可感知的信息。这种生物机制，特别是显著性检测，长期以来被用于多媒体索引中，只对图像或视频的相关部分进行分析，以便进一步处理。最近出现的硅视网膜（或称事件摄像机——测量亮度像素级变化并相应输出异步事件的传感器）提出了一个问题，即如何使注意力和显著性适应这种传感器的非常规输出类型。硅视网膜旨在重现视网膜的生物学行为。在这方面，它们在时间上产生准时的事件，这些事件可以被解释为神经脉冲，并被神经网络解释为神经脉冲。特别是，脉冲神经网络（Spiking Neural Networks，SNNs）代表了一种比传统人工网络更接近生物学的异步型人工神经网络，主要是因为它们试图模拟神经膜和动作电位随时间的动态变化。snn以脉冲序列的形式接收和处理信息。因此，它们为硅视网膜测量的传入事件模式的有效处理和分类提供了合适的候选者。在这篇论文中，我们回顾了注意机制背后的生物学背景，并介绍了一个案例研究事件视频分类与SNNs，使用一个基于生物学的低水平计算注意机制，并有有趣的初步结果。
<details>	<summary>英文摘要</summary>	Visual attention can be defined as the behavioral and cognitive process of selectively focusing on a discrete aspect of sensory cues while disregarding other perceivable information. This biological mechanism, more specifically saliency detection, has long been used in multimedia indexing to drive the analysis only on relevant parts of images or videos for further processing. The recent advent of silicon retinas (or event cameras -- sensors that measure pixel-wise changes in brightness and output asynchronous events accordingly) raises the question of how to adapt attention and saliency to the unconventional type of such sensors' output. Silicon retina aims to reproduce the biological retina behaviour. In that respect, they produce punctual events in time that can be construed as neural spikes and interpreted as such by a neural network. In particular, Spiking Neural Networks (SNNs) represent an asynchronous type of artificial neural network closer to biology than traditional artificial networks, mainly because they seek to mimic the dynamics of neural membrane and action potentials over time. SNNs receive and process information in the form of spike trains. Therefore, they make for a suitable candidate for the efficient processing and classification of incoming event patterns measured by silicon retinas. In this paper, we review the biological background behind the attentional mechanism, and introduce a case study of event videos classification with SNNs, using a biology-grounded low-level computational attention mechanism, with interesting preliminary results. </details>
<details>	<summary>注释</summary>	6 pages, 3 figures. To be published in Content-Based Multimedia Indexing (CBMI) 2021, Lille, France. This work was supported by the European Union's ERA-NET CHIST-ERA 2018 research and innovation programme under grant agreement ANR-19-CHR3-0008 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 137、脉冲时变塑性训练脉冲神经网络的泛化特征
- [ ] Characterization of Generalizability of Spike Time Dependent Plasticity trained Spiking Neural Networks 
时间：2021年05月31日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2105.14677).                     
## 摘要：脉冲时间依赖可塑性（STDP）训练的脉冲神经网络（SNN）是一种神经启发的无监督学习方法，适用于各种机器学习应用。本文利用学习算法轨迹的Hausdorff维数研究了STDP学习过程的概化性质。本文分析了STDP学习模型和相关超参数对SNN可概化性质的影响，刻画了SNN的可概化性与可学习性的权衡。该分析被用来开发一种贝叶斯优化方法来优化STDP模型的超参数，以提高SNN的泛化性能。
<details>	<summary>英文摘要</summary>	A Spiking Neural Network (SNN) trained with Spike Time Dependent Plasticity (STDP) is a neuro-inspired unsupervised learning method for various machine learning applications. This paper studies the generalizability properties of the STDP learning processes using the Hausdorff dimension of the trajectories of the learning algorithm. The paper analyzes the effects of STDP learning models and associated hyper-parameters on the generalizability properties of an SNN and characterizes the generalizability vs learnability trade-off in an SNN. The analysis is used to develop a Bayesian optimization approach to optimize the hyper-parameters for an STDP model to improve the generalizability properties of an SNN. </details>
<details>	<summary>注释</summary>	15 pages, submitted to Frontiers in Neuroscience. arXiv admin note: text overlap with arXiv:2010.08195, arXiv:2006.09313 by other authors </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 136、利用生物似然奖赏传播调整卷积脉冲神经网络
- [ ] Tuning Convolutional Spiking Neural Network with Biologically-plausible Reward Propagation 
时间：2021年05月31日                         第一作者：Tielin Zhang                        [链接](https://arxiv.org/abs/2010.04434).                     
<details>	<summary>注释</summary>	Final Version. Accepted by IEEE Transactions on Neural Networks and Learning Systems </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 135、在脉冲卷积神经网络中实现一个中心凹激发滤波器的初步研究
- [ ] Implementing a foveal-pit inspired filter in a Spiking Convolutional Neural Network: a preliminary study 
时间：2021年05月29日                         第一作者：Shriya T.P. Gupta                       [链接](https://arxiv.org/abs/2105.14326).                     
## 摘要：我们提出了一个脉冲卷积神经网络（SCNN），它结合了视网膜中央凹的高斯滤波器和秩序编码的启发差异。该模型是用一种适应于脉冲神经元的反向传播算法来训练的，如在Nengo库中实现的那样。我们已经在两个公开的数据集上评估了我们的模型的性能-一个用于数字识别任务，另一个用于车辆识别任务。该网络已达到90%的精度，其中损失是计算使用交叉熵函数。这是一个提高了57%左右的准确率获得的替代方法执行分类没有任何类型的神经滤波。总的来说，我们的概念验证研究表明，在现有的SCNN结构中引入生物学上合理的滤波可以很好地处理有噪声的输入图像，例如在我们的车辆识别任务中。基于我们的研究结果，我们计划通过在秩排序之前集成基于侧抑制的冗余约简来增强SCNN，这将进一步提高网络的分类精度。
<details>	<summary>英文摘要</summary>	We have presented a Spiking Convolutional Neural Network (SCNN) that incorporates retinal foveal-pit inspired Difference of Gaussian filters and rank-order encoding. The model is trained using a variant of the backpropagation algorithm adapted to work with spiking neurons, as implemented in the Nengo library. We have evaluated the performance of our model on two publicly available datasets - one for digit recognition task, and the other for vehicle recognition task. The network has achieved up to 90% accuracy, where loss is calculated using the cross-entropy function. This is an improvement over around 57% accuracy obtained with the alternate approach of performing the classification without any kind of neural filtering. Overall, our proof-of-concept study indicates that introducing biologically plausible filtering in existing SCNN architecture will work well with noisy input images such as those in our vehicle recognition task. Based on our results, we plan to enhance our SCNN by integrating lateral inhibition-based redundancy reduction prior to rank-ordering, which will further improve the classification accuracy by the network. </details>
<details>	<summary>注释</summary>	8 pages, 8 figures, 4 tables. 2020 International Joint Conference on Neural Networks (IJCNN) ACM-class: I.2.10; I.4.5; I.4.10 DOI: 10.1109/IJCNN48605.2020.9207612 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 134、DVS脉冲反应的中心凹激发滤波
- [ ] Foveal-pit inspired filtering of DVS spike response 
时间：2021年05月29日                         第一作者：Shriya T.P. Gupta                       [链接](https://arxiv.org/abs/2105.14331).                     
## 摘要：在本文中，我们提出了处理动态视觉传感器（DVS）记录的视觉模式与视网膜模型的基础上，中心凹坑启发差异高斯（狗）滤波器。用不同空间频率的垂直白条和黑条以恒定速度水平移动来刺激DVS传感器。由DVS传感器产生的输出脉冲被作为输入应用到一组狗过滤器，其灵感来自灵长类视觉通路的感受野结构。特别是，这些滤光片模拟了侏儒和副交感神经节细胞（视网膜的脉冲神经元）的感受野，这些细胞为中央凹的光受体服务。用中心凹模型提取的特征被用于进一步的分类，使用了一个适应于脉冲神经网络的反向传播变量训练的脉冲卷积神经网络。
<details>	<summary>英文摘要</summary>	In this paper, we present results of processing Dynamic Vision Sensor (DVS) recordings of visual patterns with a retinal model based on foveal-pit inspired Difference of Gaussian (DoG) filters. A DVS sensor was stimulated with varying number of vertical white and black bars of different spatial frequencies moving horizontally at a constant velocity. The output spikes generated by the DVS sensor were applied as input to a set of DoG filters inspired by the receptive field structure of the primate visual pathway. In particular, these filters mimic the receptive fields of the midget and parasol ganglion cells (spiking neurons of the retina) that sub-serve the photo-receptors of the foveal-pit. The features extracted with the foveal-pit model are used for further classification using a spiking convolutional neural network trained with a backpropagation variant adapted for spiking neural networks. </details>
<details>	<summary>注释</summary>	6 pages, 4 figures, 2 tables. 2021 55th Annual Conference on Information Sciences and Systems (CISS), 2021 ACM-class: I.2.10; I.4.5; I.4.10 DOI: 10.1109/CISS50987.2021.9400245 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 133、基于突触级强化学习的无梯度神经网络训练
- [ ] Gradient-Free Neural Network Training via Synaptic-Level Reinforcement Learning 
时间：2021年05月29日                         第一作者：Aman Bhargava                       [链接](https://arxiv.org/abs/2105.14383).                     
## 摘要：神经信息处理中的一个持续挑战是：神经元如何调整它们的连接性以随着时间的推移提高任务绩效（即实现学习）？人们普遍认为，在实现学习的特定大脑区域有一个一致的、突触水平的学习机制。然而，这一机制的确切性质仍不清楚。本文提出了一种基于强化学习（RL）的多层感知器（MLP）模型的简单突触级学习策略。在该算法中，每个MLP突触的动作空间由对突触权重的微小增加、减少或零动作组成，每个突触的状态由最后两个动作和奖赏信号组成。二元奖励信号表示任务绩效的改善或恶化。相对于自适应策略，静态策略产生更好的训练效果，并且与激活函数、网络形状和任务无关。训练mlp产生的字符识别性能可与梯度下降训练的同形网络相媲美。0隐单位字符识别测试的平均验证准确率为88.28%，比用梯度下降法训练的同一MLP高1.86$\pm$0.47%。32个隐单位字符识别测试的平均验证准确率为88.45%，比用梯度下降法训练的同一MLP低1.11$\pm$0.79%。鲁棒性和对梯度计算的不依赖性为训练难以区分的人工神经网络（如脉冲神经网络（SNNs）和递归神经网络（RNNs））的新技术打开了大门。此外，该方法的简单性为进一步开发类似于元胞自动机的机器智能局部规则驱动的多代理连接模型提供了独特的机会。
<details>	<summary>英文摘要</summary>	An ongoing challenge in neural information processing is: how do neurons adjust their connectivity to improve task performance over time (i.e., actualize learning)? It is widely believed that there is a consistent, synaptic-level learning mechanism in specific brain regions that actualizes learning. However, the exact nature of this mechanism remains unclear. Here we propose an algorithm based on reinforcement learning (RL) to generate and apply a simple synaptic-level learning policy for multi-layer perceptron (MLP) models. In this algorithm, the action space for each MLP synapse consists of a small increase, decrease, or null action on the synapse weight, and the state for each synapse consists of the last two actions and reward signals. A binary reward signal indicates improvement or deterioration in task performance. The static policy produces superior training relative to the adaptive policy and is agnostic to activation function, network shape, and task. Trained MLPs yield character recognition performance comparable to identically shaped networks trained with gradient descent. 0 hidden unit character recognition tests yielded an average validation accuracy of 88.28%, 1.86$\pm$0.47% higher than the same MLP trained with gradient descent. 32 hidden unit character recognition tests yielded an average validation accuracy of 88.45%, 1.11$\pm$0.79% lower than the same MLP trained with gradient descent. The robustness and lack of reliance on gradient computations opens the door for new techniques for training difficult-to-differentiate artificial neural networks such as spiking neural networks (SNNs) and recurrent neural networks (RNNs). Further, the method's simplicity provides a unique opportunity for further development of local rule-driven multi-agent connectionist models for machine intelligence analogous to cellular automata. </details>
<details>	<summary>注释</summary>	10 pages, 3 figures, submitted to NeurIPS 2021 MSC-class: 68T07 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 132、BSNN：实现人工神经网络向双稳态神经元脉冲神经网络的更快更好的转换
- [ ] BSNN: Towards Faster and Better Conversion of Artificial Neural Networks to Spiking Neural Networks with Bistable Neurons 
时间：2021年05月27日                         第一作者：Yang Li                       [链接](https://arxiv.org/abs/2105.12917).                     
## 摘要：脉冲神经网络（SNN）通过离散的二进制事件来计算和传递信息。在新兴的神经形态硬件中，它被认为比人工神经网络（ANN）在生物学上更合理、更节能。然而，由于SNN的不连续性和不可微性，训练SNN是一项相对具有挑战性的任务。最近的工作通过将ANN转换为SNN，在获得优异性能方面取得了重要进展。由于信息处理的差异，转换后的深度SNN通常会遭受严重的性能损失和较大的时延。本文分析了性能下降的原因，提出了一种新的双稳态脉冲神经网络（BSNN），解决了由相位超前和相位滞后引起的失活神经元脉冲问题。同时，基于ResNet结构的神经网络在转换时，由于快捷路径的快速传递，输出神经元的信息是不完整的。我们设计了同步神经元（SN）来帮助有效地提高性能。实验结果表明，该方法仅需1/4-1/10的时间步长即可实现几乎无损的转换。我们在具有挑战性的数据集（包括CIFAR-10（95.16%top-1）、CIFAR-100（78.12%top-1）和ImageNet（72.64%top-1））上演示了VGG16、ResNet20和ResNet34最先进的ANN-SNN转换。
<details>	<summary>英文摘要</summary>	The spiking neural network (SNN) computes and communicates information through discrete binary events. It is considered more biologically plausible and more energy-efficient than artificial neural networks (ANN) in emerging neuromorphic hardware. However, due to the discontinuous and non-differentiable characteristics, training SNN is a relatively challenging task. Recent work has achieved essential progress on an excellent performance by converting ANN to SNN. Due to the difference in information processing, the converted deep SNN usually suffers serious performance loss and large time delay. In this paper, we analyze the reasons for the performance loss and propose a novel bistable spiking neural network (BSNN) that addresses the problem of spikes of inactivated neurons (SIN) caused by the phase lead and phase lag. Also, when ResNet structure-based ANNs are converted, the information of output neurons is incomplete due to the rapid transmission of the shortcut path. We design synchronous neurons (SN) to help efficiently improve performance. Experimental results show that the proposed method only needs 1/4-1/10 of the time steps compared to previous work to achieve nearly lossless conversion. We demonstrate state-of-the-art ANN-SNN conversion for VGG16, ResNet20, and ResNet34 on challenging datasets including CIFAR-10 (95.16% top-1), CIFAR-100 (78.12% top-1), and ImageNet (72.64% top-1). </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 131、BackEISNN：一种具有自适应自反馈和平衡兴奋抑制神经元的深脉冲神经网络
- [ ] BackEISNN: A Deep Spiking Neural Network with Adaptive Self-Feedback and Balanced Excitatory-Inhibitory Neurons 
时间：2021年05月27日                         第一作者：Dongcheng Zhao                       [链接](https://arxiv.org/abs/2105.13004).                     
## 摘要：脉冲神经网络（SNNs）通过离散脉冲传递信息，在处理时空信息方面有很好的表现。由于snn的不可微性，设计性能良好的snn仍然存在困难。最近，由于梯度近似的提出，用反向传播训练的snn表现出了优越的性能。然而，对于复杂任务的处理性能，离深度神经网络还有很大的距离。我们从大脑中连接有自反馈连接的棘波神经元的自陷性中得到启发，在膜电位上应用自适应延时自反馈来调节棘波的精确度。同时，我们运用平衡的兴奋性和抑制性神经元机制来动态控制脉冲神经元的输出。结合这两种机制，我们提出了一种具有自适应自反馈和平衡兴奋和抑制神经元的深脉冲神经网络（BackEISNN）。在多个标准数据集上的实验结果表明，这两个模块不仅加快了网络的收敛速度，而且提高了精度。对于MNIST、FashionMNIST和N-MNIST数据集，我们的模型实现了最先进的性能。对于CIFAR10数据集，我们的BackEISNN在相对较轻的结构上也获得了显著的性能，与最先进的snn竞争。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) transmit information through discrete spikes, which performs well in processing spatial-temporal information. Due to the non-differentiable characteristic, there still exist difficulties in designing well-performed SNNs. Recently, SNNs trained with backpropagation have shown superior performance due to the proposal of the gradient approximation. However, the performance on complex tasks is still far away from the deep neural networks. Taking inspiration from the autapse in the brain which connects the spiking neurons with a self-feedback connection, we apply an adaptive time-delayed self-feedback on the membrane potential to regulate the spike precisions. As well as, we apply the balanced excitatory and inhibitory neurons mechanism to control the spiking neurons' output dynamically. With the combination of the two mechanisms, we propose a deep spiking neural network with adaptive self-feedback and balanced excitatory and inhibitory neurons (BackEISNN). The experimental results on several standard datasets have shown that the two modules not only accelerate the convergence of the network but also improve the accuracy. For the MNIST, FashionMNIST, and N-MNIST datasets, our model has achieved state-of-the-art performance. For the CIFAR10 dataset, our BackEISNN also gets remarkable performance on a relatively light structure that competes against state-of-the-art SNNs. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 130、基于时序神经网络的在线学习微体系结构实现框架
- [ ] A Microarchitecture Implementation Framework for Online Learning with Temporal Neural Networks 
时间：2021年05月27日                         第一作者：Harideep Nair                       [链接](https://arxiv.org/abs/2105.13262).                     
## 摘要：时间神经网络（TNNs）是一种利用时间作为资源来表示和处理信息的脉冲神经网络，类似于哺乳动物的大脑皮层。与采用单独训练和推理阶段的计算密集型深度神经网络相比，TNNs能够非常有效地进行在线增量/连续学习，是构建边缘本地感官处理单元的优秀候选。本文提出了一个用标准CMOS实现TNNs的微体系结构框架。给出了三个关键模块的门级实现：1）多突触神经元，2）多神经元列，3）基于脉冲时间依赖可塑性（STDP）的无监督和有监督在线学习算法。TNN微体系结构体现在一组特征标度方程中，用于评估任何TNN设计的门计数、面积、延迟和功耗。在45nmcmos中给出了设计的后合成结果，并证明了其在线增量学习能力。
<details>	<summary>英文摘要</summary>	Temporal Neural Networks (TNNs) are spiking neural networks that use time as a resource to represent and process information, similar to the mammalian neocortex. In contrast to compute-intensive Deep Neural Networks that employ separate training and inference phases, TNNs are capable of extremely efficient online incremental/continuous learning and are excellent candidates for building edge-native sensory processing units. This work proposes a microarchitecture framework for implementing TNNs using standard CMOS. Gate-level implementations of three key building blocks are presented: 1) multi-synapse neurons, 2) multi-neuron columns, and 3) unsupervised and supervised online learning algorithms based on Spike Timing Dependent Plasticity (STDP). The TNN microarchitecture is embodied in a set of characteristic scaling equations for assessing the gate count, area, delay and power consumption for any TNN design. Post-synthesis results (in 45nm CMOS) for the proposed designs are presented, and their online incremental learning capability is demonstrated. </details>
<details>	<summary>注释</summary>	To be published in ISVLSI 2021. arXiv admin note: substantial text overlap with arXiv:2009.00457 </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 129、用于深脉冲神经网络快速准确推理的最佳ANN-SNN转换
- [ ] Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks 
时间：2021年05月25日                         第一作者：Jianhao Ding                       [链接](https://arxiv.org/abs/2105.11654).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）作为一种受生物启发的节能神经网络，受到了研究者和工业界的广泛关注。训练深层SNN最有效的方法是通过ANN-SNN转换。然而，这种转换通常存在精度损失和推理时间长的问题，阻碍了SNN的实际应用。本文从理论上分析了ANN-SNN变换，给出了最佳变换的充分条件。为了更好地关联ANN-SNN并获得更高的准确度，我们提出了速率范数层来代替源ANN训练中的ReLU激活函数，实现了从训练的ANN到SNN的直接转换。此外，我们提出了一个最佳拟合曲线来量化源神经网络的激活值与目标SNN的实际发射率之间的拟合。结果表明，通过优化修正后的神经网络拟合曲线的上界，可以缩短推理时间，实现快速推理。我们的理论可以解释现有的快速推理工作，得到更好的结果。实验结果表明，该方法在VGG-16、PreActResNet-18和较深的结构上实现了近无损耗的转换。在典型方法能耗为0.265倍的情况下，推理速度提高了8.6倍。代码可在https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural networks, have attracted great attentions from researchers and industry. The most efficient way to train deep SNNs is through ANN-SNN conversion. However, the conversion usually suffers from accuracy loss and long inference time, which impede the practical application of SNN. In this paper, we theoretically analyze ANN-SNN conversion and derive sufficient conditions of the optimal conversion. To better correlate ANN-SNN and get greater accuracy, we propose Rate Norm Layer to replace the ReLU activation function in source ANN training, enabling direct conversion from a trained ANN to an SNN. Moreover, we propose an optimal fit curve to quantify the fit between the activation value of source ANN and the actual firing rate of target SNN. We show that the inference time can be reduced by optimizing the upper bound of the fit curve in the revised ANN to achieve fast inference. Our theory can explain the existing work on fast reasoning and get better results. The experimental results show that the proposed method achieves near loss less conversion with VGG-16, PreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster reasoning performance under 0.265x energy consumption of the typical method. The code is available at https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures, 2 tables. To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021) </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 128、深度脉冲神经网络的梯度重排剪枝
- [ ] Pruning of Deep Spiking Neural Networks through Gradient Rewiring 
时间：2021年05月20日                         第一作者：Yanqi Chen                       [链接](https://arxiv.org/abs/2105.04916).                     
<details>	<summary>注释</summary>	9 pages, 7 figures, 4 tables. To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021) </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 127、结合反向传播和STDP的半监督学习：STDP通过反向传播在脉冲神经网络中使用少量的标记数据来增强学习
- [ ] Semi-supervised learning combining backpropagation and STDP: STDP enhances learning by backpropagation with a small amount of labeled data in a spiking neural network 
时间：2021年05月19日                         第一作者：Kotaro Furuya                        [链接](https://arxiv.org/abs/2102.10530).                     
<details>	<summary>注释</summary>	9 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 126、稀疏脉冲梯度下降
- [ ] Sparse Spiking Gradient Descent 
时间：2021年05月18日                         第一作者：Nicolas Perez-Nieves                        [链接](https://arxiv.org/abs/2105.08810).                     
## 摘要：在神经形态计算设备上模拟脉冲神经网络（Spiking Neural Networks，SNNs）由于其能耗低而受到越来越多的关注。最近的进展使得训练snn在精确度方面开始与传统的人工神经网络（ANNs）竞争，同时在神经形态硬件上运行时具有能量效率。然而，训练snn的过程仍然是基于最初为ann开发的稠密张量运算，而ann没有利用snn的时空稀疏性。我们在这里提出了第一个稀疏SNN反向传播算法，该算法实现了与当前最先进的方法相同或更好的精度，同时显著提高了速度和内存效率。我们在不同复杂度的真实数据集（时尚MNIST、神经性MNIST和海德堡数字脉冲）上展示了我们的方法的有效性，实现了高达70倍的向后传递加速，并且在不损失准确度的情况下提高了40%的内存效率。
<details>	<summary>英文摘要</summary>	There is an increasing interest in emulating Spiking Neural Networks (SNNs) on neuromorphic computing devices due to their low energy consumption. Recent advances have allowed training SNNs to a point where they start to compete with traditional Artificial Neural Networks (ANNs) in terms of accuracy, while at the same time being energy efficient when run on neuromorphic hardware. However, the process of training SNNs is still based on dense tensor operations originally developed for ANNs which do not leverage the spatiotemporally sparse nature of SNNs. We present here the first sparse SNN backpropagation algorithm which achieves the same or better accuracy as current state of the art methods while being significantly faster and more memory efficient. We show the effectiveness of our method on real datasets of varying complexity (Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a speedup in the backward pass of up to 70x, and 40% more memory efficient, without losing accuracy. </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 125、PLSM：一种用于无意行为检测的并行化液态机
- [ ] PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection 
时间：2021年05月06日                         第一作者：Dipayan Das                       [链接](https://arxiv.org/abs/2105.09909).                     
## 摘要：水库计算（RC）为在低端嵌入式系统平台上部署人工智能算法提供了一个可行的选择。液体状态机（LSM）是一种仿生RC模型，它模拟大脑皮层微电路，使用可直接在神经形态硬件上实现的脉冲神经网络（SNN）。在本文中，我们提出了一种新的并行LSM（PLSM）架构，它结合了时空读出层和模型输出的语义约束。据我们所知，这样一个公式在文献中还是第一次，它提供了一个比传统的深度学习模型计算量更轻的替代方案。此外，我们还提出了一个完整的算法来实现与GPU兼容的可并行snn和lsm。利用Oops数据集实现了PLSM模型对无意/意外视频片段进行分类。从视频中无意行为的检测实验结果可以看出，本文提出的模型优于自监督模型和完全监督的传统深度学习模型。所有实现的代码都可以在我们的存储库中找到https://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.
<details>	<summary>英文摘要</summary>	Reservoir Computing (RC) offers a viable option to deploy AI algorithms on low-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired RC model that mimics the cortical microcircuits and uses spiking neural networks (SNN) that can be directly realized on neuromorphic hardware. In this paper, we present a novel Parallelized LSM (PLSM) architecture that incorporates spatio-temporal read-out layer and semantic constraints on model output. To the best of our knowledge, such a formulation has been done for the first time in literature, and it offers a computationally lighter alternative to traditional deep-learning models. Additionally, we also present a comprehensive algorithm for the implementation of parallelizable SNNs and LSMs that are GPU-compatible. We implement the PLSM model to classify unintentional/accidental video clips, using the Oops dataset. From the experimental results on detecting unintentional action in video, it can be observed that our proposed model outperforms a self-supervised model and a fully supervised traditional deep learning model. All the implemented codes can be found at our repository https://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition. </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 124、具有第一脉冲时间的快速节能神经形态深度学习
- [ ] Fast and energy-efficient neuromorphic deep learning with first-spike times 
时间：2021年05月17日                         第一作者：Julian G\"oltz                       [链接](https://arxiv.org/abs/1912.11443).                     
<details>	<summary>注释</summary>	24 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 123、SpikE：基于SpikE的多关系图数据嵌入
- [ ] SpikE: spike-based embeddings for multi-relational graph data 
时间：2021年05月17日                         第一作者：Dominik Dold                       [链接](https://arxiv.org/abs/2104.13398).                     
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 122、基于遗传算法的皮层脉冲神经网络多目标优化
- [ ] Multi-Objective Optimisation of Cortical Spiking Neural Networks With Genetic Algorithms 
时间：2021年05月14日                         第一作者：James Fitzgerald                        [链接](https://arxiv.org/abs/2105.06824).                     
## 摘要：脉冲神经网络（SNNs）通过神经元的全部或无脉冲活动进行通信。然而，在生物实验中，将大量SNN模型参数与观察到的神经活动模式相匹配仍然是一个挑战。以前使用遗传算法（GA）优化特定有效SNN模型的工作，使用Izhikevich神经元模型，仅限于单个参数和目标。这项工作应用了一种称为非支配排序遗传算法（NSGA-III）的遗传算法，以证明对同一SNN进行多目标优化的可行性，重点是搜索网络连接参数，以实现兴奋性和抑制性神经元类型的目标放电率，包括跨不同网络连接的稀疏性。我们表明，NSGA-III可以很容易地优化各种发射率。值得注意的是，当兴奋性神经放电率高于或等于抑制性神经元时，误差很小。此外，当连接稀疏性作为优化参数时，最优解需要稀疏的网络连接。我们还发现，对于兴奋性神经元的放电率低于抑制性神经元，误差通常较大。总的来说，我们成功地证明了对递归和稀疏SNN网络参数进行多目标遗传优化的可行性。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) communicate through the all-or-none spiking activity of neurons. However, fitting the large number of SNN model parameters to observed neural activity patterns, for example, in biological experiments, remains a challenge. Previous work using genetic algorithm (GA) optimisation on a specific efficient SNN model, using the Izhikevich neuronal model, was limited to a single parameter and objective. This work applied a version of GA, called non-dominated sorting GA (NSGA-III), to demonstrate the feasibility of performing multi-objective optimisation on the same SNN, focusing on searching network connectivity parameters to achieve target firing rates of excitatory and inhibitory neuronal types, including across different network connectivity sparsity. We showed that NSGA-III could readily optimise for various firing rates. Notably, when the excitatory neural firing rates were higher than or equal to that of inhibitory neurons, the errors were small. Moreover, when connectivity sparsity was considered as a parameter to be optimised, the optimal solutions required sparse network connectivity. We also found that for excitatory neural firing rates lower than that of inhibitory neurons, the errors were generally larger. Overall, we have successfully demonstrated the feasibility of implementing multi-objective GA optimisation on network parameters of recurrent and sparse SNN. </details>
<details>	<summary>注释</summary>	In: 32nd Irish Signals and Systems Conference (ISSC) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 121、基于基数编码的高效脉冲神经网络
- [ ] Efficient Spiking Neural Networks with Radix Encoding 
时间：2021年05月14日                         第一作者：Zhehui Wang                       [链接](https://arxiv.org/abs/2105.06943).                     
## 摘要：与传统的人工神经网络相比，脉冲神经网络（SNNs）由于其事件驱动的计算机制和用加法代替能耗加权乘法，在延迟和能量效率方面具有优势。然而，为了达到神经网络的精度，通常需要长脉冲序列来保证精度。传统上，一个脉冲序列需要大约一千个时间步才能达到与人工神经网络相似的精度。这抵消了snn带来的计算效率，因为更长的脉冲序列意味着更多的操作和更长的延迟。本文提出了一种具有超短脉冲序列的基数编码SNN。在新模型中，脉冲列车只需不到10个时间步。实验结果表明，与VGG-16网络体系结构和CIFAR-10数据集的最新研究成果相比，该方法具有25倍的加速比和1.1%的精度提高。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have advantages in latency and energy efficiency over traditional artificial neural networks (ANNs) due to its event-driven computation mechanism and replacement of energy-consuming weight multiplications with additions. However, in order to reach accuracy of its ANN counterpart, it usually requires long spike trains to ensure the accuracy. Traditionally, a spike train needs around one thousand time steps to approach similar accuracy as its ANN counterpart. This offsets the computation efficiency brought by SNNs because longer spike trains mean a larger number of operations and longer latency. In this paper, we propose a radix encoded SNN with ultra-short spike trains. In the new model, the spike train takes less than ten time steps. Experiments show that our method demonstrates 25X speedup and 1.1% increment on accuracy, compared with the state-of-the-art work on VGG-16 network architecture and CIFAR-10 dataset. </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 120、SpikeMS：用于运动分割的深脉冲神经网络
- [ ] SpikeMS: Deep Spiking Neural Network for Motion Segmentation 
时间：2021年05月13日                         第一作者：Chethan M. Parameshwara                       [链接](https://arxiv.org/abs/2105.06562).                     
## 摘要：脉冲神经网络（SNN）是所谓的第三代神经网络，它试图更紧密地匹配生物大脑的功能。它们固有地对时间数据进行编码，允许以较少的能量使用进行训练，并且当在神经形态硬件上编码时，可以非常节能。此外，它们非常适合于涉及基于事件的传感器的任务，这与SNN基于事件的特性相匹配。然而，由于算法和训练的复杂性，snn还没有像标准人工神经网络（ANNs）那样有效地应用于实际的大规模任务中。为了使情况进一步恶化，输入表示是非常规的，需要仔细分析和深入理解。在本文中，我们提出了第一个深度编码-解码器SNN体系结构\textit{SpikeMS}，用于解决以基于事件的DVS摄像机为输入的大规模运动分割问题。为了实现这一点，我们引入了一种新的时空损失公式，它包括脉冲计数和分类标签，并结合使用新的SNN反向传播技术。此外，我们还证明了\textit{SpikeMS}能够进行\textit{incremental predictions}，或者从比训练数据量更小的测试数据中进行预测。这对于为低延迟应用程序和需要快速预测的应用程序提供部分输入数据的输出是非常宝贵的。我们对来自EV-IMO、EED和MOD数据集的具有挑战性的合成和真实世界序列进行了评估，并取得了与可比较的ANN方法相当的结果，但使用的功率可能减少了50倍。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) are the so-called third generation of neural networks which attempt to more closely match the functioning of the biological brain. They inherently encode temporal data, allowing for training with less energy usage and can be extremely energy efficient when coded on neuromorphic hardware. In addition, they are well suited for tasks involving event-based sensors, which match the event-based nature of the SNN. However, SNNs have not been as effectively applied to real-world, large-scale tasks as standard Artificial Neural Networks (ANNs) due to the algorithmic and training complexity. To exacerbate the situation further, the input representation is unconventional and requires careful analysis and deep understanding. In this paper, we propose \textit{SpikeMS}, the first deep encoder-decoder SNN architecture for the real-world large-scale problem of motion segmentation using the event-based DVS camera as input. To accomplish this, we introduce a novel spatio-temporal loss formulation that includes both spike counts and classification labels in conjunction with the use of new techniques for SNN backpropagation. In addition, we show that \textit{SpikeMS} is capable of \textit{incremental predictions}, or predictions from smaller amounts of test data than it is trained on. This is invaluable for providing outputs even with partial input data for low-latency applications and those requiring fast predictions. We evaluated \textit{SpikeMS} on challenging synthetic and real-world sequences from EV-IMO, EED and MOD datasets and achieving results on a par with a comparable ANN method, but using potentially 50 times less power. </details>
<details>	<summary>注释</summary>	7 pages, 6 figures, 3 tables, Under review IROS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 119、基于深度连续局部学习的深脉冲卷积神经网络单目标定位
- [ ] Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning 
时间：2021年05月12日                         第一作者：Sami Barchid                       [链接](https://arxiv.org/abs/2105.05609).                     
## 摘要：随着神经形态硬件的出现，脉冲神经网络可以成为人工神经网络的一个很好的节能替代品。然而，使用脉冲神经网络来执行计算机视觉任务仍然是有限的，主要集中在简单的任务，如数字识别。由于针对这些任务的深脉冲神经网络的研究较少，因此很难处理更复杂的任务（如分割、目标检测）。本论文的目的是利用监督脉冲神经网络向现代计算机视觉迈出第一步。提出了一种用于灰度图像中单个目标定位的深度卷积脉冲神经网络。我们提出了一种基于decole的网络，decole是一种基于局部代理梯度学习的脉冲模型。Oxford IIIT Pet上报告的令人鼓舞的结果验证了脉冲神经网络在未来更精细的视觉任务中的应用。
<details>	<summary>英文摘要</summary>	With the advent of neuromorphic hardware, spiking neural networks can be a good energy-efficient alternative to artificial neural networks. However, the use of spiking neural networks to perform computer vision tasks remains limited, mainly focusing on simple tasks such as digit recognition. It remains hard to deal with more complex tasks (e.g. segmentation, object detection) due to the small number of works on deep spiking neural networks for these tasks. The objective of this paper is to make the first step towards modern computer vision with supervised spiking neural networks. We propose a deep convolutional spiking neural network for the localization of a single object in a grayscale image. We propose a network based on DECOLLE, a spiking model that enables local surrogate gradient-based learning. The encouraging results reported on Oxford-IIIT-Pet validates the exploitation of spiking neural networks with a supervised learning approach for more elaborate vision tasks in the future. </details>
<details>	<summary>邮件日期</summary>	2021年05月13日</details>

# 118、深度脉冲神经网络的梯度重排剪枝
- [ ] Pruning of Deep Spiking Neural Networks through Gradient Rewiring 
时间：2021年05月11日                         第一作者：Yanqi Chen                       [链接](https://arxiv.org/abs/2105.04916).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）因其在神经形态芯片上的生物合理性和高能量利用率而备受关注。由于这些芯片通常是资源受限的，因此snn的压缩在snn的实际应用中至关重要。现有的方法大多直接将人工神经网络中的剪枝方法应用于snn，忽略了ann与snn的区别，从而限制了被剪枝snn的性能。此外，这些方法只适用于浅层snn。本文受神经系统中突触形成和突触消除的启发，提出了一种基于连通性和权值的SNNs联合学习算法gradr（gradr），使我们能够在不受再训练的情况下无缝地优化网络结构。我们的关键创新是重新定义梯度到一个新的突触参数，允许通过充分利用剪枝和连接再生之间的竞争来更好地探索网络结构。实验结果表明，该方法在MNIST和CIFAR-10数据集上实现了最小的SNNs性能损失。此外，在前所未有的0.73%连通度下，其精度损失达到3.5%，显示了SNNs显著的结构细化能力。我们的工作表明，在深度snn中存在着极高的冗余度。我们的代码可从\url获得{https://github.com/Yanqi-Chen/Gradient-Rewiring}.
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have been attached great importance due to their biological plausibility and high energy-efficiency on neuromorphic chips. As these chips are usually resource-constrained, the compression of SNNs is thus crucial along the road of practical use of SNNs. Most existing methods directly apply pruning approaches in artificial neural networks (ANNs) to SNNs, which ignore the difference between ANNs and SNNs, thus limiting the performance of the pruned SNNs. Besides, these methods are only suitable for shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination in the neural system, we propose gradient rewiring (Grad R), a joint learning algorithm of connectivity and weight for SNNs, that enables us to seamlessly optimize network structure without retrain. Our key innovation is to redefine the gradient to a new synaptic parameter, allowing better exploration of network structures by taking full advantage of the competition between pruning and regrowth of connections. The experimental results show that the proposed method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented 0.73% connectivity, which reveals remarkable structure refining capability in SNNs. Our work suggests that there exists extremely high redundancy in deep SNNs. Our codes are available at \url{https://github.com/Yanqi-Chen/Gradient-Rewiring}. </details>
<details>	<summary>注释</summary>	9 pages,7 figures. Accepted by IJCAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月12日</details>

# 117、神经形态处理器上多层脉冲神经网络的硬件学习
- [ ] In-Hardware Learning of Multilayer Spiking Neural Networks on a Neuromorphic Processor 
时间：2021年05月08日                         第一作者：Amar Shrestha                       [链接](https://arxiv.org/abs/2105.03649).                     
## 摘要：尽管反向传播在机器学习中有着广泛的应用，但它不能直接应用于SNN训练，在模拟生物神经元和突触的神经形态处理器上也不可行。本文提出了一种具有生物似然局部更新规则的基于脉冲的反向传播算法，并对其进行了改进以适应神经形态硬件中的约束条件。该算法在intelloihi芯片上实现，实现了移动应用中多层snn的低功耗硬件监督在线学习。我们在MNIST、Fashion MNIST、CIFAR-10和MSTAR数据集上测试了这个实现，并展示了用这个实现进行增量在线学习的可能性。
<details>	<summary>英文摘要</summary>	Although widely used in machine learning, backpropagation cannot directly be applied to SNN training and is not feasible on a neuromorphic processor that emulates biological neuron and synapses. This work presents a spike-based backpropagation algorithm with biological plausible local update rules and adapts it to fit the constraint in a neuromorphic hardware. The algorithm is implemented on Intel Loihi chip enabling low power in-hardware supervised online learning of multilayered SNNs for mobile applications. We test this implementation on MNIST, Fashion-MNIST, CIFAR-10 and MSTAR datasets with promising performance and energy-efficiency, and demonstrate a possibility of incremental online learning with the implementation. </details>
<details>	<summary>注释</summary>	6 pages, 5 figures, accepted for Design Automation Conference (DAC) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 116、Izhikevich激发的具有兴奋和抑制输入的光电神经元用于高效能光子脉冲神经网络
- [ ] Izhikevich-Inspired Optoelectronic Neurons with Excitatory and Inhibitory Inputs for Energy-Efficient Photonic Spiking Neural Networks 
时间：2021年05月03日                         第一作者：Yun-jhu Lee                       [链接](https://arxiv.org/abs/2105.02809).                     
## 摘要：据我们所知，我们第一次设计、原型化和实验演示了一个受Izhikevich模型启发的光电脉冲神经元，该模型结合了兴奋性和抑制性光脉冲输入，并相应地产生光脉冲输出。光电神经元由三个晶体管组成，作为电脉冲电路，一个垂直腔面发射激光器（VCSEL）用于光脉冲输出，两个光电探测器用于激发和抑制光脉冲输入。附加的电容器和电阻完成了伊日克维奇启发的光电神经元，接收兴奋性和抑制性光脉冲作为其他光电神经元的输入。我们在Verilog-a中建立了一个详细的光电神经元模型，并模拟了各种情况下具有兴奋性输入和抑制性输入信号的电路级操作。实验结果与模拟结果非常相似，证明了兴奋性输入是如何触发光脉冲输出的，而抑制性输入是如何抑制光脉冲输出的。利用模拟的神经元模型，我们使用完全连接（FC）和卷积神经网络（CNN）进行了模拟。用MNIST手写体数字识别的仿真结果表明，无监督学习的识别率为90%，改进的有监督FC神经网络的识别率为97%。我们进一步设计了一个利用量子阻抗转换的纳米级光电神经元，其中200aj/脉冲输入可以触发具有10fj/脉冲的片上纳米激光器的输出。纳米级神经元在神经网络中以10 GSpikes/s的速度运行时，可以支持约80的扇出或克服19 dB的多余光损耗，与最先进的电子神经形态硬件（如Loihi和NeuroGrid）相比，这相当于100倍的吞吐量和1000倍的能效提高。
<details>	<summary>英文摘要</summary>	We designed, prototyped, and experimentally demonstrated, for the first time to our knowledge, an optoelectronic spiking neuron inspired by the Izhikevich model incorporating both excitatory and inhibitory optical spiking inputs and producing optical spiking outputs accordingly. The optoelectronic neurons consist of three transistors acting as electrical spiking circuits, a vertical-cavity surface-emitting laser (VCSEL) for optical spiking outputs, and two photodetectors for excitatory and inhibitory optical spiking inputs. Additional inclusion of capacitors and resistors complete the Izhikevich-inspired optoelectronic neurons, which receive excitatory and inhibitory optical spikes as inputs from other optoelectronic neurons. We developed a detailed optoelectronic neuron model in Verilog-A and simulated the circuit-level operation of various cases with excitatory input and inhibitory input signals. The experimental results closely resemble the simulated results and demonstrate how the excitatory inputs trigger the optical spiking outputs while the inhibitory inputs suppress the outputs. Utilizing the simulated neuron model, we conducted simulations using fully connected (FC) and convolutional neural networks (CNN). The simulation results using MNIST handwritten digits recognition show 90% accuracy on unsupervised learning and 97% accuracy on a supervised modified FC neural network. We further designed a nanoscale optoelectronic neuron utilizing quantum impedance conversion where a 200 aJ/spike input can trigger the output from on-chip nanolasers with 10 fJ/spike. The nanoscale neuron can support a fanout of ~80 or overcome 19 dB excess optical loss while running at 10 GSpikes/second in the neural network, which corresponds to 100x throughput and 1000x energy-efficiency improvement compared to state-of-art electrical neuromorphic hardware such as Loihi and NeuroGrid. </details>
<details>	<summary>注释</summary>	24 pages, 13 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月07日</details>

# 115、神经形态计算中的动态可靠性管理
- [ ] Dynamic Reliability Management in Neuromorphic Computing 
时间：2021年05月05日                         第一作者：Shihao Song                       [链接](https://arxiv.org/abs/2105.02038).                     
## 摘要：神经形态计算系统利用非易失性存储器（NVM）实现高密度、低能的突触存储。操作nvm所需的电压和电流升高会导致每个神经元中基于CMOS的晶体管和硬件中的突触电路老化，使晶体管的参数偏离其标称值。激进的设备缩放增加功率密度和温度，加速老化，挑战神经形态系统的可靠运行。现有的面向可靠性的技术周期性地以固定的时间间隔消除硬件中所有神经元和突触电路的压力，假设最坏的运行条件，而不实际跟踪它们在运行时的老化情况。为了消除这些电路的应力，必须中断正常操作，这会在脉冲生成和传播中引入延迟，影响脉冲间隔，从而影响性能，例如精度。我们提出了一种新的架构技术，通过设计一个智能运行时管理器（NCRTM）来缓解神经形态系统中与老化相关的可靠性问题，该管理器在执行机器学习工作负载的过程中，针对CMOS晶体管的短期老化，动态地降低神经元和突触电路的压力，以达到可靠性目标。NCRTM仅在绝对必要时才对这些电路进行去应力处理，否则，通过将去应力操作安排在关键路径之外来降低性能影响。我们评估NCRTM与国家的最先进的机器学习工作负荷的神经形态硬件。我们的结果表明，NCRTM显著提高了神经形态硬件的可靠性，对性能的影响很小。
<details>	<summary>英文摘要</summary>	Neuromorphic computing systems uses non-volatile memory (NVM) to implement high-density and low-energy synaptic storage. Elevated voltages and currents needed to operate NVMs cause aging of CMOS-based transistors in each neuron and synapse circuit in the hardware, drifting the transistor's parameters from their nominal values. Aggressive device scaling increases power density and temperature, which accelerates the aging, challenging the reliable operation of neuromorphic systems. Existing reliability-oriented techniques periodically de-stress all neuron and synapse circuits in the hardware at fixed intervals, assuming worst-case operating conditions, without actually tracking their aging at run time. To de-stress these circuits, normal operation must be interrupted, which introduces latency in spike generation and propagation, impacting the inter-spike interval and hence, performance, e.g., accuracy. We propose a new architectural technique to mitigate the aging-related reliability problems in neuromorphic systems, by designing an intelligent run-time manager (NCRTM), which dynamically destresses neuron and synapse circuits in response to the short-term aging in their CMOS transistors during the execution of machine learning workloads, with the objective of meeting a reliability target. NCRTM de-stresses these circuits only when it is absolutely necessary to do so, otherwise reducing the performance impact by scheduling de-stress operations off the critical path. We evaluate NCRTM with state-of-the-art machine learning workloads on a neuromorphic hardware. Our results demonstrate that NCRTM significantly improves the reliability of neuromorphic hardware, with marginal impact on performance. </details>
<details>	<summary>注释</summary>	Accepted in ACM JETC </details>
<details>	<summary>邮件日期</summary>	2021年05月06日</details>

# 114、neuroxplorer1.0：一个带脉冲神经网络的可扩展架构探索框架
- [ ] NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration with Spiking Neural Networks 
时间：2021年05月04日                         第一作者：Adarsha Balaji                        [链接](https://arxiv.org/abs/2105.01795).                     
## 摘要：最近，工业界和学术界都提出了许多不同的神经形态结构来执行用脉冲神经网络（SNN）设计的应用程序。因此，越来越需要一个可扩展的仿真框架来进行SNNs的架构探索，包括当今硬件的基于平台的设计，以及未来的软硬件协同设计和设计技术协同优化。我们介绍了NeuroXplorer，这是一个快速且可扩展的框架，它基于一个通用的模板来建模一个神经形态的架构，该架构可以注入给定硬件和/或技术的特定细节。NeuroXplorer可以执行低级别的周期精确架构模拟和数据流抽象的高级分析。NeuroXplorer的优化引擎可以结合面向硬件的指标，如能量、吞吐量和延迟，以及面向SNN的指标，如脉冲间隔失真和脉冲紊乱，这些指标直接影响SNN性能。我们通过许多最先进的机器学习模型的案例研究来展示NeuroXplorer的架构探索能力。
<details>	<summary>英文摘要</summary>	Recently, both industry and academia have proposed many different neuromorphic architectures to execute applications that are designed with Spiking Neural Network (SNN). Consequently, there is a growing need for an extensible simulation framework that can perform architectural explorations with SNNs, including both platform-based design of today's hardware, and hardware-software co-design and design-technology co-optimization of the future. We present NeuroXplorer, a fast and extensible framework that is based on a generalized template for modeling a neuromorphic architecture that can be infused with the specific details of a given hardware and/or technology. NeuroXplorer can perform both low-level cycle-accurate architectural simulations and high-level analysis with data-flow abstractions. NeuroXplorer's optimization engine can incorporate hardware-oriented metrics such as energy, throughput, and latency, as well as SNN-oriented metrics such as inter-spike interval distortion and spike disorder, which directly impact SNN performance. We demonstrate the architectural exploration capabilities of NeuroXplorer through case studies with many state-of-the-art machine learning models. </details>
<details>	<summary>邮件日期</summary>	2021年05月06日</details>

# 113、基于层次图像分割和关系预测的医院和实验室液体样品计算机视觉
- [ ] Computer vision for liquid samples in hospitals and medical labs using hierarchical image segmentation and relations prediction 
时间：2021年05月04日                         第一作者：Sagi Eppel                       [链接](https://arxiv.org/abs/2105.01456).                     
## 摘要：这项工作探讨了如何利用计算机视觉对透明容器（如试管、注射器、输液袋）中的医用液体样本进行图像分割和分类。处理输液、血液和尿样等液体是医学实验室和医院工作的重要组成部分。从图像中准确识别和分割液体和盛装液体的容器的能力有助于实现这些过程的自动化。现代计算机视觉通常涉及到在大量的带注释图像数据集上训练深层神经网络。这项工作提出了一个新的数据集，其中包含1300个医学样本的注释图像，包括含有液体和固体物质的血管。图像标注有液体类型（如血液、尿液）、材料相（如液体、固体、泡沫、悬浮液）、血管类型（如注射器、管子、杯子、输液瓶/袋）和血管属性（透明、不透明）。此外，容器零件（如软木塞、标签、钉子和阀门）也会进行注释。容器和材料之间的关系和层次结构也会被注释，例如哪个容器包含哪个材料，或者哪个容器相互链接或包含。在数据集上训练三个神经网络：一个网络学习检测血管，第二个网络检测每个血管内的材料和零件，第三个网络识别血管之间的关系和连通性。
<details>	<summary>英文摘要</summary>	This work explores the use of computer vision for image segmentation and classification of medical fluid samples in transparent containers (for example, tubes, syringes, infusion bags). Handling fluids such as infusion fluids, blood, and urine samples is a significant part of the work carried out in medical labs and hospitals. The ability to accurately identify and segment the liquids and the vessels that contain them from images can help in automating such processes. Modern computer vision typically involves training deep neural nets on large datasets of annotated images. This work presents a new dataset containing 1,300 annotated images of medical samples involving vessels containing liquids and solid material. The images are annotated with the type of liquid (e.g., blood, urine), the phase of the material (e.g., liquid, solid, foam, suspension), the type of vessel (e.g., syringe, tube, cup, infusion bottle/bag), and the properties of the vessel (transparent, opaque). In addition, vessel parts such as corks, labels, spikes, and valves are annotated. Relations and hierarchies between vessels and materials are also annotated, such as which vessel contains which material or which vessels are linked or contain each other. Three neural networks are trained on the dataset: One network learns to detect vessels, a second net detects the materials and parts inside each vessel, and a third net identifies relationships and connectivity between vessels. </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 112、在神经形态处理器Loihi上利用脉冲神经网络实现资源受限导航
- [ ] Simplified Klinokinesis using Spiking Neural Networks for Resource-Constrained Navigation on the Neuromorphic Processor Loihi 
时间：2021年05月04日                         第一作者：Apoorv Kishore                       [链接](https://arxiv.org/abs/2105.01358).                     
## 摘要：C。线虫通过Klingokinesis显示趋化性，蠕虫基于单个浓度传感器感知浓度，计算浓度梯度，通过梯度上升/下降到目标浓度，然后进行轮廓跟踪，从而进行觅食。仿生的实现需要具有多离子通道动力学的复杂神经元以及中间神经元来控制。虽然这是自主机器人的一项关键功能，但在英特尔的Loihi等节能神经形态硬件上实现这一功能需要对网络进行自适应，以适应特定于硬件的约束，但这一点尚未实现。在本文中，我们证明了基于klinokinesis的趋化性对Loihi的适应性，通过仅用LIF神经元实现必要的神经元动力学以及所有功能（如Heaviside函数和减法）的完全基于脉冲的实现。我们的结果表明，Loihi实现在性能方面与Python上的软件相当——无论是在觅食还是轮廓跟踪过程中。Loihi结果在噪声环境中也具有弹性。因此，我们证明了Loihi上趋化性的成功适应-现在可以结合丰富的SNN块阵列用于基于SNN的复杂机器人控制。
<details>	<summary>英文摘要</summary>	C. elegans shows chemotaxis using klinokinesis where the worm senses the concentration based on a single concentration sensor to compute the concentration gradient to perform foraging through gradient ascent/descent towards the target concentration followed by contour tracking. The biomimetic implementation requires complex neurons with multiple ion channel dynamics as well as interneurons for control. While this is a key capability of autonomous robots, its implementation on energy-efficient neuromorphic hardware like Intel's Loihi requires adaptation of the network to hardware-specific constraints, which has not been achieved. In this paper, we demonstrate the adaptation of chemotaxis based on klinokinesis to Loihi by implementing necessary neuronal dynamics with only LIF neurons as well as a complete spike-based implementation of all functions e.g. Heaviside function and subtractions. Our results show that Loihi implementation is equivalent to the software counterpart on Python in terms of performance - both during foraging and contour tracking. The Loihi results are also resilient in noisy environments. Thus, we demonstrate a successful adaptation of chemotaxis on Loihi - which can now be combined with the rich array of SNN blocks for SNN based complex robotic control. </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 111、光谱机器学习在胰腺肿块影像分类中的应用
- [ ] Spectral Machine Learning for Pancreatic Mass Imaging Classification 
时间：2021年05月03日                         第一作者：Yiming Liu                       [链接](https://arxiv.org/abs/2105.00728).                     
## 摘要：我们提出了一种新的光谱机器学习（SML）方法用于胰腺肿块的CT筛查。我们的算法是基于公共数据源，用250名患者（50名胰腺正常患者和200名胰腺异常患者）的大约30000张图像进行训练的。样本外诊断分类的准确率为94.6%，基于113例患者共约15000张图像，其中32例胰腺正常患者和81例胰腺异常患者中有26例得到正确诊断。SML能够在诊断分类中自动选择基本图像（平均每个病人5或9张图像），达到上述精度。在一台具有标准CPU运行环境的笔记本电脑上，诊断113例患者的计算时间为75秒。影响光谱学习与机器学习结合的因素包括：1）在分类训练中，利用样本协方差矩阵的几个最大特征值对应的特征向量（脉冲特征向量）来选择输入属性，只考虑原始图像的基本信息，噪声较小；2） 基于平均水平谱检验去除无关像素，在保持较高分类精度的同时，降低了对存储容量的挑战，提高了计算效率；3） 采用最先进的机器学习分类、梯度提升和随机森林。我们的方法展示了在人工智能时代胰腺肿块筛查的实用性和提高的图像诊断准确率。
<details>	<summary>英文摘要</summary>	We present a novel spectral machine learning (SML) method in screening for pancreatic mass using CT imaging. Our algorithm is trained with approximately 30,000 images from 250 patients (50 patients with normal pancreas and 200 patients with abnormal pancreas findings) based on public data sources. A test accuracy of 94.6 percents was achieved in the out-of-sample diagnosis classification based on a total of approximately 15,000 images from 113 patients, whereby 26 out of 32 patients with normal pancreas and all 81 patients with abnormal pancreas findings were correctly diagnosed. SML is able to automatically choose fundamental images (on average 5 or 9 images for each patient) in the diagnosis classification and achieve the above mentioned accuracy. The computational time is 75 seconds for diagnosing 113 patients in a laptop with standard CPU running environment. Factors that influenced high performance of a well-designed integration of spectral learning and machine learning included: 1) use of eigenvectors corresponding to several of the largest eigenvalues of sample covariance matrix (spike eigenvectors) to choose input attributes in classification training, taking into account only the fundamental information of the raw images with less noise; 2) removal of irrelevant pixels based on mean-level spectral test to lower the challenges of memory capacity and enhance computational efficiency while maintaining superior classification accuracy; 3) adoption of state-of-the-art machine learning classification, gradient boosting and random forest. Our methodology showcases practical utility and improved accuracy of image diagnosis in pancreatic mass screening in the era of AI. </details>
<details>	<summary>注释</summary>	17 pages, 3 figures MSC-class: 62P10 </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 110、Neko：探索神经形态学习规则的图书馆
- [ ] Neko: a Library for Exploring Neuromorphic Learning Rules 
时间：2021年05月01日                         第一作者：Zixuan Zhao                       [链接](https://arxiv.org/abs/2105.00324).                     
## 摘要：神经形态计算领域正处于一个积极探索的时期。虽然已经开发了许多工具来模拟神经元动力学或将深层网络转换为脉冲模型，但用于学习规则的通用软件库仍然没有得到充分的探索。这在一定程度上是由于设计新的学习规则的多样性和挑战性，从编码方法到梯度近似，从模仿贝叶斯大脑的群体方法到部署在忆阻器横杆上的约束学习算法。为了解决这一差距，我们提出了Neko，一个模块化的，可扩展的图书馆，重点是帮助设计新的学习算法。我们在三个例子中展示了Neko的实用性：在线局部学习、概率学习和模拟设备学习。我们的结果表明，Neko可以复制最先进的算法，并且在一种情况下，在精度和速度上有显著的优势。此外，它还提供了包括梯度比较在内的工具，可以帮助开发新的算法变体。Neko是一个开源的Python库，支持PyTorch和TensorFlow后端。
<details>	<summary>英文摘要</summary>	The field of neuromorphic computing is in a period of active exploration. While many tools have been developed to simulate neuronal dynamics or convert deep networks to spiking models, general software libraries for learning rules remain underexplored. This is partly due to the diverse, challenging nature of efforts to design new learning rules, which range from encoding methods to gradient approximations, from population approaches that mimic the Bayesian brain to constrained learning algorithms deployed on memristor crossbars. To address this gap, we present Neko, a modular, extensible library with a focus on aiding the design of new learning algorithms. We demonstrate the utility of Neko in three exemplar cases: online local learning, probabilistic learning, and analog on-device learning. Our results show that Neko can replicate the state-of-the-art algorithms and, in one case, lead to significant outperformance in accuracy and speed. Further, it offers tools including gradient comparison that can help develop new algorithmic variants. Neko is an open source Python library that supports PyTorch and TensorFlow backends. </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 109、一种新的脉冲神经网络近似汉明权值计算方法：一种FPGA友好结构
- [ ] A Novel Approximate Hamming Weight Computing for Spiking Neural Networks: an FPGA Friendly Architecture 
时间：2021年04月29日                         第一作者：Kaveh Akbarzadeh-Sherbaf                       [链接](https://arxiv.org/abs/2104.14594).                     
## 摘要：稀疏长二值向量的Hamming权值是许多科学应用中的重要模块，特别是在我们感兴趣的脉冲神经网络中。为了提高FPGA实现的面积和延迟，我们从突触传输失败的角度出发，提出了一种利用FPGA查找表压缩长输入向量的方法。为了评估这种方法的有效性，我们使用一个简单的线性加法器来计算压缩向量的'1'个数。我们将压缩器分为具有两级以上查找表的浅压缩器和具有两级以上查找表的深压缩器。该方法生成的体系结构显示，对于不同配置的浅层压缩器，面积和延迟分别减少了82%和35%。此外，我们的模拟结果显示，仅使用深压缩器计算一个脉冲神经网络1024位向量的汉明权值，保留了网络的混沌行为，但对学习性能影响不大。
<details>	<summary>英文摘要</summary>	Hamming weights of sparse and long binary vectors are important modules in many scientific applications, particularly in spiking neural networks that are of our interest. To improve both area and latency of their FPGA implementations, we propose a method inspired from synaptic transmission failure for exploiting FPGA lookup tables to compress long input vectors. To evaluate the effectiveness of this approach, we count the number of `1's of the compressed vector using a simple linear adder. We classify the compressors into shallow ones with up to two levels of lookup tables and deep ones with more than two levels. The architecture generated by this approach shows up to 82% and 35% reductions for different configurations of shallow compressors in area and latency respectively. Moreover, our simulation results show that calculating the Hamming weight of a 1024-bit vector of a spiking neural network by the use of only deep compressors preserves the chaotic behavior of the network while slightly impacts on the learning performance. </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 108、Hessian感知的脉冲神经网络量化
- [ ] Hessian Aware Quantization of Spiking Neural Networks 
时间：2021年04月29日                         第一作者：Hin Wai Lui                        [链接](https://arxiv.org/abs/2104.14117).                     
## 摘要：为了实现脉冲神经网络（SNNs）的低延迟、高吞吐量和能量效率优势，减少在神经形态硬件上运行时的内存和计算需求是一个重要的步骤。神经形态结构允许大规模并行计算，具有可变和局部位精度。然而，如何将不同的比特精度分配给网络的不同层或连接并非易事。在这项工作中，我们演示了分层Hessian跟踪分析如何测量损耗对层权重的任何扰动的敏感性，这可以用来指导量化SNN时特定于层的比特精度的分配。另外，目前基于梯度的SNN训练方法采用的是多状态变量的复杂神经元模型，计算效率和记忆效率都不理想。为了应对这一挑战，我们提出了一个简化的神经元模型，该模型将状态变量的数量减少了4倍，同时仍然兼容基于梯度的训练。我们发现，使用逐层位精度时对模型精度的影响与该层的Hessian轨迹密切相关。优化后的量化网络精度仅下降了0.2%，但网络规模却减小了58%。这减少了内存使用，并允许使用具有更简单数字电路的定点算法，从而提高了总体吞吐量和能源效率。
<details>	<summary>英文摘要</summary>	To achieve the low latency, high throughput, and energy efficiency benefits of Spiking Neural Networks (SNNs), reducing the memory and compute requirements when running on a neuromorphic hardware is an important step. Neuromorphic architecture allows massively parallel computation with variable and local bit-precisions. However, how different bit-precisions should be allocated to different layers or connections of the network is not trivial. In this work, we demonstrate how a layer-wise Hessian trace analysis can measure the sensitivity of the loss to any perturbation of the layer's weights, and this can be used to guide the allocation of a layer-specific bit-precision when quantizing an SNN. In addition, current gradient based methods of SNN training use a complex neuron model with multiple state variables, which is not ideal for compute and memory efficiency. To address this challenge, we present a simplified neuron model that reduces the number of state variables by 4-fold while still being compatible with gradient based training. We find that the impact on model accuracy when using a layer-wise bit-precision correlated well with that layer's Hessian trace. The accuracy of the optimal quantized network only dropped by 0.2%, yet the network size was reduced by 58%. This reduces memory usage and allows fixed-point arithmetic with simpler digital circuits to be used, increasing the overall throughput and energy efficiency. </details>
<details>	<summary>邮件日期</summary>	2021年04月30日</details>

# 107、用于语音分类的液态机中硬件友好的突触顺序和时间标度
- [ ] Hardware-Friendly Synaptic Orders and Timescales in Liquid State Machines for Speech Classification 
时间：2021年04月29日                         第一作者：Vivek Saraswat                       [链接](https://arxiv.org/abs/2104.14264).                     
## 摘要：液体状态机是一种受大脑启发的脉冲神经网络（SNNs），具有随机存储连接和仿生神经元和突触模型。为了解决时间分类问题，提出了储层计算网络作为深度神经网络的替代方法。以往的研究表明，二阶（双指数）突触波形是实现高精度的TI-46语音数字识别的关键。长时间范围（ms）仿生突触波形的提出是对紧凑且节能的神经形态硬件的挑战。在这项工作中，我们分析了突触顺序的作用，即：{\delta}（单时间步的高输出），0th（有限脉冲宽度的矩形），一阶（指数下降）和二阶（指数上升和下降）以及突触时间尺度对水库输出响应和更全面参数扫描下TI-46语音数字分类精度的影响。我们发现最佳操作点与油藏中的最佳峰值活动范围相关。此外，提出的第0级突触的表现与生物学上合理的第2级突触相当。这对电路设计者来说是一个很大的放松，因为突触是SNN内存中实现中最丰富的组件。强调了0阶synapse模拟和混合信号实现的电路优势，通过消除运算放大器和数模转换器电路，在面积和功耗方面节省了2-3个数量级。这对一个完整的神经网络实现有着重要的影响，重点在于外围限制和克服这些限制的算法简化。
<details>	<summary>英文摘要</summary>	Liquid State Machines are brain inspired spiking neural networks (SNNs) with random reservoir connectivity and bio-mimetic neuronal and synaptic models. Reservoir computing networks are proposed as an alternative to deep neural networks to solve temporal classification problems. Previous studies suggest 2nd order (double exponential) synaptic waveform to be crucial for achieving high accuracy for TI-46 spoken digits recognition. The proposal of long-time range (ms) bio-mimetic synaptic waveforms is a challenge to compact and power efficient neuromorphic hardware. In this work, we analyze the role of synaptic orders namely: {\delta} (high output for single time step), 0th (rectangular with a finite pulse width), 1st (exponential fall) and 2nd order (exponential rise and fall) and synaptic timescales on the reservoir output response and on the TI-46 spoken digits classification accuracy under a more comprehensive parameter sweep. We find the optimal operating point to be correlated to an optimal range of spiking activity in the reservoir. Further, the proposed 0th order synapses perform at par with the biologically plausible 2nd order synapses. This is substantial relaxation for circuit designers as synapses are the most abundant components in an in-memory implementation for SNNs. The circuit benefits for both analog and mixed-signal realizations of 0th order synapse are highlighted demonstrating 2-3 orders of savings in area and power consumptions by eliminating Op-Amps and Digital to Analog Converter circuits. This has major implications on a complete neural network implementation with focus on peripheral limitations and algorithmic simplifications to overcome them. </details>
<details>	<summary>邮件日期</summary>	2021年04月30日</details>

# 106、神经形态计算是图灵完备的
- [ ] Neuromorphic Computing is Turing-Complete 
时间：2021年04月28日                         第一作者：Prasanna Date                       [链接](https://arxiv.org/abs/2104.13983).                     
## 摘要：神经形态计算是一种非冯诺依曼计算范式，通过模拟人脑来执行计算。神经形态的系统是非常节能的，并且已知消耗的能量比CPU和GPU少数千倍。它们有可能在未来推动诸如自动车辆、边缘计算和物联网等关键用例。出于这个原因，它们被寻求成为未来计算领域不可或缺的一部分。神经形态系统主要用于基于脉冲的机器学习应用，尽管在图论、微分方程和基于脉冲的仿真中也有一些非机器学习的应用。这些应用表明，神经形态计算可能是通用计算能力。然而，神经形态计算的通用可计算性尚未建立。在这项工作中，我们证明了神经形态计算是图灵完备的，因此能够进行通用计算。具体来说，我们提出了一个神经形态计算模型，只有两个神经元参数（阈值和泄漏）和两个突触参数（权重和延迟）。我们设计了用于计算所有{mu}递归函数（即常数、后继函数和投影函数）和所有{mu}递归算子（即合成算子、本原递归算子和极小化算子）的神经形态电路。鉴于{\mu}-递归函数和算子正是可以用图灵机计算的函数和算子，本文建立了神经形态计算的图灵完备性。
<details>	<summary>英文摘要</summary>	Neuromorphic computing is a non-von Neumann computing paradigm that performs computation by emulating the human brain. Neuromorphic systems are extremely energy-efficient and known to consume thousands of times less power than CPUs and GPUs. They have the potential to drive critical use cases such as autonomous vehicles, edge computing and internet of things in the future. For this reason, they are sought to be an indispensable part of the future computing landscape. Neuromorphic systems are mainly used for spike-based machine learning applications, although there are some non-machine learning applications in graph theory, differential equations, and spike-based simulations. These applications suggest that neuromorphic computing might be capable of general-purpose computing. However, general-purpose computability of neuromorphic computing has not been established yet. In this work, we prove that neuromorphic computing is Turing-complete and therefore capable of general-purpose computing. Specifically, we present a model of neuromorphic computing, with just two neuron parameters (threshold and leak), and two synaptic parameters (weight and delay). We devise neuromorphic circuits for computing all the {\mu}-recursive functions (i.e., constant, successor and projection functions) and all the {\mu}-recursive operators (i.e., composition, primitive recursion and minimization operators). Given that the {\mu}-recursive functions and operators are precisely the ones that can be computed using a Turing machine, this work establishes the Turing-completeness of neuromorphic computing. </details>
<details>	<summary>邮件日期</summary>	2021年04月30日</details>

# 105、SpikE：基于SpikE的多关系图数据嵌入
- [ ] SpikE: spike-based embeddings for multi-relational graph data 
时间：2021年04月27日                         第一作者：Dominik Dold                       [链接](https://arxiv.org/abs/2104.13398).                     
## 摘要：尽管最近成功地将基于脉冲的编码与错误反向传播算法相结合，脉冲神经网络仍然主要应用于源于感觉处理的任务，操作于传统的数据结构，如视觉或听觉数据。一种在工业和研究中得到广泛应用的丰富数据表示是所谓的知识图——一种基于图的结构，其中实体被描述为节点，实体之间的关系被描述为边。像分子、社会网络和工业工厂系统这样的复杂系统可以用知识图的公共语言来描述，允许使用图嵌入算法在这些信息密集的环境中进行上下文感知的预测。我们提出了一种基于脉冲的算法，其中图中的节点由神经元群体的单脉冲时间表示，而群体之间的关系则由脉冲时间差表示。学习这种基于脉冲的嵌入只需要有关脉冲时间和脉冲时间差的知识，这与最近提出的训练脉冲神经网络的框架是一致的。所提出的模型可以很容易地映射到当前的神经形态硬件系统，从而将知识图上的推理转移到这些体系结构蓬勃发展的领域，为这项技术开辟了一个很有前途的工业应用领域。
<details>	<summary>英文摘要</summary>	Despite the recent success of reconciling spike-based coding with the error backpropagation algorithm, spiking neural networks are still mostly applied to tasks stemming from sensory processing, operating on traditional data structures like visual or auditory data. A rich data representation that finds wide application in industry and research is the so-called knowledge graph - a graph-based structure where entities are depicted as nodes and relations between them as edges. Complex systems like molecules, social networks and industrial factory systems can be described using the common language of knowledge graphs, allowing the usage of graph embedding algorithms to make context-aware predictions in these information-packed environments. We propose a spike-based algorithm where nodes in a graph are represented by single spike times of neuron populations and relations as spike time differences between populations. Learning such spike-based embeddings only requires knowledge about spike times and spike time differences, compatible with recently proposed frameworks for training spiking neural networks. The presented model is easily mapped to current neuromorphic hardware systems and thereby moves inference on knowledge graphs into a domain where these architectures thrive, unlocking a promising industrial application area for this technology. </details>
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月29日</details>

# 104、基于稀疏脉冲卷积神经网络的事件摄像机学习
- [ ] Learning from Event Cameras with Sparse Spiking Convolutional Neural Networks 
时间：2021年04月26日                         第一作者：Lo\"ic Cordone                       [链接](https://arxiv.org/abs/2104.12579).                     
## 摘要：卷积神经网络（CNNs）由于其令人印象深刻的结果和易于学习的特点，现已成为解决计算机视觉问题的实际方法。这些网络由一层层相互连接的单元组成，这些单元被称为人工神经元，松散地模拟了生物大脑中的神经元。然而，它们在传统硬件（CPU/GPU）上的实现导致了高功耗，使得它们在嵌入式系统上的集成变得困难。以汽车为例，嵌入式算法在能量、延迟和精度方面都有很高的限制。为了设计更有效的计算机视觉算法，我们建议采用事件摄像机和脉冲神经网络（SNNs）的端到端生物启发方法。事件摄像机输出异步和稀疏的事件，提供了一个非常有效的数据源，但是用同步和密集的算法（如CNNs）处理这些事件并没有产生任何显著的好处。为了解决这一局限性，我们使用了脉冲神经网络（SNNs），这是一种更具生物真实感的神经网络，其中单位使用离散脉冲进行通信。由于它们的操作性质，它们对硬件友好且节能，但培训它们仍然是一项挑战。我们的方法使用流行的深度学习框架PyTorch，直接在事件数据上训练稀疏脉冲卷积神经网络。在流行的DVS128手势数据集上，在准确性、稀疏性和训练时间方面的性能使得这种仿生方法有可能在低功耗的神经形态硬件上嵌入实时应用程序。
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) are now the de facto solution for computer vision problems thanks to their impressive results and ease of learning. These networks are composed of layers of connected units called artificial neurons, loosely modeling the neurons in a biological brain. However, their implementation on conventional hardware (CPU/GPU) results in high power consumption, making their integration on embedded systems difficult. In a car for example, embedded algorithms have very high constraints in term of energy, latency and accuracy. To design more efficient computer vision algorithms, we propose to follow an end-to-end biologically inspired approach using event cameras and spiking neural networks (SNNs). Event cameras output asynchronous and sparse events, providing an incredibly efficient data source, but processing these events with synchronous and dense algorithms such as CNNs does not yield any significant benefits. To address this limitation, we use spiking neural networks (SNNs), which are more biologically realistic neural networks where units communicate using discrete spikes. Due to the nature of their operations, they are hardware friendly and energy-efficient, but training them still remains a challenge. Our method enables the training of sparse spiking convolutional neural networks directly on event data, using the popular deep learning framework PyTorch. The performances in terms of accuracy, sparsity and training time on the popular DVS128 Gesture Dataset make it possible to use this bio-inspired approach for the future embedding of real-time applications on low-power neuromorphic hardware. </details>
<details>	<summary>注释</summary>	Accepted to the International Joint Conference on Neural Networks (IJCNN) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 103、脉冲神经网络第二部分：时空模式检测
- [ ] Spiking Neural Networks -- Part II: Detecting Spatio-Temporal Patterns 
时间：2021年04月26日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2010.14217).                     
<details>	<summary>注释</summary>	The first two authors have equally contributed to this work. This version corrects some errors in the published paper </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 102、STDP在联想信息存储和检索中的神经动力学作用
- [ ] Neurodynamical Role of STDP in Storage and Retrieval of Associative Information 
时间：2021年04月25日                         第一作者：Hongkyu Yoon                        [链接](https://arxiv.org/abs/2104.12249).                     
## 摘要：棘突时间依赖性可塑性（STDP）是一个生物学过程，在这个过程中，神经元棘突的精确顺序和时间影响突触修饰的程度。虽然已经有许多研究集中于STDP在神经编码中的作用，但STDP在大脑宏观层面的功能意义尚未得到充分的探索。在这项工作中，我们提出STDP在一个脉冲神经元群中呈现以“记忆平面”的形式存储高维信息。基于STDP的神经活动将周期性的时空输入模式转换为相应的记忆平面，在该记忆平面中，存储的信息可以通过适当的提示动态恢复。利用显示输入和记忆平面之间解析关系的动力系统理论，我们能够演示高维关联数据集的特定记忆过程。在自联想记忆任务中，可以从振荡的神经状态中提取连续流到系统中的一组图像。第二个应用程序处理从句子中嵌入语义记忆成分的过程。结果表明，词汇可以同时回忆多个句子，也可以只回忆一个句子，这取决于它们的语法关系。这意味着该框架易于处理多组具有复合结构的联想记忆。
<details>	<summary>英文摘要</summary>	Spike-timing-dependent plasticity (STDP) is a biological process in which the precise order and timing of neuronal spikes affect the degree of synaptic modification. While there has been numerous research focusing on the role of STDP in neural coding, the functional implications of STDP at the macroscopic level in the brain have not been fully explored yet. In this work, we propose that STDP in an ensemble of spiking neurons renders storing high dimensional information in the form of a `memory plane'. Neural activity based on STDP transforms periodic spatio-temporal input patterns into the corresponding memory plane, where the stored information can be dynamically revived with a proper cue. Using the dynamical systems theory that shows the analytic relation between the input and the memory plane, we were able to demonstrate a specific memory process for high-dimensional associative data sets. In the auto-associative memory task, a group of images that were continuously streamed to the system can be retrieved from the oscillating neural state. The second application deals with the process of semantic memory components that are embedded from sentences. The results show that words can recall multiple sentences simultaneously or one exclusively, depending on their grammatical relations. This implies that the proposed framework is apt to process multiple groups of associative memories with a composite structure. </details>
<details>	<summary>注释</summary>	14 pages of main text followed by 19 pages of supplements. Source for simplified MATLAB programs performing two numerical tests presented in this article can be found in the following link: https://github.com/hkyoon94/NRSTDP.git </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 101、基于生物激励优化器的深度神经网络学习
- [ ] Learning in Deep Neural Networks Using a Biologically Inspired Optimizer 
时间：2021年04月23日                         第一作者：Giorgia Dellaferrera                       [链接](https://arxiv.org/abs/2104.11604).                     
## 摘要：众所周知，大脑中的可塑性回路通过突触整合和突触强度的局部调节机制受到突触重量分布的影响。然而，迄今为止设计的大多数人工神经网络训练算法都忽略了刺激依赖性可塑性与局部学习信号之间的复杂相互作用。在这里，我们提出了一个新的生物启发优化人工神经网络（ANNs）和脉冲神经网络（SNNs），它结合了在皮层神经元树突中观察到的突触整合的关键原理：GRAPES（调整错误信号传播的群体责任）。GRAPES在神经网络的每个节点上对误差信号进行依赖于权值分布的调制。结果表明，这种生物激励机制使神经网络的收敛速度得到了系统的提高，并通过前馈结构和递归结构大大提高了神经网络和snn的分类精度。此外，我们证明了GRAPES支持复杂度不断增加的模型的性能可伸缩性，并通过使网络基于先前获得的知识泛化到不可见的任务来减轻灾难性遗忘。GRAPES的本地特性最小化了所需的内存资源，使其最适合专用硬件实现。总的来说，我们的工作表明协调神经生理学和机器智能是提高神经网络性能的关键。
<details>	<summary>英文摘要</summary>	Plasticity circuits in the brain are known to be influenced by the distribution of the synaptic weights through the mechanisms of synaptic integration and local regulation of synaptic strength. However, the complex interplay of stimulation-dependent plasticity with local learning signals is disregarded by most of the artificial neural network training algorithms devised so far. Here, we propose a novel biologically inspired optimizer for artificial (ANNs) and spiking neural networks (SNNs) that incorporates key principles of synaptic integration observed in dendrites of cortical neurons: GRAPES (Group Responsibility for Adjusting the Propagation of Error Signals). GRAPES implements a weight-distribution dependent modulation of the error signal at each node of the neural network. We show that this biologically inspired mechanism leads to a systematic improvement of the convergence rate of the network, and substantially improves classification accuracy of ANNs and SNNs with both feedforward and recurrent architectures. Furthermore, we demonstrate that GRAPES supports performance scalability for models of increasing complexity and mitigates catastrophic forgetting by enabling networks to generalize to unseen tasks based on previously acquired knowledge. The local characteristics of GRAPES minimize the required memory resources, making it optimally suited for dedicated hardware implementations. Overall, our work indicates that reconciling neurophysiology insights with machine intelligence is key to boosting the performance of neural networks. </details>
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 100、膜电位和激活阈稳态的持续学习和适应
- [ ] Continuous Learning and Adaptation with Membrane Potential and Activation Threshold Homeostasis 
时间：2021年04月22日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2104.10851).                     
## 摘要：大多数经典（非脉冲）神经网络模型忽略了内部神经元动力学，将神经元视为简单的输入积分器。然而，生物神经元的内部状态受复杂动力学控制，在学习、适应以及整个网络活动和行为中起着至关重要的作用。本文提出了一种膜电位和激活阈稳态（MPATH）神经元模型，该模型结合了多种生物激励机制，用一个类似于生物神经元膜时间常数的参数来有效地模拟神经元内部动力学。该模型允许神经元在输入波动时通过自动调节其活动来维持一种形式的动态平衡。MPATH模型的一个结果是，它给神经元灌输了一种时间感，而不需要反复连接，为建立依赖于神经元活动时间方面的过程模型铺平了道路。实验证明了该模型能够适应并不断地从输入中学习。
<details>	<summary>英文摘要</summary>	Most classical (non-spiking) neural network models disregard internal neuron dynamics and treat neurons as simple input integrators. However, biological neurons have an internal state governed by complex dynamics that plays a crucial role in learning, adaptation and the overall network activity and behaviour. This paper presents the Membrane Potential and Activation Threshold Homeostasis (MPATH) neuron model, which combines several biologically inspired mechanisms to efficiently simulate internal neuron dynamics with a single parameter analogous to the membrane time constant in biological neurons. The model allows neurons to maintain a form of dynamic equilibrium by automatically regulating their activity when presented with fluctuating input. One consequence of the MPATH model is that it imbues neurons with a sense of time without recurrent connections, paving the way for modelling processes that depend on temporal aspects of neuron activity. Experiments demonstrate the model's ability to adapt to and continually learn from its input. </details>
<details>	<summary>注释</summary>	19 pages </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 99、具有时间信息的抗噪声深脉冲神经网络
- [ ] Noise-Robust Deep Spiking Neural Networks with Temporal Information 
时间：2021年04月22日                         第一作者：Seongsik Park                       [链接](https://arxiv.org/abs/2104.11169).                     
## 摘要：脉冲神经网络（SNNs）是一种具有时间信息的节能神经网络。SNNs在神经形态器件上表现出了优越的效率，但是这种器件容易受到噪声的影响，阻碍了其在实际应用中的应用。一些研究提高了噪声鲁棒性，但大多数研究既没有考虑深度snn，也没有考虑时间信息。本文采用不同的神经编码方法研究了噪声对深度SNN的影响，提出了一种具有时间信息的抗噪声深度SNN。通过这些方法，我们实现了一个对脉冲删除和抖动有效且鲁棒的深度SNN。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have emerged as energy-efficient neural networks with temporal information. SNNs have shown a superior efficiency on neuromorphic devices, but the devices are susceptible to noise, which hinders them from being applied in real-world applications. Several studies have increased noise robustness, but most of them considered neither deep SNNs nor temporal information. In this paper, we investigate the effect of noise on deep SNNs with various neural coding methods and present a noise-robust deep SNN with temporal information. With the proposed methods, we have achieved a deep SNN that is efficient and robust to spike deletion and jitter. </details>
<details>	<summary>注释</summary>	Accepted to DAC 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 98、一种用于高能效目标检测的全脉冲混合神经网络
- [ ] A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection 
时间：2021年04月21日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2104.10719).                     
## 摘要：该文提出了一种用于资源受限平台中能量有效且鲁棒的目标检测的全脉冲混合神经网络（FSHNN）。该网络结构基于卷积SNN，采用漏泄集成火灾神经元模型。该模型将无监督脉冲时变塑性（STDP）学习与反向传播（STBP）学习方法相结合，并采用蒙特卡罗方法对不确定性误差进行估计。与基于DNN的目标探测器相比，FSHNN提供了更好的精度，同时具有150倍的能效。当受到噪声输入数据和标记较少的训练数据的影响时，它的性能也优于这些目标检测器，具有较低的不确定性误差。
<details>	<summary>英文摘要</summary>	This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on Convolutional SNN using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being 150X energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error. </details>
<details>	<summary>注释</summary>	10 pages, Submitted Manuscript </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 97、时间模式学习的神经形态算法硬件协同设计
- [ ] Neuromorphic Algorithm-hardware Codesign for Temporal Pattern Learning 
时间：2021年04月21日                         第一作者：Haowen Fang                       [链接](https://arxiv.org/abs/2104.10712).                     
## 摘要：神经形态计算和脉冲神经网络（SNN）模拟生物系统的行为，以其高能量效率完成认知任务的潜力引起了人们的兴趣。然而，时间动力学和脉冲计时等因素对信息处理至关重要，但往往被现有的研究所忽视，限制了神经形态计算的性能和应用。一方面，由于缺乏有效的SNN训练算法，很难利用时态神经动力学。许多现有的算法仍然统计处理神经元激活。另一方面，利用时间神经动力学也对硬件设计提出了挑战。突触表现出时间的动态性，作为保存历史信息的记忆单位，但通常被简化为与重量的联系。目前大多数的模型将突触激活整合到一些存储介质中，以表示膜电位，并在神经元发出脉冲信号后重新设置膜电位。这样做是因为它的硬件简单，只需要一个“清晰”的信号来擦除存储介质，但会破坏存储在神经元中的时间信息。在这项工作中，我们提出了一个有效的训练算法，可以训练SNN学习复杂的时空模式。我们在两个复杂的数据集上获得了有竞争力的精确度。我们还通过一个新的时间模式关联任务证明了该模型的优越性。利用该算法，我们开发了一个基于记忆器的神经元和突触网络的CMOS电路实现，该网络保留了关键的神经动力学特性，降低了复杂度。对神经元模型的电路实现进行了仿真，验证了该模型对具有自适应阈值的时间脉冲模式的反应能力。
<details>	<summary>英文摘要</summary>	Neuromorphic computing and spiking neural networks (SNN) mimic the behavior of biological systems and have drawn interest for their potential to perform cognitive tasks with high energy efficiency. However, some factors such as temporal dynamics and spike timings prove critical for information processing but are often ignored by existing works, limiting the performance and applications of neuromorphic computing. On one hand, due to the lack of effective SNN training algorithms, it is difficult to utilize the temporal neural dynamics. Many existing algorithms still treat neuron activation statistically. On the other hand, utilizing temporal neural dynamics also poses challenges to hardware design. Synapses exhibit temporal dynamics, serving as memory units that hold historical information, but are often simplified as a connection with weight. Most current models integrate synaptic activations in some storage medium to represent membrane potential and institute a hard reset of membrane potential after the neuron emits a spike. This is done for its simplicity in hardware, requiring only a "clear" signal to wipe the storage medium, but destroys temporal information stored in the neuron. In this work, we derive an efficient training algorithm for Leaky Integrate and Fire neurons, which is capable of training a SNN to learn complex spatial temporal patterns. We achieved competitive accuracy on two complex datasets. We also demonstrate the advantage of our model by a novel temporal pattern association task. Codesigned with this algorithm, we have developed a CMOS circuit implementation for a memristor-based network of neuron and synapses which retains critical neural dynamics with reduced complexity. This circuit implementation of the neuron model is simulated to demonstrate its ability to react to temporal spiking patterns with an adaptive threshold. </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 96、脉冲神经网络无监督模式识别的权值散度易化原理
- [ ] The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks 
时间：2021年04月20日                         第一作者：Oleg Nikitin                       [链接](https://arxiv.org/abs/2104.09943).                     
## 摘要：信号处理任务和生物神经元之间的相似性使我们理解了输入信号识别的自组织优化原理。在本文中，我们讨论了生物系统和技术系统之间的相似性。我们建议在众所周知的STDP突触可塑性规则的基础上，将权值调整指向与背景噪声和相关信号之间的最大差异相关的状态。物理约束的重量增长原理被用作控制重量变化的基础。有人提出，生物突触的直接修饰受到可塑性发育所需的生化物质的存在和产生的限制。本文利用信噪比信息来控制这类物质的产生和储存，并驱动神经元的突触压力达到信噪比最佳的状态。通过几个不同输入信号模式的实验来了解该方法的功能。
<details>	<summary>英文摘要</summary>	Parallels between the signal processing tasks and biological neurons lead to an understanding of the principles of self-organized optimization of input signal recognition. In the present paper, we discuss such similarities among biological and technical systems. We propose the addition to the well-known STDP synaptic plasticity rule to directs the weight modification towards the state associated with the maximal difference between the background noise and correlated signals. The principle of physically constrained weight growth is used as a basis for such control of the modification of the weights. It is proposed, that biological synaptic straight modification is restricted by the existence and production of bio-chemical 'substances' needed for plasticity development. In this paper, the information about the noise-to-signal ratio is used to control such a substances' production and storage and to drive the neuron's synaptic pressures towards the state with the best signal-to-noise ratio. Several experiments with different input signal regimes are considered to understand the functioning of the proposed approach. </details>
<details>	<summary>注释</summary>	9 pages, 5 figures, submitted to the conference ICANN 2021 MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 95、脉冲神经网络的基本组成模型
- [ ] A Basic Compositional Model for Spiking Neural Networks 
时间：2021年04月20日                         第一作者：Nancy Lynch                        [链接](https://arxiv.org/abs/1808.03884).                     
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 94、基于原始语音训练的CNNs中间卷积层解释
- [ ] Interpreting intermediate convolutional layers of CNNs trained on raw speech 
时间：2021年04月19日                         第一作者：Ga\v{s}per Begu\v{s}                        [链接](https://arxiv.org/abs/2104.09489).                     
## 摘要：提出了一种基于原始语音数据的CNNs中间层的非监督解释与可视化技术。我们表明，在每个卷积层的ReLU激活后，对特征映射进行平均可以得到可解释的时间序列数据。提出的技术可以对中间卷积层进行声学分析。为了揭示语音中有意义的表征是如何在cnn的中间层被编码的，我们将个体的潜在变量操纵到训练范围之外的边缘水平。我们在两个模型上训练和探索内部表示——一个是裸GAN架构，另一个是ciwGAN扩展，它迫使生成器输出信息数据，并导致出现有语言意义的表示。对语音的三个基本声学特性进行了解释和可视化：周期振动（对应元音）、非周期噪声振动（对应擦音）和沉默（对应停止）。我们还认为，所提出的技术允许对中间层进行声学分析，这与对人类语音数据的声学分析类似：我们可以从中间层提取F0、强度、持续时间、共振峰和其他声学特性，以便测试CNNs在何处以及如何编码各种类型的信息。这些模型是在两个不同复杂度的语音过程上训练的：一个简单的[s]和一个计算复杂的重叠（复制材料）。观察插值和中间层变化之间的因果关系可以揭示单个变量如何转化为中间层激活的脉冲。利用这种技术，我们可以分析语音中有意义的单位是如何在不同的卷积层中被编码的。
<details>	<summary>英文摘要</summary>	This paper presents a technique to interpret and visualize intermediate layers in CNNs trained on raw speech data in an unsupervised manner. We show that averaging over feature maps after ReLU activation in each convolutional layer yields interpretable time-series data. The proposed technique enables acoustic analysis of intermediate convolutional layers. To uncover how meaningful representation in speech gets encoded in intermediate layers of CNNs, we manipulate individual latent variables to marginal levels outside of the training range. We train and probe internal representations on two models -- a bare GAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). We also argue that the proposed technique allows acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. The models are trained on two speech processes with different degrees of complexity: a simple presence of [s] and a computationally complex presence of reduplication (copied material). Observing the causal effect between interpolation and the resulting changes in intermediate layers can reveal how individual variables get transformed into spikes in activation in intermediate layers. Using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in different convolutional layers. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 93、一种新的视觉处理器神经元模型
- [ ] A Novel Neuron Model of Visual Processor 
时间：2021年04月15日                         第一作者：Jizhao Liu                       [链接](https://arxiv.org/abs/2104.07257).                     
## 摘要：模拟和模仿人类或哺乳动物的神经网络是模式识别和计算机视觉领域多年来研究的热点。受猫初级视皮层神经元传导特性的启发，脉冲耦合神经网络（PCNNs）可以表现出同步振荡行为，无需训练即可处理数字图像。然而，根据对猫初级视皮层单个细胞的研究，当一个神经元受到外部周期性信号的刺激时，脉冲间间隔（ISI）分布表现为多峰分布。这种现象不能用所有的PCNN模型来解释。通过分析PCNN的工作机制，提出了一种新的由连续耦合神经网络（CCNN）构成的初级视皮层神经元模型。该模型继承了原PCNN模型的阈值指数衰减和同步脉冲振荡特性，能表现出与猫初级视皮层神经元测试结果一致的混沌行为。因此，我们的CCNN模型更接近真实的视觉神经网络。对于图像分割任务，基于CCNN模型的算法比现有的视觉皮层神经网络模型具有更好的性能。我们的方法的优势在于，它有助于神经生理学家进一步了解初级视觉皮层是如何工作的，并可用于定量预测真实神经网络的时空行为。CCNN还可能激励工程师为人工智能目的创建大脑启发的深度学习网络。
<details>	<summary>英文摘要</summary>	Simulating and imitating the neuronal network of humans or mammals is a popular topic that has been explored for many years in the fields of pattern recognition and computer vision. Inspired by neuronal conduction characteristics in the primary visual cortex of cats, pulse-coupled neural networks (PCNNs) can exhibit synchronous oscillation behavior, which can process digital images without training. However, according to the study of single cells in the cat primary visual cortex, when a neuron is stimulated by an external periodic signal, the interspike-interval (ISI) distributions represent a multimodal distribution. This phenomenon cannot be explained by all PCNN models. By analyzing the working mechanism of the PCNN, we present a novel neuron model of the primary visual cortex consisting of a continuous-coupled neural network (CCNN). Our model inherited the threshold exponential decay and synchronous pulse oscillation property of the original PCNN model, and it can exhibit chaotic behavior consistent with the testing results of cat primary visual cortex neurons. Therefore, our CCNN model is closer to real visual neural networks. For image segmentation tasks, the algorithm based on CCNN model has better performance than the state-of-art of visual cortex neural network model. The strength of our approach is that it helps neurophysiologists further understand how the primary visual cortex works and can be used to quantitatively predict the temporal-spatial behavior of real neural networks. CCNN may also inspire engineers to create brain-inspired deep learning networks for artificial intelligence purposes. </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 92、与神经形态处理器兼容的误差传播脉冲神经网络
- [ ] An error-propagation spiking neural network compatible with neuromorphic processors 
时间：2021年04月12日                         第一作者：Matteo Cartiglia                       [链接](https://arxiv.org/abs/2104.05241).                     
## 摘要：脉冲神经网络在低功耗感知处理和边缘计算硬件平台的设计中显示出巨大的潜力。然而，在这种结构上实现片上学习算法仍然是一个开放的挑战，特别是对于依赖于反向传播算法的多层网络。本文提出了一种基于峰值的学习方法，该方法利用局部权值更新机制来逼近反向传播，并与模拟/数字混合神经形态电路兼容。我们介绍了一种网络结构，它使突触权值更新机制能够跨层反向传播错误信号，并提出了一种网络，该网络可以被训练来区分具有相同平均放电率但不同脉冲计时的两种基于脉冲的模式。这项工作是朝着设计超低功耗混合信号神经形态处理系统迈出的第一步，该系统具有片上学习电路，可被训练以识别不同的脉冲活动时空模式（例如，由基于事件的视觉或听觉传感器产生）。
<details>	<summary>英文摘要</summary>	Spiking neural networks have shown great promise for the design of low-power sensory-processing and edge-computing hardware platforms. However, implementing on-chip learning algorithms on such architectures is still an open challenge, especially for multi-layer networks that rely on the back-propagation algorithm. In this paper, we present a spike-based learning method that approximates back-propagation using local weight update mechanisms and which is compatible with mixed-signal analog/digital neuromorphic circuits. We introduce a network architecture that enables synaptic weight update mechanisms to back-propagate error signals across layers and present a network that can be trained to distinguish between two spike-based patterns that have identical mean firing rates, but different spike-timings. This work represents a first step towards the design of ultra-low power mixed-signal neuromorphic processing systems with on-chip learning circuits that can be trained to recognize different spatio-temporal patterns of spiking activity (e.g. produced by event-based vision or auditory sensors). </details>
<details>	<summary>注释</summary>	2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 91、实值输入到脉冲序列的自适应转换
- [ ] Adaptive conversion of real-valued input into spike trains 
时间：2021年04月12日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2104.05401).                     
## 摘要：本文提出了一种生物学上可行的方法，将实值输入转化为脉冲序列，用脉冲神经网络进行处理。所提出的方法模仿视网膜神经节细胞的适应性行为，并允许输入神经元适应其对输入统计量变化的反应。因此，输入层不是被动地接收值并将其转发到隐藏层和输出层，而是充当一个自我调节滤波器，它强调与平均值的偏差，同时允许输入神经元对平均值本身有效地脱敏。该方法的另一个优点是每个变量只需要一个输入神经元，而不是像常用的基于高斯感受野的转换方法那样需要整个神经元群。此外，由于输入的统计数据随着时间的推移而自然出现，因此在将数据馈送到网络之前不必对其进行预处理。这使得脉冲神经网络能够处理原始的、非标准化的流数据。通过概念验证实验验证了该方法的有效性。
<details>	<summary>英文摘要</summary>	This paper presents a biologically plausible method for converting real-valued input into spike trains for processing with spiking neural networks. The proposed method mimics the adaptive behaviour of retinal ganglion cells and allows input neurons to adapt their response to changes in the statistics of the input. Thus, rather than passively receiving values and forwarding them to the hidden and output layers, the input layer acts as a self-regulating filter which emphasises deviations from the average while allowing the input neurons to become effectively desensitised to the average itself. Another merit of the proposed method is that it requires only one input neuron per variable, rather than an entire population of neurons as in the case of the commonly used conversion method based on Gaussian receptive fields. In addition, since the statistics of the input emerge naturally over time, it becomes unnecessary to pre-process the data before feeding it to the network. This enables spiking neural networks to process raw, non-normalised streaming data. A proof-of-concept experiment is performed to demonstrate that the proposed method operates as expected. </details>
<details>	<summary>注释</summary>	8 pages Journal-ref: 2016 International Joint Conference on Neural Networks DOI: 10.1109/IJCNN.2016.7727314 </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 90、PrivateSNN：完全隐私保护的脉冲神经网络
- [ ] PrivateSNN: Fully Privacy-Preserving Spiking Neural Networks 
时间：2021年04月07日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2104.03414).                     
## 摘要：我们如何在边缘设备上为神经系统带来隐私和能源效率？在本文中，我们提出了PrivateSNN，其目的是从预先训练好的ANN模型中构建低功耗的脉冲神经网络（SNNs），而不泄漏数据集中包含的敏感信息。在这里，我们解决两种类型的泄漏问题：1）在ANN-SNN转换过程中，网络访问真实训练数据时引起的数据泄漏。2） 类泄漏是由网络参数重构类相关特征时产生的泄漏的概念。为了解决数据泄漏问题，我们从预训练的神经网络生成合成图像，并利用生成的图像将神经网络转换为snn。然而，由于权重参数相对于ANN参数具有相同（或缩放）的值，转换后的snn对于类泄漏仍然是脆弱的。因此，我们通过使用基于时间脉冲的学习规则训练SNN来加密SNN权重。利用时间数据更新权值参数，使得网络在空间域难以解释。我们观察到，加密的PrivateSNN不仅可以在没有巨大性能下降（小于~5%）的情况下实现，而且具有显著的能量效率增益（与标准ANN相比大约x60）。我们在各种数据集上进行了广泛的实验，包括CIFAR10、CIFAR100和tinyimagnet，强调了隐私保护SNN训练的重要性。
<details>	<summary>英文摘要</summary>	How can we bring both privacy and energy-efficiency to a neural system on edge devices? In this paper, we propose PrivateSNN, which aims to build low-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without leaking sensitive information contained in a dataset. Here, we tackle two types of leakage problems: 1) Data leakage caused when the networks access real training data during an ANN-SNN conversion process. 2) Class leakage is the concept of leakage caused when class-related features can be reconstructed from network parameters. In order to address the data leakage issue, we generate synthetic images from the pre-trained ANNs and convert ANNs to SNNs using generated images. However, converted SNNs are still vulnerable with respect to the class leakage since the weight parameters have the same (or scaled) value with respect to ANN parameters. Therefore, we encrypt SNN weights by training SNNs with a temporal spike-based learning rule. Updating weight parameters with temporal data makes networks difficult to be interpreted in the spatial domain. We observe that the encrypted PrivateSNN can be implemented not only without the huge performance drop (less than ~5%) but also with significant energy-efficiency gain (about x60 compared to the standard ANN). We conduct extensive experiments on various datasets including CIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of privacy-preserving SNN training. </details>
<details>	<summary>邮件日期</summary>	2021年04月09日</details>

# 89、基于神经形态立体视觉系统的实时立体深度估计
- [ ] Instantaneous Stereo Depth Estimation of Real-World Stimuli with a Neuromorphic Stereo-Vision Setup 
时间：2021年04月06日                         第一作者：Nicoletta Risi                       [链接](https://arxiv.org/abs/2104.02541).                     
## 摘要：在生物学中，立体匹配问题得到了有效的解决，即在两个不同的视图中匹配相应的特征来重建深度。然而，它仍然是经典机器视觉方法的计算瓶颈。利用事件摄像机的特性，最近提出的立体视觉脉冲神经网络（SNN）结构有可能简化立体匹配问题。一些结合事件摄像机和基于峰值的神经形态处理器的解决方案已经存在。然而，他们要么在数字硬件上模拟，要么在简化的刺激物上测试。在这项工作中，我们使用动态视觉传感器3D人体姿势数据集（DHP19）来验证一个基于大脑启发事件的立体匹配架构，该架构是在一个混合信号神经形态处理器上实现的，并具有真实世界的数据。我们的实验表明，这种由重合检测器和视差敏感神经元组成的SNN结构能够在瞬间提供对输入视差的粗略估计，从而实时检测到深度移动的刺激的存在。
<details>	<summary>英文摘要</summary>	The stereo-matching problem, i.e., matching corresponding features in two different views to reconstruct depth, is efficiently solved in biology. Yet, it remains the computational bottleneck for classical machine vision approaches. By exploiting the properties of event cameras, recently proposed Spiking Neural Network (SNN) architectures for stereo vision have the potential of simplifying the stereo-matching problem. Several solutions that combine event cameras with spike-based neuromorphic processors already exist. However, they are either simulated on digital hardware or tested on simplified stimuli. In this work, we use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a brain-inspired event-based stereo-matching architecture implemented on a mixed-signal neuromorphic processor with real-world data. Our experiments show that this SNN architecture, composed of coincidence detectors and disparity sensitive neurons, is able to provide a coarse estimate of the input disparity instantaneously, thereby detecting the presence of a stimulus moving in depth in real-time. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 88、乘性突触脉冲神经网络的非线性计算
- [ ] Nonlinear computations in spiking neural networks through multiplicative synapses 
时间：2021年03月31日                         第一作者：Michele Nardin                       [链接](https://arxiv.org/abs/2009.03857).                     
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 87、在BrainScaleS-2移动系统上演示模拟推理
- [ ] Demonstrating Analog Inference on the BrainScaleS-2 Mobile System 
时间：2021年03月29日                         第一作者：Yannik Stradmann                       [链接](https://arxiv.org/abs/2103.15960).                     
## 摘要：我们提出了BrainScaleS-2mobile系统作为一个基于BrainScaleS-2asic的小型模拟推理机，并展示了它在医学心电图数据集分类方面的能力。利用ASIC的模拟网络核心实现了卷积深度神经网络的乘法累加运算。我们测量了ASIC的总能量消耗192uJ，并实现了每个心电图患者样本276us的分类时间。房颤患者在14.0（10%）假阳性时的检出率为93.7%（7%）。该系统具有体积小、功耗高、I/O灵活等优点，可直接应用于边缘推理应用。未来可能的应用还可以在一个BrainScaleS-2asic上，将传统的机器学习层与神经网络的在线学习结合起来。该系统已成功参与德国联邦教育和研究部（BMBF）独立评审的“Pilotinnovationswettbewerb‘Energieeffizientes KI system’”竞赛，并被证明运行可靠。
<details>	<summary>英文摘要</summary>	We present the BrainScaleS-2 mobile system as a compact analog inference engine based on the BrainScaleS-2 ASIC and demonstrate its capabilities at classifying a medical electrocardiogram dataset. The analog network core of the ASIC is utilized to perform the multiply-accumulate operations of a convolutional deep neural network. We measure a total energy consumption of 192uJ for the ASIC and achieve a classification time of 276us per electrocardiographic patient sample. Patients with atrial fibrillation are correctly identified with a detection rate of 93.7(7)% at 14.0(10)% false positives. The system is directly applicable to edge inference applications due to its small size, power envelope and flexible I/O capabilities. Possible future applications can furthermore combine conventional machine learning layers with online-learning in spiking neural networks on a single BrainScaleS-2 ASIC. The system has successfully participated and proven to operate reliably in the independently judged competition "Pilotinnovationswettbewerb 'Energieeffizientes KI-System'" of the German Federal Ministry of Education and Research (BMBF). </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 86、基于峰值模式识别的蛋白质结构库计算
- [ ] Protein Structured Reservoir computing for Spike-based Pattern Recognition 
时间：2021年03月29日                         第一作者：Karolos-Alex                       [链接](https://arxiv.org/abs/2008.03330).                     
<details>	<summary>注释</summary>	15 pages, 9 figures DOI: 10.1109/TPDS.2021.3068826 </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 85、能量衰减网络（EDeN）
- [ ] Energy Decay Network (EDeN) 
时间：2021年03月10日                         第一作者：Jamie Nicholas Shelley                       [链接](https://arxiv.org/abs/2103.15552).                     
## 摘要：本文和伴随的Python和C++框架是作者感知到的问题与狭窄的（基于歧视的）人工智能的产物。（人工智能）该框架试图通过使用共同的调节/交换值（能量）的潜在结构表达来开发经验的遗传转移，从而创建一个模型，通过遗传和实时信号处理影响，神经结构和所有单元过程相互依赖地共同发展；成功的路径是由每个时期的穗分布的稳定性来定义的，它受遗传编码的形态发育的影响偏见。这些这些原则的目的是建立一个多样化和强大的网络，能够适应一般的任务，通过在一个旨在将学习转移到其他领域的模拟训练规模的媒介。
<details>	<summary>英文摘要</summary>	This paper and accompanying Python and C++ Framework is the product of the authors perceived problems with narrow (Discrimination based) AI. (Artificial Intelligence) The Framework attempts to develop a genetic transfer of experience through potential structural expressions using a common regulation/exchange value (energy) to create a model whereby neural architecture and all unit processes are co-dependently developed by genetic and real time signal processing influences; successful routes are defined by stability of the spike distribution per epoch which is influenced by genetically encoded morphological development biases.These principles are aimed towards creating a diverse and robust network that is capable of adapting to general tasks by training within a simulation designed for transfer learning to other mediums at scale. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 84、利用脉冲间隔对脉冲神经网络的直观解释
- [ ] Visual Explanations from Spiking Neural Networks using Interspike Intervals 
时间：2021年03月26日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2103.14441).                     
## 摘要：脉冲神经网络（SNNs）计算并与异步二进制时态事件进行通信，通过神经形态硬件可以显著节省能量。最近在训练snn方面的算法研究表明，snn在各种分类任务上都有很好的性能。然而，目前还没有一种可视化的工具来分析和解释这种时间深度SNNs的内部脉冲行为。在这篇论文中，我们提出了一个新的概念，为SNNs的生物合理的可视化，称为峰激活图（SAM）。提出的SAM避免了脉冲神经元的不可微性，不需要计算梯度来获得直观的解释。相反，SAM通过在不同的时间步长上向前传播输入峰值来计算时间可视化图。SAM通过突出显示具有短峰间期活动的神经元，产生与输入数据的每个时间步相对应的注意图。有趣的是，没有反向传播过程和类标签，SAM在捕获细粒度细节的同时突出了图像的区分区域。利用SAM，我们首次全面分析了内部脉冲在各种SNN训练配置中的工作方式，这取决于优化类型、泄漏行为，以及面对对手的例子时。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) compute and communicate with asynchronous binary temporal events that can lead to significant energy savings with neuromorphic hardware. Recent algorithmic efforts on training SNNs have shown competitive performance on a variety of classification tasks. However, a visualization tool for analysing and explaining the internal spike behavior of such temporal deep SNNs has not been explored. In this paper, we propose a new concept of bio-plausible visualization for SNNs, called Spike Activation Map (SAM). The proposed SAM circumvents the non-differentiable characteristic of spiking neurons by eliminating the need for calculating gradients to obtain visual explanations. Instead, SAM calculates a temporal visualization map by forward propagating input spikes over different time-steps. SAM yields an attention map corresponding to each time-step of input data by highlighting neurons with short inter-spike interval activity. Interestingly, without both the backpropagation process and the class label, SAM highlights the discriminative region of the image while capturing fine-grained details. With SAM, for the first time, we provide a comprehensive analysis on how internal spikes work in various SNN training configurations depending on optimization types, leak behavior, as well as when faced with adversarial examples. </details>
<details>	<summary>邮件日期</summary>	2021年03月29日</details>

# 83、基于粗糙度指数和粗糙距离的医学图像分割基准研究
- [ ] Roughness Index and Roughness Distance for Benchmarking Medical Segmentation 
时间：2021年03月23日                         第一作者：Vidhiwar Singh Rathour                       [链接](https://arxiv.org/abs/2103.12350).                     
## 摘要：医学图像分割是医学图像分析中最具挑战性的任务之一，在许多临床应用中得到了广泛的发展。现有的大多数度量方法都是先为自然图像设计，然后扩展到医学图像。虽然物体表面在医学分割和定量分析中起着重要的作用，即分析脑肿瘤表面、测量灰质体积，但现有的大多数测量方法在分析物体表面时，特别是在判断给定体积物体的表面光滑度或粗糙度或分析其表面粗糙度时，都是有限的拓扑错误。本文首先分析了现有的各种医学图像分割方法的优缺点，特别是对体数据的分割。在此基础上，提出了一个适合于医学图像分割分析和评价的粗糙度指数和粗糙度距离。我们提出的方法解决了两类分割错误，即（i）边界/曲面上的拓扑错误和（ii）边界/曲面上的不规则性。这项工作的贡献是四个方面：（i）检测表面上的不规则脉冲/孔，（ii）提出粗糙度指数来测量给定物体的表面粗糙度，（iii）提出粗糙度距离来测量两个边界/表面之间的距离，利用提出的粗糙度指数和（iv）提出一种有助于去除的算法使表面光滑的不规则的尖刺/孔。我们提出的粗糙度指数和粗糙度距离是建立在固体表面粗糙度参数已在土木工程中成功开发。
<details>	<summary>英文摘要</summary>	Medical image segmentation is one of the most challenging tasks in medical image analysis and has been widely developed for many clinical applications. Most of the existing metrics have been first designed for natural images and then extended to medical images. While object surface plays an important role in medical segmentation and quantitative analysis i.e. analyze brain tumor surface, measure gray matter volume, most of the existing metrics are limited when it comes to analyzing the object surface, especially to tell about surface smoothness or roughness of a given volumetric object or to analyze the topological errors. In this paper, we first analysis both pros and cons of all existing medical image segmentation metrics, specially on volumetric data. We then propose an appropriate roughness index and roughness distance for medical image segmentation analysis and evaluation. Our proposed method addresses two kinds of segmentation errors, i.e. (i)topological errors on boundary/surface and (ii)irregularities on the boundary/surface. The contribution of this work is four-fold: (i) detect irregular spikes/holes on a surface, (ii) propose roughness index to measure surface roughness of a given object, (iii) propose a roughness distance to measure the distance of two boundaries/surfaces by utilizing the proposed roughness index and (iv) suggest an algorithm which helps to remove the irregular spikes/holes to smooth the surface. Our proposed roughness index and roughness distance are built upon the solid surface roughness parameter which has been successfully developed in the civil engineering. </details>
<details>	<summary>注释</summary>	Paper has been accepted at BIOIMAGING2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 82、论系统软件在神经形态计算能量管理中的作用
- [ ] On the Role of System Software in Energy Management of Neuromorphic Computing 
时间：2021年03月22日                         第一作者：Twisha Titirsha                       [链接](https://arxiv.org/abs/2103.12231).                     
## 摘要：最近，DYNAPs和Loihi等神经形态计算系统被引入计算机界，以提高机器学习程序的性能和能量效率，特别是那些使用脉冲神经网络（Spiking Neural Network，SNN）实现的程序。神经形态系统的系统软件的作用是对一个大型机器学习模型（例如，有许多神经元和突触）进行聚类，并将这些聚类映射到硬件的计算资源。在这项工作中，我们建立了一个神经形态硬件的能量消耗，考虑了神经元和突触所消耗的能量，以及互连上通信脉冲所消耗的能量。基于这样的公式，我们首先评估了系统软件在管理神经形态系统能量消耗方面的作用。接下来，我们提出一个简单的启发式映射方法，将神经元和突触放置到计算资源上，以减少能量消耗。我们用10个机器学习应用来评估我们的方法，并证明所提出的映射方法可以显著降低神经形态计算系统的能量消耗。
<details>	<summary>英文摘要</summary>	Neuromorphic computing systems such as DYNAPs and Loihi have recently been introduced to the computing community to improve performance and energy efficiency of machine learning programs, especially those that are implemented using Spiking Neural Network (SNN). The role of a system software for neuromorphic systems is to cluster a large machine learning model (e.g., with many neurons and synapses) and map these clusters to the computing resources of the hardware. In this work, we formulate the energy consumption of a neuromorphic hardware, considering the power consumed by neurons and synapses, and the energy consumed in communicating spikes on the interconnect. Based on such formulation, we first evaluate the role of a system software in managing the energy consumption of neuromorphic systems. Next, we formulate a simple heuristic-based mapping approach to place the neurons and synapses onto the computing resources to reduce energy consumption. We evaluate our approach with 10 machine learning applications and demonstrate that the proposed mapping approach leads to a significant reduction of energy consumption of neuromorphic computing systems. </details>
<details>	<summary>注释</summary>	To appear in 18th Computer Frontiers 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 81、皮层振荡在脉冲神经网络中实现了基于采样的计算
- [ ] Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks 
时间：2021年03月22日                         第一作者：Agnes Korcsak-Gorzo                       [链接](https://arxiv.org/abs/2006.11099).                     
<details>	<summary>注释</summary>	30 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 80、融合流网：使用传感器融合和深度融合脉冲模拟网络架构的节能光流估计
- [ ] Fusion-FlowNet: Energy-Efficient Optical Flow Estimation using Sensor Fusion and Deep Fused Spiking-Analog Network Architectures 
时间：2021年03月19日                         第一作者：Chankyu Lee                       [链接](https://arxiv.org/abs/2103.10592).                     
## 摘要：标准的基于帧的相机采样光强度帧严重影响了高速运动的运动模糊，当动态范围较大时，无法准确地感知场景。另一方面，基于事件的相机通过异步检测单个像素强度的变化来克服这些限制。但是，事件摄影机仅提供运动中像素的信息，导致数据稀疏。因此，估计像素的整体密集行为是困难的。为了解决与传感器相关的问题，我们提出了Fusion FlowNet，一个传感器融合框架，用于利用基于帧和基于事件的传感器的能量高效光流估计，利用它们的互补特性。我们提出的网络结构也是一个融合了脉冲神经网络（SNNs）和模拟神经网络（ANNs）的网络，其中每个网络分别被设计为同时处理异步事件流和基于规则帧的图像。我们的网络使用无监督学习进行端到端训练，以避免昂贵的视频注释。该方法在不同的环境（快速运动和具有挑战性的光照条件）下具有良好的通用性，并在多车辆立体事件相机（MVSEC）数据集上展示了最先进的光流预测。此外，我们的网络在网络参数数量和计算能量成本方面提供了大量的节省。
<details>	<summary>英文摘要</summary>	Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately when the dynamic range is high. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only provide information about pixels in motion, leading to sparse data. Hence, estimating the overall dense behavior of pixels is difficult. To address such issues associated with the sensors, we present Fusion-FlowNet, a sensor fusion framework for energy-efficient optical flow estimation using both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations. The method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost. </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 79、皮层振荡在脉冲神经网络中实现了基于采样的计算
- [ ] Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks 
时间：2021年03月19日                         第一作者：Agnes Korcsak-Gorzo                       [链接](https://arxiv.org/abs/2006.11099).                     
<details>	<summary>注释</summary>	33 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 78、基于自适应脉冲递归神经网络的精确高效时域分类
- [ ] Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks 
时间：2021年03月12日                         第一作者：Bojian Yin                       [链接](https://arxiv.org/abs/2103.12593).                     
## 摘要：受生物神经元更详细模型的启发，脉冲神经网络（Spiking neural networks，SNNs）作为一种更具生物合理性和潜在更强大的神经计算模型被研究，其目的是提取生物神经元的能量效率；然而，与传统的人工神经网络（ANNs）相比，这种网络的性能仍然很差。在这里，我们展示了一个新的替代梯度与可调和自适应脉冲神经元的循环网络相结合如何在时域的挑战性基准（如语音和手势识别）上产生snn的最新技术。这也超过了标准的经典递归神经网络（RNN）的性能，接近现代最好的人工神经网络。由于这些snn表现出稀疏脉冲，我们证明它们在理论上比具有类似性能的rnn计算效率高一到三个数量级。总之，SNNs是人工智能硬件实现的一个有吸引力的解决方案。
<details>	<summary>英文摘要</summary>	Inspired by more detailed modeling of biological neurons, Spiking neural networks (SNNs) have been investigated both as more biologically plausible and potentially more powerful models of neural computation, and also with the aim of extracting biological neurons' energy efficiency; the performance of such networks however has remained lacking compared to classical artificial neural networks (ANNs). Here, we demonstrate how a novel surrogate gradient combined with recurrent networks of tunable and adaptive spiking neurons yields state-of-the-art for SNNs on challenging benchmarks in the time-domain, like speech and gesture recognition. This also exceeds the performance of standard classical recurrent neural networks (RNNs) and approaches that of the best modern ANNs. As these SNNs exhibit sparse spiking, we show that they theoretically are one to three orders of magnitude more computationally efficient compared to RNNs with comparable performance. Together, this positions SNNs as an attractive solution for AI hardware implementations. </details>
<details>	<summary>注释</summary>	11 pages </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 77、脉冲神经元的线性约束学习
- [ ] Linear Constraints Learning for Spiking Neurons 
时间：2021年03月10日                         第一作者：Huy Le Nguyen                       [链接](https://arxiv.org/abs/2103.12564).                     
## 摘要：利用脉冲编码神经元对信息进行精确的脉冲定时编码，已经被证明比速率编码方法具有更强的计算能力。然而，现有的针对脉冲神经元的监督学习算法大多比较复杂，时间复杂度较差。针对这些局限性，我们提出了一种有监督的多脉冲学习算法，减少了所需的训练迭代次数。我们通过将大量的权值更新描述为一个线性约束满足问题来实现这一点，可以有效地解决这个问题。实验结果表明，该方法在MNIST数据集上比现有算法具有更好的效率。此外，我们提供了LIF神经元模型的分类能力的实验结果，与系统的几个参数有关。
<details>	<summary>英文摘要</summary>	Encoding information with precise spike timings using spike-coded neurons has been shown to be more computationally powerful than rate-coded approaches. However, most existing supervised learning algorithms for spiking neurons are complicated and offer poor time complexity. To address these limitations, we propose a supervised multi-spike learning algorithm which reduces the required number of training iterations. We achieve this by formulating a large number of weight updates as a linear constraint satisfaction problem, which can be solved efficiently. Experimental results show this method offers better efficiency compared to existing algorithms on the MNIST dataset. Additionally, we provide experimental results on the classification capacity of the LIF neuron model, relative to several parameters of the system. </details>
<details>	<summary>注释</summary>	17 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 76、约束塑性储备作为神经网络频率和权值控制的一种自然方法
- [ ] Constrained plasticity reserve as a natural way to control frequency and weights in spiking neural networks 
时间：2021年03月15日                         第一作者：Oleg Nikitin                        [链接](https://arxiv.org/abs/2103.08143).                     
## 摘要：生物神经元具有自适应性，并进行复杂的计算，包括过滤冗余信息。这种处理通常与贝叶斯推理相联系。然而，最常见的神经细胞模型，包括生物学上合理的模型，如霍奇金-赫胥黎或伊兹克维奇，在单个细胞水平上并不具备预测动力学。现代的突触可塑性或互联适应规则也不能为神经元适应不断变化的输入信号强度的能力提供基础。虽然自然神经元突触生长受到蛋白质供应和循环的精确控制和限制，但广泛使用的STDP等重量校正规则在变化率和范围上是有效的无限制的。在这篇文章中，我们将介绍一种新的机制，通过抽象蛋白质储备限制的STDP生长，并通过细胞内优化算法来控制神经元放电率稳态和重量变化之间的联系。我们将展示，这些细胞动力学如何帮助神经元过滤强烈的信号，帮助神经元保持稳定的放电频率。我们还将检验这种滤波不影响神经元在无监督模式下识别相关输入的能力。这种方法可用于机器学习领域，以提高人工智能系统的鲁棒性。
<details>	<summary>英文摘要</summary>	Biological neurons have adaptive nature and perform complex computations involving the filtering of redundant information. Such processing is often associated with Bayesian inference. Yet most common models of neural cells, including biologically plausible, such as Hodgkin-Huxley or Izhikevich do not possess predictive dynamics on the level of a single cell. The modern rules of synaptic plasticity or interconnections weights adaptation also do not provide grounding for the ability of neurons to adapt to the ever-changing input signal intensity. While natural neuron synaptic growth is precisely controlled and restricted by protein supply and recycling, weight correction rules such as widely used STDP are efficiently unlimited in change rate and scale. In the present article, we will introduce new mechanics of interconnection between neuron firing rate homeostasis and weight change by means of STDP growth bounded by abstract protein reserve, controlled by the intracellular optimization algorithm. We will show, how these cellular dynamics help neurons to filter out the intense signals to help neurons keep a stable firing rate. We will also examine that such filtering does not affect the ability of neurons to recognize the correlated inputs in unsupervised mode. Such an approach might be used in the machine learning domain to improve the robustness of AI systems. </details>
<details>	<summary>注释</summary>	24 pages, 12 figures MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年03月16日</details>

# 75、事件摄影机的时间顺序最近事件（TORE）卷
- [ ] Time-Ordered Recent Event (TORE) Volumes for Event Cameras 
时间：2021年03月10日                         第一作者：R. Wes Baldwin                       [链接](https://arxiv.org/abs/2103.06108).                     
## 摘要：事件摄像机是一种令人兴奋的新型传感器，能够以极低的延迟和宽的动态范围实现高速成像。不幸的是，大多数机器学习体系结构都不是直接处理稀疏数据的，比如从事件摄像机生成的数据。许多最先进的事件摄像机算法依赖于内插事件表示法——模糊了关键的定时信息，增加了数据量，限制了整体网络性能。本文详细介绍了一种称为时间顺序最近事件（TORE）卷的事件表示。存储卷被设计成以最小的信息丢失紧凑地存储原始峰值定时信息。这种仿生设计内存效率高，计算速度快，避免时间阻塞（即固定和预定义的帧速率），并包含来自过去数据的“本地内存”。该设计在一系列具有挑战性的任务（如事件去噪、图像重建、分类和人体姿态估计）上进行了评估，并显示出显著提高了最先进的性能。存储卷是当前使用事件表示的任何算法的易于实现的替代品。
<details>	<summary>英文摘要</summary>	Event cameras are an exciting, new sensor modality enabling high-speed imaging with extremely low-latency and wide dynamic range. Unfortunately, most machine learning architectures are not designed to directly handle sparse data, like that generated from event cameras. Many state-of-the-art algorithms for event cameras rely on interpolated event representations - obscuring crucial timing information, increasing the data volume, and limiting overall network performance. This paper details an event representation called Time-Ordered Recent Event (TORE) volumes. TORE volumes are designed to compactly store raw spike timing information with minimal information loss. This bio-inspired design is memory efficient, computationally fast, avoids time-blocking (i.e. fixed and predefined frame rates), and contains "local memory" from past data. The design is evaluated on a wide range of challenging tasks (e.g. event denoising, image reconstruction, classification, and human pose estimation) and is shown to dramatically improve state-of-the-art performance. TORE volumes are an easy-to-implement replacement for any algorithm currently utilizing event representations. </details>
<details>	<summary>邮件日期</summary>	2021年03月11日</details>

# 74、具有耐久性的脉冲神经网络到神经形态硬件的映射
- [ ] Endurance-Aware Mapping of Spiking Neural Networks to Neuromorphic Hardware 
时间：2021年03月09日                         第一作者：Twisha Titirsha                       [链接](https://arxiv.org/abs/2103.05707).                     
## 摘要：神经形态计算系统正在采用忆阻器来实现高密度和低功耗的突触存储，如硬件中的交叉阵列。这些系统在执行脉冲神经网络（SNNs）时是节能的。我们观察到记忆型纵横制中的长位线和字线是寄生电压降的主要来源，寄生电压降会造成电流不对称。通过电路仿真，我们发现了这种不对称性导致的显著耐久性变化。因此，如果临界忆阻器（耐久性较低的忆阻器）被过度利用，它们可能会导致交叉杆寿命的缩短。我们提出eSpine，这是一种新的技术，通过在映射机器学习工作负载的每个纵横杆中加入耐久性变化来提高寿命，确保具有更高激活的突触总是在具有更高耐久性的忆阻器上实现，反之亦然。eSpine分两步工作。首先，它使用Kernighan-Lin图划分算法将工作负载划分为神经元和突触的簇，每个簇可以放在一个横杆中。第二，利用粒子群优化算法（PSO）的一个实例将簇映射到分片上，通过分析簇在工作负载中的激活情况，将簇的突触放置到十字杆的忆阻器上。我们评估了一个国家的最先进的神经形态硬件模型与相变记忆（PCM）为基础的忆阻器eSpine。使用10个SNN工作负载，我们证明了有效生存期的显著改进。
<details>	<summary>英文摘要</summary>	Neuromorphic computing systems are embracing memristors to implement high density and low power synaptic storage as crossbar arrays in hardware. These systems are energy efficient in executing Spiking Neural Networks (SNNs). We observe that long bitlines and wordlines in a memristive crossbar are a major source of parasitic voltage drops, which create current asymmetry. Through circuit simulations, we show the significant endurance variation that results from this asymmetry. Therefore, if the critical memristors (ones with lower endurance) are overutilized, they may lead to a reduction of the crossbar's lifetime. We propose eSpine, a novel technique to improve lifetime by incorporating the endurance variation within each crossbar in mapping machine learning workloads, ensuring that synapses with higher activation are always implemented on memristors with higher endurance, and vice versa. eSpine works in two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to partition a workload into clusters of neurons and synapses, where each cluster can fit in a crossbar. Second, it uses an instance of Particle Swarm Optimization (PSO) to map clusters to tiles, where the placement of synapses of a cluster to memristors of a crossbar is performed by analyzing their activation within the workload. We evaluate eSpine for a state-of-the-art neuromorphic hardware model with phase-change memory (PCM)-based memristors. Using 10 SNN workloads, we demonstrate a significant improvement in the effective lifetime. </details>
<details>	<summary>注释</summary>	Accepted for publication in IEEE Transactions on Parallel and Distributed Systems (TPDS) </details>
<details>	<summary>邮件日期</summary>	2021年03月11日</details>

# 73、一种用于无监督特征学习的高并行度类初启脉冲神经网络
- [ ] High-parallelism Inception-like Spiking Neural Networks for Unsupervised Feature Learning 
时间：2021年03月09日                         第一作者：Mingyuan Meng                       [链接](https://arxiv.org/abs/2001.01680).                     
<details>	<summary>注释</summary>	Published at Neurocomputing DOI: 10.1016/j.neucom.2021.02.027 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 72、基于在线进化脉冲神经网络的流数据无监督异常检测
- [ ] Unsupervised Anomaly Detection in Stream Data with Online Evolving Spiking Neural Networks 
时间：2021年03月08日                         第一作者：Piotr S. Maci\k{a}g (1)                       [链接](https://arxiv.org/abs/1912.08785).                     
<details>	<summary>注释</summary>	52 pages Journal-ref: Neural Networks, Volume 139, 2021, Pages 118-139 DOI: 10.1016/j.neunet.2021.02.017 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 71、一点点能量就有很大的帮助：高效节能，从卷积神经网络到脉冲神经网络的精确转换
- [ ] A Little Energy Goes a Long Way: Energy-Efficient, Accurate Conversion from Convolutional Neural Networks to Spiking Neural Networks 
时间：2021年03月06日                         第一作者：Dengyu Wu                       [链接](https://arxiv.org/abs/2103.00944).                     
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 70、神经形态平台上强化学习的双记忆结构
- [ ] A Dual-Memory Architecture for Reinforcement Learning on Neuromorphic Platforms 
时间：2021年03月05日                         第一作者：Wilkie Olin-Ammentorp                       [链接](https://arxiv.org/abs/2103.04780).                     
## 摘要：强化学习（RL）是生物系统中学习的基础，并提供了一个框架来解决现实世界人工智能应用的众多挑战。RL技术的有效实现允许部署在边缘用例中的代理获得新的能力，例如改进的导航、理解复杂情况和关键决策。为了实现这个目标，我们描述了一个灵活的架构来在神经形态平台上进行强化学习。该体系结构是使用Intel神经形态处理器实现的，并演示了如何使用脉冲动力学解决各种任务。我们的研究为现实世界的RL应用提出了一个可用的节能解决方案，并证明了神经形态平台对RL问题的适用性。
<details>	<summary>英文摘要</summary>	Reinforcement learning (RL) is a foundation of learning in biological systems and provides a framework to address numerous challenges with real-world artificial intelligence applications. Efficient implementations of RL techniques could allow for agents deployed in edge-use cases to gain novel abilities, such as improved navigation, understanding complex situations and critical decision making. Towards this goal, we describe a flexible architecture to carry out reinforcement learning on neuromorphic platforms. This architecture was implemented using an Intel neuromorphic processor and demonstrated solving a variety of tasks using spiking dynamics. Our study proposes a usable energy efficient solution for real-world RL applications and demonstrates applicability of the neuromorphic platforms for RL problems. </details>
<details>	<summary>注释</summary>	20 pages, 6 figures ACM-class: I.2 </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 69、基于在线元学习的脉冲神经网络快速设备自适应
- [ ] Fast On-Device Adaptation for Spiking Neural Networks via Online-Within-Online Meta-Learning 
时间：2021年02月21日                         第一作者：Bleema Rosenfeld                       [链接](https://arxiv.org/abs/2103.03901).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）由于其低功耗特性，近年来在移动医疗管理和自然语言处理等应用中作为边缘智能的机器学习模型得到了广泛的应用。在这种高度个人化的用例中，模型必须能够用最少的训练数据来适应个体的独特特征。元学习被认为是一种训练模型的方法，这种模型能够快速适应新的任务。为数不多的现有snn元学习解决方案离线运行，并且需要某种形式的反向传播，这与当前的神经形态边缘设备不兼容。在这篇论文中，我们提出了一个SNN的在线内在线元学习规则OWOML-SNN，它能够在任务流上实现终身学习，并且依赖于本地的、无后台的、嵌套的更新。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have recently gained popularity as machine learning models for on-device edge intelligence for applications such as mobile healthcare management and natural language processing due to their low power profile. In such highly personalized use cases, it is important for the model to be able to adapt to the unique features of an individual with only a minimal amount of training data. Meta-learning has been proposed as a way to train models that are geared towards quick adaptation to new tasks. The few existing meta-learning solutions for SNNs operate offline and require some form of backpropagation that is incompatible with the current neuromorphic edge-devices. In this paper, we propose an online-within-online meta-learning rule for SNNs termed OWOML-SNN, that enables lifelong learning on a stream of tasks, and relies on local, backprop-free, nested updates. </details>
<details>	<summary>注释</summary>	Accepted for publication at DSLW 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 68、机器人神经形态感知工具箱
- [ ] A toolbox for neuromorphic sensing in robotics 
时间：2021年03月03日                         第一作者：Julien Dupeyroux                       [链接](https://arxiv.org/abs/2103.02751).                     
## 摘要：由神经形态计算引入的第三代人工智能（AI）正在彻底改变机器人和自主系统感知世界、处理信息以及与环境交互的方式。神经形态系统的高灵活性、能量效率和鲁棒性的承诺得到了模拟脉冲神经网络的软件工具和硬件集成（神经形态处理器）的广泛支持。然而，尽管人们在神经形态视觉（基于事件的摄像机）方面做出了努力，但值得注意的是，大多数机器人可用的传感器与神经形态计算（信息被编码成脉冲）本质上仍然不兼容。为了方便传统传感器的使用，我们需要将输出信号转换成脉冲流，即一系列事件（+1，-1）及其相应的时间戳。在这篇论文中，我们从机器人学的角度对编码算法进行了回顾，并进一步通过一个基准测试来评估它们的性能。我们还引入了ROS（机器人操作系统）工具箱来编码和解码来自机器人上任何类型传感器的输入信号。这项倡议旨在刺激和促进神经形态人工智能的机器人集成，并有机会使传统的现成传感器适应最强大的机器人工具之一ROS中的脉冲神经网络。
<details>	<summary>英文摘要</summary>	The third generation of artificial intelligence (AI) introduced by neuromorphic computing is revolutionizing the way robots and autonomous systems can sense the world, process the information, and interact with their environment. The promises of high flexibility, energy efficiency, and robustness of neuromorphic systems is widely supported by software tools for simulating spiking neural networks, and hardware integration (neuromorphic processors). Yet, while efforts have been made on neuromorphic vision (event-based cameras), it is worth noting that most of the sensors available for robotics remain inherently incompatible with neuromorphic computing, where information is encoded into spikes. To facilitate the use of traditional sensors, we need to convert the output signals into streams of spikes, i.e., a series of events (+1, -1) along with their corresponding timestamps. In this paper, we propose a review of the coding algorithms from a robotics perspective and further supported by a benchmark to assess their performance. We also introduce a ROS (Robot Operating System) toolbox to encode and decode input signals coming from any type of sensor available on a robot. This initiative is meant to stimulate and facilitate robotic integration of neuromorphic AI, with the opportunity to adapt traditional off-the-shelf sensors to spiking neural nets within one of the most powerful robotic tools, ROS. </details>
<details>	<summary>注释</summary>	7 pages, 3 figures, 3 tables, 7 algorithms </details>
<details>	<summary>邮件日期</summary>	2021年03月05日</details>

# 67、基于事件的合成孔径成像
- [ ] Event-based Synthetic Aperture Imaging 
时间：2021年03月03日                         第一作者：Xiang Zhang                       [链接](https://arxiv.org/abs/2103.02376).                     
## 摘要：合成孔径成像（syntheticapertureimaging，SAI）是通过模糊掉离焦的前景遮挡，并从多视点图像中重建出在焦遮挡的目标，从而达到透视效果。然而，非常密集的遮挡和极端的光照条件可能会对基于传统帧相机的SAI带来显著的干扰，导致性能退化。为了解决这些问题，我们提出了一种基于事件摄像机的SAI系统，它可以产生具有极低延迟和高动态范围的异步事件。因此，它可以通过几乎连续的视图来消除密集遮挡的干扰，同时解决过度/不足曝光的问题。为了重建遮挡目标，提出了一种由脉冲神经网络（SNNs）和卷积神经网络（CNNs）组成的混合编解码网络。在混合网络中，首先对采集到的事件的时空信息进行SNN层编码，然后通过样式转换CNN解码器将其转换为遮挡目标的视觉图像。实验结果表明，该方法在处理高密度遮挡和极端光照条件下具有良好的性能，可以用纯事件数据重建高质量的视觉图像。
<details>	<summary>英文摘要</summary>	Synthetic aperture imaging (SAI) is able to achieve the see through effect by blurring out the off-focus foreground occlusions and reconstructing the in-focus occluded targets from multi-view images. However, very dense occlusions and extreme lighting conditions may bring significant disturbances to SAI based on conventional frame-based cameras, leading to performance degeneration. To address these problems, we propose a novel SAI system based on the event camera which can produce asynchronous events with extremely low latency and high dynamic range. Thus, it can eliminate the interference of dense occlusions by measuring with almost continuous views, and simultaneously tackle the over/under exposure problems. To reconstruct the occluded targets, we propose a hybrid encoder-decoder network composed of spiking neural networks (SNNs) and convolutional neural networks (CNNs). In the hybrid network, the spatio-temporal information of the collected events is first encoded by SNN layers, and then transformed to the visual image of the occluded targets by a style-transfer CNN decoder. Through experiments, the proposed method shows remarkable performance in dealing with very dense occlusions and extreme lighting conditions, and high quality visual images can be reconstructed using pure event data. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月04日</details>

# 66、一点点能量就有很大的帮助：高效节能，从卷积神经网络到脉冲神经网络的精确转换
- [ ] A Little Energy Goes a Long Way: Energy-Efficient, Accurate Conversion from Convolutional Neural Networks to Spiking Neural Networks 
时间：2021年03月01日                         第一作者：Dengyu Wu                       [链接](https://arxiv.org/abs/2103.00944).                     
## 摘要：脉冲神经网络（SNNs）提供了一种处理时空数据的固有能力，也就是说，处理现实世界中的感官数据，但是很难训练出高精度的模型。SNN的一个主要研究方向是将预先训练好的卷积神经网络（CNN）转换成具有相同结构的SNN。最先进的转换方法正在接近精度极限，即SNN对原始CNN的精度损失接近于零。然而，我们注意到，只有当处理输入消耗的能量显著增加时，这才有可能实现。在本文中，我们认为这种“能量换精度”的趋势是不必要的——一点点能量可以大大提高精度损失的接近零。具体来说，我们提出了一种新的CNN到SNN转换方法，该方法能够使用合理的短脉冲序列（例如，CIFAR10图像的256个时间步）来实现接近零的精度损失。新的转换方法称为显式电流控制（ECC），包含三种技术（电流归一化、残差消除阈值和批量归一化一致性维护），以便在处理输入时显式控制流经SNN的电流。我们将ECC实现到一个昵称为SpKeras的工具中，该工具可以方便地导入Keras-CNN模型并将其转换为snn。我们使用该工具进行了一系列广泛的实验——使用VGG16和各种数据集，如CIFAR10和CIFAR100——并与最先进的转换方法进行了比较。结果表明，ECC是一种很有前途的方法，它可以同时优化系统的能耗和精度损失。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) offer an inherent ability to process spatial-temporal data, or in other words, realworld sensory data, but suffer from the difficulty of training high accuracy models. A major thread of research on SNNs is on converting a pre-trained convolutional neural network (CNN) to an SNN of the same structure. State-of-the-art conversion methods are approaching the accuracy limit, i.e., the near-zero accuracy loss of SNN against the original CNN. However, we note that this is made possible only when significantly more energy is consumed to process an input. In this paper, we argue that this trend of ''energy for accuracy'' is not necessary -- a little energy can go a long way to achieve the near-zero accuracy loss. Specifically, we propose a novel CNN-to-SNN conversion method that is able to use a reasonably short spike train (e.g., 256 timesteps for CIFAR10 images) to achieve the near-zero accuracy loss. The new conversion method, named as explicit current control (ECC), contains three techniques (current normalisation, thresholding for residual elimination, and consistency maintenance for batch-normalisation), in order to explicitly control the currents flowing through the SNN when processing inputs. We implement ECC into a tool nicknamed SpKeras, which can conveniently import Keras CNN models and convert them into SNNs. We conduct an extensive set of experiments with the tool -- working with VGG16 and various datasets such as CIFAR10 and CIFAR100 -- and compare with state-of-the-art conversion methods. Results show that ECC is a promising method that can optimise over energy consumption and accuracy loss simultaneously. </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 65、SpikeDyn：一种动态环境下具有连续无监督学习能力的节能型Spiking神经网络框架
- [ ] SpikeDyn: A Framework for Energy-Efficient Spiking Neural Networks with Continual and Unsupervised Learning Capabilities in Dynamic Environments 
时间：2021年02月28日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2103.00424).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）由于其生物学上的合理性，具有高效无监督和持续学习能力的潜力，但其复杂性仍然是一个严重的研究挑战，以使其能够针对资源受限的场景（如嵌入式系统、物联网边缘等）进行节能设计。我们提出了SpikeDyn，一个在动态环境中具有连续和无监督学习能力的节能snn的综合框架，用于训练和推理阶段。它是通过以下多种不同的机制实现的：1）减少神经元的操作，用直接的侧抑制代替抑制神经元；2）一种记忆和能量受限的SNN模型搜索算法，该算法利用分析模型来估计不同候选SNN的记忆足迹和能量消耗建立并选择一个Pareto最优SNN模型；3）一个轻量级的连续无监督学习算法，采用自适应学习率、自适应膜阈值电位、权值衰减和减少虚假更新。实验结果表明，对于一个由400个兴奋神经元组成的网络，我们的SpikeDyn在训练和推理方面的平均能耗分别比现有的方法降低了51%和37%。由于改进的学习算法，SpikeDyn对最近学习的任务进行分类，平均比最新技术提高了21%，对以前学习的任务平均提高了8%。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) bear the potential of efficient unsupervised and continual learning capabilities because of their biological plausibility, but their complexity still poses a serious research challenge to enable their energy-efficient design for resource-constrained scenarios (like embedded systems, IoT-Edge, etc.). We propose SpikeDyn, a comprehensive framework for energy-efficient SNNs with continual and unsupervised learning capabilities in dynamic environments, for both the training and inference phases. It is achieved through the following multiple diverse mechanisms: 1) reduction of neuronal operations, by replacing the inhibitory neurons with direct lateral inhibitions; 2) a memory- and energy-constrained SNN model search algorithm that employs analytical models to estimate the memory footprint and energy consumption of different candidate SNN models and selects a Pareto-optimal SNN model; and 3) a lightweight continual and unsupervised learning algorithm that employs adaptive learning rates, adaptive membrane threshold potential, weight decay, and reduction of spurious updates. Our experimental results show that, for a network with 400 excitatory neurons, our SpikeDyn reduces the energy consumption on average by 51% for training and by 37% for inference, as compared to the state-of-the-art. Due to the improved learning algorithm, SpikeDyn provides on avg. 21% accuracy improvement over the state-of-the-art, for classifying the most recently learned task, and by 8% on average for the previously learned tasks. </details>
<details>	<summary>注释</summary>	To appear at the 58th IEEE/ACM Design Automation Conference (DAC), December 2021, San Francisco, CA, USA </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 64、传统人工神经网络到脉冲神经网络的优化转换
- [ ] Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks 
时间：2021年02月28日                         第一作者：Shikuang Deng                       [链接](https://arxiv.org/abs/2103.00476).                     
## 摘要：脉冲神经网络（SNNs）是一种受生物启发的人工神经网络（ANNs），由脉冲神经元组成，用于处理异步离散信号。由于snn的离散性，使得snn在功耗和推理速度上都有很大的提高，但通常很难直接从零开始训练。另一种方法是通过复制神经网络的权值和调整snn中神经元的峰值阈值电位，将传统的ann转化为snn。研究人员设计了新的SNN结构和转换算法来减小转换误差。然而，一个有效的转换应该解决SNN和ANN体系结构之间的差异，用一个有效的损失函数的近似值，这个函数在这个领域是缺失的。在这项工作中，我们分析了递归归约到分层求和的转换误差，并提出了一种新的策略管道，通过结合阈值平衡和软重置机制将权值转移到目标SNN。这种流水线使得转换后的SNN和传统的ann之间几乎没有精度损失，只有典型SNN模拟时间的$\sim1/10$。我们的方法有望在能量和内存有限的情况下，更好地支持SNNs，并将其移植到嵌入式平台上。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are biology-inspired artificial neural networks (ANNs) that comprise of spiking neurons to process asynchronous discrete signals. While more efficient in power consumption and inference speed on the neuromorphic hardware, SNNs are usually difficult to train directly from scratch with spikes due to the discreteness. As an alternative, many efforts have been devoted to converting conventional ANNs into SNNs by copying the weights from ANNs and adjusting the spiking threshold potential of neurons in SNNs. Researchers have designed new SNN architectures and conversion algorithms to diminish the conversion error. However, an effective conversion should address the difference between the SNN and ANN architectures with an efficient approximation \DSK{of} the loss function, which is missing in the field. In this work, we analyze the conversion error by recursive reduction to layer-wise summation and propose a novel strategic pipeline that transfers the weights to the target SNN by combining threshold balance and soft-reset mechanisms. This pipeline enables almost no accuracy loss between the converted SNNs and conventional ANNs with only $\sim1/10$ of the typical SNN simulation time. Our method is promising to get implanted onto embedded platforms with better support of SNNs with limited energy and memory. </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 63、SparkXD：一种基于近似DRAM的弹性高效脉冲神经网络推理框架
- [ ] SparkXD: A Framework for Resilient and Energy-Efficient Spiking Neural Network Inference using Approximate DRAM 
时间：2021年02月28日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2103.00421).                     
## 摘要：脉冲神经网络（SNNs）由于其生物稀疏性，具有实现低能耗的潜力。一些研究表明，片外存储器（DRAM）访问是SNN处理中最消耗能量的操作。然而，SNN系统中的最新技术并没有优化每个访问的DRAM能量，因此阻碍了实现高能量效率。为了最大限度地减少每次存取的DRAM能量，一个按键旋钮用于降低DRAM电源电压，但这可能会导致DRAM错误（即所谓的近似DRAM）。针对这一点，我们提出了SparkXD，这是一个新的框架，它提供了一个综合的联合解决方案，用于使用低功耗dram在电压引起的错误下进行弹性和节能SNN推断。SparkXD的关键机制是：（1）通过考虑近似DRAM误码的容错训练来提高SNN的容错性；（2）分析改进的SNN模型的容错性，找到满足目标精度约束的最大可容忍误码率；（3）能量有效的DRAM数据映射对于弹性SNN模型，该模型将权重映射到适当的DRAM位置以最小化DRAM访问能量。通过这些机制，SparkXD减轻了DRAM（近似）错误的负面影响，并提供了所需的精度。实验结果表明，当目标精度在基线设计的1%以内（即SNN没有DRAM错误）时，SparkXD在不同网络规模下平均降低DRAM能量约40%。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have the potential for achieving low energy consumption due to their biologically sparse computation. Several studies have shown that the off-chip memory (DRAM) accesses are the most energy-consuming operations in SNN processing. However, state-of-the-art in SNN systems do not optimize the DRAM energy-per-access, thereby hindering achieving high energy-efficiency. To substantially minimize the DRAM energy-per-access, a key knob is to reduce the DRAM supply voltage but this may lead to DRAM errors (i.e., the so-called approximate DRAM). Towards this, we propose SparkXD, a novel framework that provides a comprehensive conjoint solution for resilient and energy-efficient SNN inference using low-power DRAMs subjected to voltage-induced errors. The key mechanisms of SparkXD are: (1) improving the SNN error tolerance through fault-aware training that considers bit errors from approximate DRAM, (2) analyzing the error tolerance of the improved SNN model to find the maximum tolerable bit error rate (BER) that meets the targeted accuracy constraint, and (3) energy-efficient DRAM data mapping for the resilient SNN model that maps the weights in the appropriate DRAM location to minimize the DRAM access energy. Through these mechanisms, SparkXD mitigates the negative impact of DRAM (approximation) errors, and provides the required accuracy. The experimental results show that, for a target accuracy within 1% of the baseline design (i.e., SNN without DRAM errors), SparkXD reduces the DRAM energy by ca. 40% on average across different network sizes. </details>
<details>	<summary>注释</summary>	To appear at the 58th IEEE/ACM Design Automation Conference (DAC), December 2021, San Francisco, CA, USA </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 62、结合脉冲神经网络和人工神经网络的图像增强分类
- [ ] Combining Spiking Neural Network and Artificial Neural Network for Enhanced Image Classification 
时间：2021年02月28日                         第一作者：Naoya Muramatsu                        [链接](https://arxiv.org/abs/2102.10592).                     
<details>	<summary>注释</summary>	This paper written for DEIM 2021 (https://db-event.jpn.org/deim2021/) has 12 pages, 6 figures and 3 tables MSC-class: 68T05 (Primary) 68T05 (Secondary) ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 61、癫痫发作预测的神经形态计算新方法
- [ ] A New Neuromorphic Computing Approach for Epileptic Seizure Prediction 
时间：2021年02月25日                         第一作者：Fengshi Tian                       [链接](https://arxiv.org/abs/2102.12773).                     
## 摘要：报道了几种利用卷积神经网络（CNNs）预测癫痫发作的高特异性和敏感性方法。然而，cnn的计算成本很高，耗电量也很大。这些不便使得基于CNN的方法很难在可穿戴设备上实现。基于能量有效的脉冲神经网络（SNNs），提出了一种用于癫痫发作预测的神经形态计算方法。该方法利用设计的高斯随机离散编码器从脑电样本中产生脉冲序列，并在结合了CNNs和SNNs优点的脉冲卷积神经网络（spiking-convolutional neural network，spiking-CNN）中进行预测。实验结果表明，该方法的灵敏度、特异性和AUC分别为95.1%、99.2%和0.912%，计算复杂度比CNN降低了98.58%，表明该方法硬件友好，精度高。
<details>	<summary>英文摘要</summary>	Several high specificity and sensitivity seizure prediction methods with convolutional neural networks (CNNs) are reported. However, CNNs are computationally expensive and power hungry. These inconveniences make CNN-based methods hard to be implemented on wearable devices. Motivated by the energy-efficient spiking neural networks (SNNs), a neuromorphic computing approach for seizure prediction is proposed in this work. This approach uses a designed gaussian random discrete encoder to generate spike sequences from the EEG samples and make predictions in a spiking convolutional neural network (Spiking-CNN) which combines the advantages of CNNs and SNNs. The experimental results show that the sensitivity, specificity and AUC can remain 95.1%, 99.2% and 0.912 respectively while the computation complexity is reduced by 98.58% compared to CNN, indicating that the proposed Spiking-CNN is hardware friendly and of high precision. </details>
<details>	<summary>注释</summary>	Accepted to 2021 IEEE International Symposium on Circuits and Systems (ISCAS) Journal-ref: 2021 IEEE International Symposium on Circuits and Systems (ISCAS) </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 60、皮层振荡在脉冲神经网络中实现了基于采样的计算
- [ ] Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks 
时间：2021年02月23日                         第一作者：Agnes Korcsak-Gorzo                       [链接](https://arxiv.org/abs/2006.11099).                     
<details>	<summary>注释</summary>	28 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 59、STDP通过反向传播增强脉冲神经网络的学习
- [ ] STDP enhances learning by backpropagation in a spiking neural network 
时间：2021年02月21日                         第一作者：Kotaro Furuya                        [链接](https://arxiv.org/abs/2102.10530).                     
## 摘要：提出了一种用于脉冲神经网络的半监督学习方法。该方法由反向传播的有监督学习和脉冲时间依赖可塑性（STDP）的无监督学习组成，STDP是一种生物学上合理的学习规则。数值实验表明，在使用少量标记数据的情况下，该方法在不增加标记的情况下提高了精度。现有的判别模型半监督学习方法还没有实现这一特性。对于事件驱动系统，可以实现所提出的学习方法。因此，如果在神经形态硬件上实现，它在实时性问题上会非常高效。结果表明，STDP在监督学习后的应用中除了自组织外，还起着重要的作用，这不同于以往将STDP作为预训练解释为自组织的方法。
<details>	<summary>英文摘要</summary>	A semi-supervised learning method for spiking neural networks is proposed. The proposed method consists of supervised learning by backpropagation and subsequent unsupervised learning by spike-timing-dependent plasticity (STDP), which is a biologically plausible learning rule. Numerical experiments show that the proposed method improves the accuracy without additional labeling when a small amount of labeled data is used. This feature has not been achieved by existing semi-supervised learning methods of discriminative models. It is possible to implement the proposed learning method for event-driven systems. Hence, it would be highly efficient in real-time problems if it were implemented on neuromorphic hardware. The results suggest that STDP plays an important role other than self-organization when applied after supervised learning, which differs from the previous method of using STDP as pre-training interpreted as self-organization. </details>
<details>	<summary>注释</summary>	9 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 58、结合脉冲神经网络和人工神经网络的图像增强分类
- [ ] Combining Spiking Neural Network and Artificial Neural Network for Enhanced Image Classification 
时间：2021年02月21日                         第一作者：Naoya Muramatsu                        [链接](https://arxiv.org/abs/2102.10592).                     
## 摘要：随着深度神经网络的不断创新，更接近生物脑突触的脉冲神经网络（spiking neural networks，SNNs）因其低功耗而备受关注。然而，对于连续数据值，它们必须采用编码过程将值转换为峰值序列。因此，它们还没有超过直接处理这些值的人工神经网络（ANNs）的性能。为此，我们将人工神经网络和神经网络相结合，建立了多功能混合神经网络（HNNs），以提高相关性能。
<details>	<summary>英文摘要</summary>	With the continued innovations of deep neural networks, spiking neural networks (SNNs) that more closely resemble biological brain synapses have attracted attention owing to their low power consumption. However, for continuous data values, they must employ a coding process to convert the values to spike trains. Thus, they have not yet exceeded the performance of artificial neural networks (ANNs), which handle such values directly. To this end, we combine an ANN and an SNN to build versatile hybrid neural networks (HNNs) that improve the concerned performance. </details>
<details>	<summary>注释</summary>	This paper written for DEIM 2021 (https://db-event.jpn.org/deim2021/) has 12 pages, 6 figures and 3 tables MSC-class: 68T05 (Primary) 68T05 (Secondary) ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 57、脉冲神经形态芯片学习纠缠量子态
- [ ] Spiking neuromorphic chip learns entangled quantum states 
时间：2021年02月18日                         第一作者：Stefanie Czischek                       [链接](https://arxiv.org/abs/2008.01039).                     
<details>	<summary>注释</summary>	21 pages, 6 figures Submission to SciPost </details>
<details>	<summary>邮件日期</summary>	2021年02月22日</details>

# 56、方程：神经形态实现的脉冲驱动平衡传播
- [ ] EqSpike: Spike-driven Equilibrium Propagation for Neuromorphic Implementations 
时间：2021年02月17日                         第一作者：Erwann Martin                       [链接](https://arxiv.org/abs/2010.07859).                     
<details>	<summary>邮件日期</summary>	2021年02月18日</details>

# 55、阴阳数据集
- [ ] The Yin-Yang dataset 
时间：2021年02月16日                         第一作者：Laura Kriener                       [链接](https://arxiv.org/abs/2102.08211).                     
## 摘要：阴阳数据集是为研究生物似然误差反向传播和脉冲神经网络的深度学习而开发的。它提供了一些优势，可以替代经典的深度学习数据集，特别是在算法和模型原型场景中。首先，它体积更小，因此学习速度更快，因此更适合部署在网络规模有限的神经形态芯片上。第二，与深度神经网络相比，它在使用浅层神经网络所能达到的精度之间存在着非常明显的差距。
<details>	<summary>英文摘要</summary>	The Yin-Yang dataset was developed for research on biologically plausible error backpropagation and deep learning in spiking neural networks. It serves as an alternative to classic deep learning datasets, especially in algorithm- and model-prototyping scenarios, by providing several advantages. First, it is smaller and therefore faster to learn, thereby being better suited for the deployment on neuromorphic chips with limited network sizes. Second, it exhibits a very clear gap between the accuracies achievable using shallow as compared to deep neural networks. </details>
<details>	<summary>注释</summary>	3 pages, 3 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2021年02月17日</details>

# 54、用GANs归化神经形态视觉事件流
- [ ] Naturalizing Neuromorphic Vision Event Streams Using GANs 
时间：2021年02月14日                         第一作者：Dennis Robey                       [链接](https://arxiv.org/abs/2102.07243).                     
## 摘要：动态视觉传感器能够在资源受限的环境中以高时间分辨率工作，但代价是捕获静态内容。事件流的稀疏特性使得下游处理任务更高效，因为它们适合于功率高效的脉冲神经网络。与神经形态视觉相关的挑战之一是缺乏事件流的可解释性。虽然大多数应用用例并不打算让事件流被除分类网络之外的任何东西直观地解释，但是在传统高速CMOS传感器无法到达的空间中集成这些传感器的机会将丢失。例如，像内窥镜这样的生物入侵传感器必须符合严格的电源预算，这就不允许以兆赫的速度进行图像集成。虽然动态视觉传感可以填补这一空白，解释的挑战仍然存在，并将降低临床诊断的信心。产生式对抗网络的使用为克服和补偿视觉芯片空间分辨率差和缺乏可解释性提供了一种可能的解决方案。本文系统地应用Pix2Pix网络对CIFAR-10和linnaeus5数据集的事件流进行自然化处理。通过对归化的事件流进行图像分类（其收敛到等效原始图像的2.81%以内），并对CIFAR-10和Linnaeus 5数据集的未处理事件流进行13.19%的相关改进，对网络的质量进行了基准测试。
<details>	<summary>英文摘要</summary>	Dynamic vision sensors are able to operate at high temporal resolutions within resource constrained environments, though at the expense of capturing static content. The sparse nature of event streams enables efficient downstream processing tasks as they are suited for power-efficient spiking neural networks. One of the challenges associated with neuromorphic vision is the lack of interpretability of event streams. While most application use-cases do not intend for the event stream to be visually interpreted by anything other than a classification network, there is a lost opportunity to integrating these sensors in spaces that conventional high-speed CMOS sensors cannot go. For example, biologically invasive sensors such as endoscopes must fit within stringent power budgets, which do not allow MHz-speeds of image integration. While dynamic vision sensing can fill this void, the interpretation challenge remains and will degrade confidence in clinical diagnostics. The use of generative adversarial networks presents a possible solution to overcoming and compensating for a vision chip's poor spatial resolution and lack of interpretability. In this paper, we methodically apply the Pix2Pix network to naturalize the event stream from spike-converted CIFAR-10 and Linnaeus 5 datasets. The quality of the network is benchmarked by performing image classification of naturalized event streams, which converges to within 2.81% of equivalent raw images, and an associated improvement over unprocessed event streams by 13.19% for the CIFAR-10 and Linnaeus 5 datasets. </details>
<details>	<summary>注释</summary>	5 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年02月16日</details>

# 53、利用混合信号脉冲学习电路实现高效均衡网络
- [ ] Implementing efficient balanced networks with mixed-signal spike-based learning circuits 
时间：2021年02月12日                         第一作者：Julian B\"uchel                       [链接](https://arxiv.org/abs/2010.14353).                     
<details>	<summary>注释</summary>	5 pages, 6 figures. Accepted at IEEE International Symposium on Circuits and Systems 2021 </details>
<details>	<summary>邮件日期</summary>	2021年02月15日</details>

# 52、基于广义期望最大化的脉冲神经网络多样本在线学习
- [ ] Multi-Sample Online Learning for Spiking Neural Networks based on Generalized Expectation Maximization 
时间：2021年02月05日                         第一作者：Hyeryung Jang                        [链接](https://arxiv.org/abs/2102.03280).                     
## 摘要：脉冲神经网络（SNNs）提供了一种新的计算范式，它通过二元神经动态激活来获取生物大脑的一些效率。概率SNN模型通常通过使用对数似然梯度的无偏估计来训练以最大化期望输出的可能性。虽然先前的工作使用单样本估计器从一次运行的网络，本文提出利用多个隔间采样独立的脉冲信号，同时共享突触权重。其关键思想是利用这些信号来获得对数似然训练准则及其梯度的更精确的统计估计。该方法基于广义期望最大化（GEM），利用重要性抽样优化了对数似然的近似。导出的在线学习算法实现了一个具有全局每隔室学习信号的三因素规则。在神经形态MNIST-DVS数据集上的分类任务的实验结果表明，当增加用于训练和推理的隔室数量时，在对数似然性、准确性和校准方面有显著的改进。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) offer a novel computational paradigm that captures some of the efficiency of biological brains by processing through binary neural dynamic activations. Probabilistic SNN models are typically trained to maximize the likelihood of the desired outputs by using unbiased estimates of the log-likelihood gradients. While prior work used single-sample estimators obtained from a single run of the network, this paper proposes to leverage multiple compartments that sample independent spiking signals while sharing synaptic weights. The key idea is to use these signals to obtain more accurate statistical estimates of the log-likelihood training criterion, as well as of its gradient. The approach is based on generalized expectation-maximization (GEM), which optimizes a tighter approximation of the log-likelihood using importance sampling. The derived online learning algorithm implements a three-factor rule with global per-compartment learning signals. Experimental results on a classification task on the neuromorphic MNIST-DVS data set demonstrate significant improvements in terms of log-likelihood, accuracy, and calibration when increasing the number of compartments used for training and inference. </details>
<details>	<summary>注释</summary>	To be presented at ICASSP 2021. Author's Accepted Manuscript. (A longer version can be found at arXiv:2007.11894), Author's Accepted Manuscript. arXiv admin note: text overlap with arXiv:2007.11894 </details>
<details>	<summary>邮件日期</summary>	2021年02月08日</details>

# 51、优化的脉冲神经元通过双峰时间编码对图像进行高精度分类
- [ ] Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes 
时间：2021年01月26日                         第一作者：Christoph St\"ockl                        [链接](https://arxiv.org/abs/2002.00860).                     
<details>	<summary>注释</summary>	23 pages, 5 figures, 1 tables </details>
<details>	<summary>邮件日期</summary>	2021年01月27日</details>

# 50、一种基于Loihi神经形态处理器的DVS摄像机手势识别算法
- [ ] An Efficient Spiking Neural Network for Recognizing Gestures with a DVS Camera on the Loihi Neuromorphic Processor 
时间：2021年01月25日                         第一作者：Riccardo Massa                       [链接](https://arxiv.org/abs/2006.09985).                     
<details>	<summary>注释</summary>	Accepted for publication at the 2020 International Joint Conference on Neural Networks (IJCNN) </details>
<details>	<summary>邮件日期</summary>	2021年01月26日</details>

# 49、腿型机器人仿生运动的神经形态自适应脉冲CPG
- [ ] Neuromorphic adaptive spiking CPG towards bio-inspired locomotion of legged robots 
时间：2021年01月24日                         第一作者：Pablo Lopez-Osorio                       [链接](https://arxiv.org/abs/2101.09709).                     
## 摘要：近年来，脊椎动物的运动机制为机器人系统性能的提高提供了灵感。这些机制包括它们的运动对通过生物传感器记录的环境变化的适应性。在这方面，我们的目标是复制这种适应性的腿机器人通过一个脉冲中心模式发生器。这种脉冲中心模式发生器产生不同的运动（节奏）模式，这些模式由外部刺激驱动，即连接到机器人的力敏感电阻器的输出，以提供反馈。脉冲中枢模式发生器由五个漏神经元群组成，这些漏神经元群具有特定的拓扑结构，使得节律模式可以由上述外部刺激产生和驱动。因此，末端机器人平台（任意腿机器人）的运动可以通过使用任意传感器作为输入来适应地形。采用brian2模拟器和SpiNNaker神经形态平台，对具有自适应学习的脉冲中心模式发生器进行了软硬件仿真验证。特别是，我们的实验清楚地表明，当输入刺激不同时，脉冲中央模式发生器群体中产生的脉冲之间的振荡频率发生了适应性变化。为了验证脉冲中心模式发生器的鲁棒性和适应性，我们通过改变传感器的输出进行了多次测试。这些实验在brian2和SpiNNaker中进行；两个实现都显示出相似的行为，Pearson相关系数为0.905。
<details>	<summary>英文摘要</summary>	In recent years, locomotion mechanisms exhibited by vertebrate animals have been the inspiration for the improvement in the performance of robotic systems. These mechanisms include the adaptability of their locomotion to any change registered in the environment through their biological sensors. In this regard, we aim to replicate such kind of adaptability in legged robots through a Spiking Central Pattern Generator. This Spiking Central Pattern Generator generates different locomotion (rhythmic) patterns which are driven by an external stimulus, that is, the output of a Force Sensitive Resistor connected to the robot to provide feedback. The Spiking Central Pattern Generator consists of a network of five populations of Leaky Integrate-and-Fire neurons designed with a specific topology in such a way that the rhythmic patterns can be generated and driven by the aforementioned external stimulus. Therefore, the locomotion of the end robotic platform (any-legged robot) can be adapted to the terrain by using any sensor as input. The Spiking Central Pattern Generator with adaptive learning has been numerically validated at software and hardware level, using the Brian 2 simulator and the SpiNNaker neuromorphic platform for the latest. In particular, our experiments clearly show an adaptation in the oscillation frequencies between the spikes produced in the populations of the Spiking Central Pattern Generator while the input stimulus varies. To validate the robustness and adaptability of the Spiking Central Pattern Generator, we have performed several tests by variating the output of the sensor. These experiments were carried out in Brian 2 and SpiNNaker; both implementations showed a similar behavior with a Pearson correlation coefficient of 0.905. </details>
<details>	<summary>注释</summary>	23 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年01月26日</details>

# 48、事件驱动目标识别的脉冲学习系统
- [ ] A Spike Learning System for Event-driven Object Recognition 
时间：2021年01月21日                         第一作者：Shibo Zhou                       [链接](https://arxiv.org/abs/2101.08850).                     
## 摘要：事件驱动传感器，如激光雷达和动态视觉传感器（DVS）在高分辨率和高速应用中受到越来越多的关注。为了提高识别精度，人们做了大量的工作。然而，对于识别延迟或时间效率这一基本问题的研究还远远不够。在本文中，我们提出了一个脉冲学习系统，该系统使用脉冲神经网络（SNN）和一种新的时态编码来实现精确快速的目标识别。提出的时态编码方案将每个事件的到达时间和数据映射到SNN脉冲时间，使得异步到达的事件立即得到处理而没有延迟。该方案很好地结合了SNN的异步处理能力，提高了时间效率。与现有系统相比的一个关键优势是，每个识别任务的事件累积时间由系统自动确定，而不是由用户预先设置。系统可以在不等待所有输入事件的情况下提前完成识别。在7个激光雷达和DVS数据集上进行了广泛的实验。结果表明，该系统在取得显著时间效率的同时，具有最先进的识别精度。实验结果表明，在不同的实验条件下，在KITTI数据集上，识别延迟降低了56.3%-91.7%。
<details>	<summary>英文摘要</summary>	Event-driven sensors such as LiDAR and dynamic vision sensor (DVS) have found increased attention in high-resolution and high-speed applications. A lot of work has been conducted to enhance recognition accuracy. However, the essential topic of recognition delay or time efficiency is largely under-explored. In this paper, we present a spiking learning system that uses the spiking neural network (SNN) with a novel temporal coding for accurate and fast object recognition. The proposed temporal coding scheme maps each event's arrival time and data into SNN spike time so that asynchronously-arrived events are processed immediately without delay. The scheme is integrated nicely with the SNN's asynchronous processing capability to enhance time efficiency. A key advantage over existing systems is that the event accumulation time for each recognition task is determined automatically by the system rather than pre-set by the user. The system can finish recognition early without waiting for all the input events. Extensive experiments were conducted over a list of 7 LiDAR and DVS datasets. The results demonstrated that the proposed system had state-of-the-art recognition accuracy while achieving remarkable time efficiency. Recognition delay was shown to reduce by 56.3% to 91.7% in various experiment settings over the popular KITTI dataset. </details>
<details>	<summary>注释</summary>	Shibo Zhou and Wei Wang contributed equally to this work ACM-class: I.5.1; I.5.4; I.2.6; I.2.10 </details>
<details>	<summary>邮件日期</summary>	2021年01月25日</details>

# 47、亚稳材料的自主合成
- [ ] Autonomous synthesis of metastable materials 
时间：2021年01月19日                         第一作者：Sebastian Ament                       [链接](https://arxiv.org/abs/2101.07385).                     
## 摘要：人工智能的自主实验为加速科学发现提供了新的范例。非平衡材料合成是复杂的、资源密集型实验的标志，其加速将是材料发现和发展的分水岭。非平衡合成相图的绘制最近通过高通量实验得到了加速，但由于参数空间太大而无法进行详尽的探索，因此仍然限制了材料的研究。我们演示了加速合成和探索亚稳材料通过分层自主实验所管辖的科学自主推理代理（SARA）。SARA集成了机器人材料的合成和表征以及一系列人工智能方法，有效地揭示了加工相图的结构。SARA设计了用于平行材料合成的横向梯度激光脉冲退火（lg-LSA）实验，并利用光谱技术快速识别相变。多维参数空间的有效探索是通过嵌套的主动学习（AL）循环来实现的，该循环建立在先进的机器学习模型之上，该模型结合了实验的基本物理以及端到端的不确定性量化。有了这一点，以及在多个尺度上的协调，SARA体现了人工智能对复杂科学任务的利用。我们通过自主绘制Bi$\u2$O$\u3$系统的合成相边界来证明它的性能，从而在建立一个合成相图时产生数量级的加速，该合成相图包括在室温下动力学稳定$\delta$-Bi$\u2$O$\u3$的条件，固体氧化物燃料电池等电化学技术的关键发展。
<details>	<summary>英文摘要</summary>	Autonomous experimentation enabled by artificial intelligence (AI) offers a new paradigm for accelerating scientific discovery. Non-equilibrium materials synthesis is emblematic of complex, resource-intensive experimentation whose acceleration would be a watershed for materials discovery and development. The mapping of non-equilibrium synthesis phase diagrams has recently been accelerated via high throughput experimentation but still limits materials research because the parameter space is too vast to be exhaustively explored. We demonstrate accelerated synthesis and exploration of metastable materials through hierarchical autonomous experimentation governed by the Scientific Autonomous Reasoning Agent (SARA). SARA integrates robotic materials synthesis and characterization along with a hierarchy of AI methods that efficiently reveal the structure of processing phase diagrams. SARA designs lateral gradient laser spike annealing (lg-LSA) experiments for parallel materials synthesis and employs optical spectroscopy to rapidly identify phase transitions. Efficient exploration of the multi-dimensional parameter space is achieved with nested active learning (AL) cycles built upon advanced machine learning models that incorporate the underlying physics of the experiments as well as end-to-end uncertainty quantification. With this, and the coordination of AL at multiple scales, SARA embodies AI harnessing of complex scientific tasks. We demonstrate its performance by autonomously mapping synthesis phase boundaries for the Bi$_2$O$_3$ system, leading to orders-of-magnitude acceleration in establishment of a synthesis phase diagram that includes conditions for kinetically stabilizing $\delta$-Bi$_2$O$_3$ at room temperature, a critical development for electrochemical technologies such as solid oxide fuel cells. </details>
<details>	<summary>邮件日期</summary>	2021年01月20日</details>

# 46、模拟七鳃鳗机器人在SpiNNaker和Loihi神经形态板上运行的脉冲中心模式发生器
- [ ] A Spiking Central Pattern Generator for the control of a simulated lamprey robot running on SpiNNaker and Loihi neuromorphic boards 
时间：2021年01月18日                         第一作者：Emmanouil Angelidis                       [链接](https://arxiv.org/abs/2101.07001).                     
## 摘要：中枢模式发生器（CPGs）模型长期以来被用来研究动物运动的神经机制，也被用作机器人研究的工具。在这项工作中，我们提出了一个脉冲CPG神经网络及其在神经形态硬件上的实现作为一种手段来控制一个模拟七鳃鳗模型。为了建立我们的CPG模型，我们采用了自然出现的动态系统，这些系统是通过在神经工程框架（NEF）中使用递归神经种群而产生的。我们定义了我们模型背后的数学公式，它由一个由高电平信号调制的耦合抽象振荡器系统组成，能够产生各种输出步态。我们证明，利用这种中央模式发生器模型的数学公式，可以将该模型转化为一个脉冲神经网络（SNN），该网络可以很容易地用SNN模拟器Nengo进行模拟。然后利用脉冲CPG模型生成不同场景下模拟七鳃鳗机器人模型的游动步态。我们证明，通过修改网络的输入（可以由感官信息提供），机器人可以在方向和速度上进行动态控制。该方法可推广应用于工程应用和科学研究中的其它类型的cpg。我们在两个神经形态平台上测试我们的系统，SpiNNaker和Loihi。最后，我们证明了这类脉冲算法在能量效率和计算速度方面显示了利用神经形态硬件理论优势的潜力。
<details>	<summary>英文摘要</summary>	Central Pattern Generators (CPGs) models have been long used to investigate both the neural mechanisms that underlie animal locomotion as well as a tool for robotic research. In this work we propose a spiking CPG neural network and its implementation on neuromorphic hardware as a means to control a simulated lamprey model. To construct our CPG model, we employ the naturally emerging dynamical systems that arise through the use of recurrent neural populations in the Neural Engineering Framework (NEF). We define the mathematical formulation behind our model, which consists of a system of coupled abstract oscillators modulated by high-level signals, capable of producing a variety of output gaits. We show that with this mathematical formulation of the Central Pattern Generator model, the model can be turned into a Spiking Neural Network (SNN) that can be easily simulated with Nengo, an SNN simulator. The spiking CPG model is then used to produce the swimming gaits of a simulated lamprey robot model in various scenarios. We show that by modifying the input to the network, which can be provided by sensory information, the robot can be controlled dynamically in direction and pace. The proposed methodology can be generalized to other types of CPGs suitable for both engineering applications and scientific research. We test our system on two neuromorphic platforms, SpiNNaker and Loihi. Finally, we show that this category of spiking algorithms shows a promising potential to exploit the theoretical advantages of neuromorphic hardware in terms of energy efficiency and computational speed. </details>
<details>	<summary>注释</summary>	25 pages, 15 figures </details>
<details>	<summary>邮件日期</summary>	2021年01月19日</details>

# 45、用于时空特征提取的卷积脉冲神经网络
- [ ] Convolutional Spiking Neural Networks for Spatio-Temporal Feature Extraction 
时间：2021年01月18日                         第一作者：Ali Samadzadeh                       [链接](https://arxiv.org/abs/2003.12346).                     
<details>	<summary>注释</summary>	10 pages, 7 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2021年01月20日</details>

# 44、方程：神经形态实现的脉冲驱动平衡传播
- [ ] EqSpike: Spike-driven Equilibrium Propagation for Neuromorphic Implementations 
时间：2021年01月15日                         第一作者：Erwann Martin                       [链接](https://arxiv.org/abs/2010.07859).                     
<details>	<summary>邮件日期</summary>	2021年01月18日</details>

# 43、概率脉冲神经网络的多样本在线学习
- [ ] Multi-Sample Online Learning for Probabilistic Spiking Neural Networks 
时间：2021年01月05日                         第一作者：Hyeryung Jang                        [链接](https://arxiv.org/abs/2007.11894).                     
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2021年01月06日</details>

# 42、人工脉冲量子神经元
- [ ] An Artificial Spiking Quantum Neuron 
时间：2020年12月30日                         第一作者：Lasse Bj{\o}rn Kristensen                       [链接](https://arxiv.org/abs/1907.06269).                     
<details>	<summary>邮件日期</summary>	2021年01月01日</details>

# 41、深Q网络向事件驱动脉冲神经网络转化的策略与基准
- [ ] Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking Neural Networks 
时间：2020年12月23日                         第一作者：Weihao Tan                       [链接](https://arxiv.org/abs/2009.14456).                     
<details>	<summary>注释</summary>	Accepted by AAAI2021 </details>
<details>	<summary>邮件日期</summary>	2020年12月24日</details>

# 40、生成模型的进化变分优化
- [ ] Evolutionary Variational Optimization of Generative Models 
时间：2020年12月22日                         第一作者：Jakob Drefs                       [链接](https://arxiv.org/abs/2012.12294).                     
## 摘要：我们结合两种流行的优化方法来推导生成模型的学习算法：变分优化和进化算法。利用截断后验概率作为变分分布族，实现了离散时滞生成模型的组合。截断后验概率的变分参数是一组潜在状态。通过将这些状态解释为个体的基因组，并利用变分下界来定义适应度，我们可以应用进化算法来实现变分循环。所使用的变分分布是非常灵活的，我们证明了进化算法可以有效地优化变分界。此外，变分回路通常适用（“黑盒”），无需分析推导。为了说明该方法的普遍适用性，我们将该方法应用于三种生成模型（使用噪声或贝叶斯网、二进制稀疏编码以及脉冲和板稀疏编码）。为了证明新的变分方法的有效性和效率，我们使用了图像去噪和修复的标准竞争基准。这些基准允许对各种方法进行定量比较，包括概率方法、深层确定性和生成性网络以及非局部图像处理方法。在“零镜头”学习（当只使用损坏的图像进行训练时）的范畴中，我们观察到进化变分算法在许多基准设置中显著改善了最新的状态。对于一个著名的修复基准，我们还观察到了各种算法的最新性能，尽管我们只对损坏的图像进行训练。总的来说，我们的研究强调了研究生成模型的优化方法以提高性能的重要性。
<details>	<summary>英文摘要</summary>	We combine two popular optimization approaches to derive learning algorithms for generative models: variational optimization and evolutionary algorithms. The combination is realized for generative models with discrete latents by using truncated posteriors as the family of variational distributions. The variational parameters of truncated posteriors are sets of latent states. By interpreting these states as genomes of individuals and by using the variational lower bound to define a fitness, we can apply evolutionary algorithms to realize the variational loop. The used variational distributions are very flexible and we show that evolutionary algorithms can effectively and efficiently optimize the variational bound. Furthermore, the variational loop is generally applicable ("black box") with no analytical derivations required. To show general applicability, we apply the approach to three generative models (we use noisy-OR Bayes Nets, Binary Sparse Coding, and Spike-and-Slab Sparse Coding). To demonstrate effectiveness and efficiency of the novel variational approach, we use the standard competitive benchmarks of image denoising and inpainting. The benchmarks allow quantitative comparisons to a wide range of methods including probabilistic approaches, deep deterministic and generative networks, and non-local image processing methods. In the category of "zero-shot" learning (when only the corrupted image is used for training), we observed the evolutionary variational algorithm to significantly improve the state-of-the-art in many benchmark settings. For one well-known inpainting benchmark, we also observed state-of-the-art performance across all categories of algorithms although we only train on the corrupted image. In general, our investigations highlight the importance of research on optimization methods for generative models to achieve performance improvements. </details>
<details>	<summary>邮件日期</summary>	2020年12月24日</details>

# 39、用直接训练的更大的脉冲神经网络进行更深入的研究
- [ ] Going Deeper With Directly-Trained Larger Spiking Neural Networks 
时间：2020年12月18日                         第一作者：Hanle Zheng                       [链接](https://arxiv.org/abs/2011.05280).                     
<details>	<summary>注释</summary>	12 pages, 6 figures, conference or other essential info </details>
<details>	<summary>邮件日期</summary>	2020年12月21日</details>

# 38、脉冲神经网络到神经形态硬件的热感知编译
- [ ] Thermal-Aware Compilation of Spiking Neural Networks to Neuromorphic Hardware 
时间：2020年12月17日                         第一作者：Twisha Titirsha                        [链接](https://arxiv.org/abs/2010.04773).                     
<details>	<summary>注释</summary>	Accepted for publication at LCPC 2020 </details>
<details>	<summary>邮件日期</summary>	2020年12月21日</details>

# 37、基于贝叶斯学习的二元权值脉冲神经网络训练
- [ ] BiSNN: Training Spiking Neural Networks with Binary Weights via Bayesian Learning 
时间：2020年12月15日                         第一作者：Hyeryung Jang                        [链接](https://arxiv.org/abs/2012.08300).                     
## 摘要：基于人工神经网络（ANN）的电池供电设备的推理可以通过限制突触权值为二进制，从而消除执行乘法的需要，从而提高能量效率。另一种新兴的方法依赖于使用脉冲神经网络（SNNs），这是一种受生物启发的动态事件驱动模型，通过使用二进制稀疏激活来提高能源效率。本文介绍了一种SNN模型，它结合了时间稀疏二进制激活和二进制权值的优点。推导了两种学习规则，第一种基于直通和代理梯度技术的组合，第二种基于贝叶斯范式。实验验证了全精度实现的性能损失，并证明了贝叶斯范式在精度和校准方面的优势。
<details>	<summary>英文摘要</summary>	Artificial Neural Network (ANN)-based inference on battery-powered devices can be made more energy-efficient by restricting the synaptic weights to be binary, hence eliminating the need to perform multiplications. An alternative, emerging, approach relies on the use of Spiking Neural Networks (SNNs), biologically inspired, dynamic, event-driven models that enhance energy efficiency via the use of binary, sparse, activations. In this paper, an SNN model is introduced that combines the benefits of temporally sparse binary activations and of binary weights. Two learning rules are derived, the first based on the combination of straight-through and surrogate gradient techniques, and the second based on a Bayesian paradigm. Experiments validate the performance loss with respect to full-precision implementations, and demonstrate the advantage of the Bayesian paradigm in terms of accuracy and calibration. </details>
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2020年12月16日</details>

# 36、脉冲神经元Hebbian和STDP学习权值的约束
- [ ] Constraints on Hebbian and STDP learned weights of a spiking neuron 
时间：2020年12月14日                         第一作者：Dominique Chu                        [链接](https://arxiv.org/abs/2012.07664).                     
## 摘要：我们从数学上分析了Hebbian和STDP学习规则对权值的限制，这些规则应用于权值归一化的脉冲神经元。在纯Hebbian学习的情况下，我们发现标准化的权值等于权值的提升概率，直到依赖于学习率的修正项，并且通常很小。对于STDP算法，可以导出类似的关系，其中标准化的权重值反映了权重的提升和降级概率之间的差异。这些关系实际上很有用，因为它们允许检查Hebbian和STDP算法的收敛性。另一个应用是新颖性检测。我们使用MNIST数据集演示了这一点。
<details>	<summary>英文摘要</summary>	We analyse mathematically the constraints on weights resulting from Hebbian and STDP learning rules applied to a spiking neuron with weight normalisation. In the case of pure Hebbian learning, we find that the normalised weights equal the promotion probabilities of weights up to correction terms that depend on the learning rate and are usually small. A similar relation can be derived for STDP algorithms, where the normalised weight values reflect a difference between the promotion and demotion probabilities of the weight. These relations are practically useful in that they allow checking for convergence of Hebbian and STDP algorithms. Another application is novelty detection. We demonstrate this using the MNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2020年12月15日</details>

# 35、生物神经网络的低阶模型
- [ ] Low-Order Model of Biological Neural Networks 
时间：2020年12月12日                         第一作者：Huachuan Wang                        [链接](https://arxiv.org/abs/2012.06720).                     
## 摘要：生物神经网络的生物似真低阶模型（LOM）是由树突节点/树、脉冲/非脉冲神经元、无监督/有监督协方差/累积学习机制、反馈连接和最大泛化方案组成的递归层次网络。这些组件模型的动机和必要性在于使LOM易于学习和检索，而无需区分、优化或迭代，以及聚类、检测和识别多个/层次损坏、扭曲和闭塞的时间和空间模式。
<details>	<summary>英文摘要</summary>	A biologically plausible low-order model (LOM) of biological neural networks is a recurrent hierarchical network of dendritic nodes/trees, spiking/nonspiking neurons, unsupervised/ supervised covariance/accumulative learning mechanisms, feedback connections, and a scheme for maximal generalization. These component models are motivated and necessitated by making LOM learn and retrieve easily without differentiation, optimization, or iteration, and cluster, detect and recognize multiple/hierarchical corrupted, distorted, and occluded temporal and spatial patterns. </details>
<details>	<summary>邮件日期</summary>	2020年12月15日</details>

# 34、脉冲神经网络第一部分：空间模式检测
- [ ] Spiking Neural Networks -- Part I: Detecting Spatial Patterns 
时间：2020年12月09日                         第一作者：Hyeryung Jang                       [链接](https://arxiv.org/abs/2010.14208).                     
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 33、脉冲神经网络第二部分：时空模式检测
- [ ] Spiking Neural Networks -- Part II: Detecting Spatio-Temporal Patterns 
时间：2020年12月09日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2010.14217).                     
<details>	<summary>注释</summary>	Submitted. The first two authors have equally contributed to this work </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 32、脉冲神经网络第三部分：神经形态通信
- [ ] Spiking Neural Networks -- Part III: Neuromorphic Communications 
时间：2020年12月09日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2010.14220).                     
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 31、一种训练脉冲神经网络的多智能体进化机器人框架
- [ ] A multi-agent evolutionary robotics framework to train spiking neural networks 
时间：2020年12月07日                         第一作者：Souvik Das                       [链接](https://arxiv.org/abs/2012.03485).                     
## 摘要：提出了一种基于多智能体进化机器人（ER）的训练脉冲神经网络（SNN）的新框架。snn群体的权重以及它们在ER环境中控制的机器人的形态参数被视为表型。该框架的规则根据某些机器人在竞争环境中捕获食物的效率，选择它们及其snn进行繁殖，而选择其他snn进行淘汰。虽然机器人和它们的snn没有通过任何损失函数获得生存或繁衍的明确奖励，但当它们进化到捕猎食物并在这些规则下生存时，这些驱动力隐而不露。它们捕获食物的效率随着世代的变化而呈现出间断平衡的进化特征。给出了两种表型遗传算法：变异遗传算法和带变异交叉遗传算法。通过对每种算法进行100个实验，比较了这些算法的性能。我们发现，在SNN中，带突变的交叉比仅带统计显著性差异的突变能提高40%的学习速度。
<details>	<summary>英文摘要</summary>	A novel multi-agent evolutionary robotics (ER) based framework, inspired by competitive evolutionary environments in nature, is demonstrated for training Spiking Neural Networks (SNN). The weights of a population of SNNs along with morphological parameters of bots they control in the ER environment are treated as phenotypes. Rules of the framework select certain bots and their SNNs for reproduction and others for elimination based on their efficacy in capturing food in a competitive environment. While the bots and their SNNs are given no explicit reward to survive or reproduce via any loss function, these drives emerge implicitly as they evolve to hunt food and survive within these rules. Their efficiency in capturing food as a function of generations exhibit the evolutionary signature of punctuated equilibria. Two evolutionary inheritance algorithms on the phenotypes, Mutation and Crossover with Mutation, are demonstrated. Performances of these algorithms are compared using ensembles of 100 experiments for each algorithm. We find that Crossover with Mutation promotes 40% faster learning in the SNN than mere Mutation with a statistically significant margin. </details>
<details>	<summary>注释</summary>	9 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2020年12月08日</details>

# 30、大脑的功能是否像一台使用相位三值计算的量子相位计算机？
- [ ] Does the brain function as a quantum phase computer using phase ternary computation? 
时间：2020年12月04日                         第一作者：Andrew Simon Johnson                        [链接](https://arxiv.org/abs/2012.06537).                     
## 摘要：在这里，我们提供的证据表明，神经通信的基本基础来自于压力脉冲/孤子，它能够以足够的时间精度进行计算，以克服任何处理错误。神经系统内的信号传递和计算是复杂而不同的现象。动作电位是塑性的，这使得动作电位峰值对于神经计算来说是一个不合适的固定点，但是动作电位阈值适合于这个目的。此外，由脉冲神经元计时的神经模型的运算速率低于克服加工误差所需的速率。以视网膜处理为例，我们证明了基于电缆理论的当代神经传导理论不适合解释视网膜和大脑其他部分的完整功能所需的短计算时间。此外，电缆理论不能帮助传播的行动电位，因为在激活阈值没有足够的电荷在激活地点连续离子通道静电开放。对大脑神经网络的解构表明它是一组量子相位计算机中的一员，其中图灵机是最简单的：大脑是另一个基于相位三值计算的计算机。然而，使用图灵机制的尝试无法解决视网膜的编码或智能的计算，因为基于图灵的计算机的技术是根本不同的。我们证明了大脑神经网络中的编码是基于量子的，其中量子有一个时间变量和一个相位基变量，这使得相位三值计算成为可能，正如之前在视网膜中所证明的那样。
<details>	<summary>英文摘要</summary>	Here we provide evidence that the fundamental basis of nervous communication is derived from a pressure pulse/soliton capable of computation with sufficient temporal precision to overcome any processing errors. Signalling and computing within the nervous system are complex and different phenomena. Action potentials are plastic and this makes the action potential peak an inappropriate fixed point for neural computation, but the action potential threshold is suitable for this purpose. Furthermore, neural models timed by spiking neurons operate below the rate necessary to overcome processing error. Using retinal processing as our example, we demonstrate that the contemporary theory of nerve conduction based on cable theory is inappropriate to account for the short computational time necessary for the full functioning of the retina and by implication the rest of the brain. Moreover, cable theory cannot be instrumental in the propagation of the action potential because at the activation-threshold there is insufficient charge at the activation site for successive ion channels to be electrostatically opened. Deconstruction of the brain neural network suggests that it is a member of a group of Quantum phase computers of which the Turing machine is the simplest: the brain is another based upon phase ternary computation. However, attempts to use Turing based mechanisms cannot resolve the coding of the retina or the computation of intelligence, as the technology of Turing based computers is fundamentally different. We demonstrate that that coding in the brain neural network is quantum based, where the quanta have a temporal variable and a phase-base variable enabling phase ternary computation as previously demonstrated in the retina. </details>
<details>	<summary>注释</summary>	16 pages, 7 figures. Key Words: Plasticity; Action potential; Timing; Error redaction; Synchronization; Quantum phase computation; Phase ternary computation; Retinal model ACM-class: I.2; J.3 </details>
<details>	<summary>邮件日期</summary>	2020年12月14日</details>

# 29、DIET-SNN：深脉冲神经网络中带泄漏和阈值优化的直接输入编码
- [ ] DIET-SNN: Direct Input Encoding With Leakage and Threshold Optimization in Deep Spiking Neural Networks 
时间：2020年12月02日                         第一作者：Nitin Rathi                       [链接](https://arxiv.org/abs/2008.03658).                     
<details>	<summary>邮件日期</summary>	2020年12月03日</details>

# 28、从头开始训练低潜伏期深脉冲神经网络的批标准化研究
- [ ] Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch 
时间：2020年11月30日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2010.01729).                     
<details>	<summary>邮件日期</summary>	2020年12月01日</details>

# 27、DIET-SNN：深脉冲神经网络中带泄漏和阈值优化的直接输入编码
- [ ] DIET-SNN: Direct Input Encoding With Leakage and Threshold Optimization in Deep Spiking Neural Networks 
时间：2020年11月29日                         第一作者：Nitin Rathi                       [链接](https://arxiv.org/abs/2008.03658).                     
<details>	<summary>邮件日期</summary>	2020年12月01日</details>

# 26、编译脉冲神经网络以减轻神经形态的硬件约束
- [ ] Compiling Spiking Neural Networks to Mitigate Neuromorphic Hardware Constraints 
时间：2020年11月27日                         第一作者：Adarsha Balaji                        [链接](https://arxiv.org/abs/2011.13965).                     
## 摘要：脉冲神经网络（SNNs）是在{resource}和{power}约束平台上进行时空模式识别的有效计算模型。在神经形态硬件上执行snn可以进一步降低这些平台的能耗。随着模型尺寸和复杂性的增加，将基于SNN的应用程序映射到基于tile的神经形态硬件变得越来越具有挑战性。这归因于神经突触核心的局限性，即。一种横杆，每个突触后神经元只能容纳固定数量的突触前连接。对于具有许多神经元和每个神经元的突触前连接的基于SNN的复杂模型，（1）在训练后可能需要修剪连接以适应交叉资源，导致模型质量的损失，例如准确性，（2）神经元和突触需要分块并放置在硬件的神经系统核心上，这可能导致延迟和能量消耗增加。在这项工作中，我们提出（1）一种新的展开技术，将具有许多突触前连接的神经元功能分解为一系列同质的神经单元，以显著提高交叉杆的利用率并保留所有突触前连接，（2）SpiNeMap，提出了一种在神经形态硬件上映射snn的新方法，旨在最小化能量消耗和峰值潜伏期。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are efficient computation models to perform spatio-temporal pattern recognition on {resource}- and {power}-constrained platforms. SNNs executed on neuromorphic hardware can further reduce energy consumption of these platforms. With increasing model size and complexity, mapping SNN-based applications to tile-based neuromorphic hardware is becoming increasingly challenging. This is attributed to the limitations of neuro-synaptic cores, viz. a crossbar, to accommodate only a fixed number of pre-synaptic connections per post-synaptic neuron. For complex SNN-based models that have many neurons and pre-synaptic connections per neuron, (1) connections may need to be pruned after training to fit onto the crossbar resources, leading to a loss in model quality, e.g., accuracy, and (2) the neurons and synapses need to be partitioned and placed on the neuro-sypatic cores of the hardware, which could lead to increased latency and energy consumption. In this work, we propose (1) a novel unrolling technique that decomposes a neuron function with many pre-synaptic connections into a sequence of homogeneous neural units to significantly improve the crossbar utilization and retain all pre-synaptic connections, and (2) SpiNeMap, a novel methodology to map SNNs on neuromorphic hardware with an aim to minimize energy consumption and spike latency. </details>
<details>	<summary>邮件日期</summary>	2020年12月01日</details>

# 25、一种用于在线学习的时态神经网络结构
- [ ] A Temporal Neural Network Architecture for Online Learning 
时间：2020年11月27日                         第一作者：James E. Smith                       [链接](https://arxiv.org/abs/2011.13844).                     
## 摘要：一个长期存在的观点是，通过模拟大脑新皮质的运作，脉冲神经网络（SNN）可以实现类似的理想特性：灵活的学习、速度和效率。时态神经网络（TNNs）是一种snn，用来传递和处理编码为相对峰值时间的信息（与峰值速率相反）。提出了一种TNN体系结构，并在在线监督分类的大背景下证明了TNN的操作。首先，通过无监督学习，TNN根据相似性将输入模式划分为多个簇。然后TNN将一个簇标识符传递给一个简单的在线监督解码器，解码器完成分类任务。TNN学习过程只使用每个突触的局部信号来调整突触的权重，聚类行为在全局范围内出现。系统架构是在抽象层描述的，类似于传统数字设计中的门和寄存器传输层。除了整体架构的特性之外，一些TNN组件对于这项工作来说是新的。虽然没有直接解决，但总体研究目标是TNNs的直接硬件实现。因此，所有的架构元素都很简单，处理的精度很低。重要的是，低精度导致学习时间非常快。使用历史悠久的MNIST数据集的仿真结果表明，学习时间比其他在线方法至少快一个数量级，同时提供了类似的错误率。
<details>	<summary>英文摘要</summary>	A long-standing proposition is that by emulating the operation of the brain's neocortex, a spiking neural network (SNN) can achieve similar desirable features: flexible learning, speed, and efficiency. Temporal neural networks (TNNs) are SNNs that communicate and process information encoded as relative spike times (in contrast to spike rates). A TNN architecture is proposed, and, as a proof-of-concept, TNN operation is demonstrated within the larger context of online supervised classification. First, through unsupervised learning, a TNN partitions input patterns into clusters based on similarity. The TNN then passes a cluster identifier to a simple online supervised decoder which finishes the classification task. The TNN learning process adjusts synaptic weights by using only signals local to each synapse, and clustering behavior emerges globally. The system architecture is described at an abstraction level analogous to the gate and register transfer levels in conventional digital design. Besides features of the overall architecture, several TNN components are new to this work. Although not addressed directly, the overall research objective is a direct hardware implementation of TNNs. Consequently, all the architecture elements are simple, and processing is done at very low precision. Importantly, low precision leads to very fast learning times. Simulation results using the time-honored MNIST dataset demonstrate learning times at least an order of magnitude faster than other online approaches while providing similar error rates. </details>
<details>	<summary>注释</summary>	13 pages, 10 figures ACM-class: C.3; I.2.6; I.5.3 </details>
<details>	<summary>邮件日期</summary>	2020年11月30日</details>

# 24、结合可学习膜时间常数提高脉冲神经网络的学习能力
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2020年11月27日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>邮件日期</summary>	2020年11月30日</details>

# 23、PeleNet：Loihi的储层计算框架
- [ ] PeleNet: A Reservoir Computing Framework for Loihi 
时间：2020年11月24日                         第一作者：Carlo Michaelis                       [链接](https://arxiv.org/abs/2011.12338).                     
## 摘要：脉冲神经网络的高级框架是快速原型化和复杂算法高效开发的关键因素。在过去的几年里，这种框架已经出现在传统的计算机上，但是编程神经形态的硬件仍然是一个挑战。通常需要具备神经形态芯片硬件知识的低级编程。PeleNet框架旨在简化神经形态硬件Loihi的储层计算。它是在英特尔的NxSDK之上构建的，是用Python编写的。该框架管理权重矩阵、参数和探测。特别是，它提供了一个自动和有效的网络分布在几个核心和芯片。这样，用户就不用面对技术细节，可以集中精力进行实验。
<details>	<summary>英文摘要</summary>	High-level frameworks for spiking neural networks are a key factor for fast prototyping and efficient development of complex algorithms. Such frameworks have emerged in the last years for traditional computers, but programming neuromorphic hardware is still a challenge. Often low level programming with knowledge about the hardware of the neuromorphic chip is required. The PeleNet framework aims to simplify reservoir computing for the neuromorphic hardware Loihi. It is build on top of the NxSDK from Intel and is written in Python. The framework manages weight matrices, parameters and probes. In particular, it provides an automatic and efficient distribution of networks over several cores and chips. With this, the user is not confronted with technical details and can concentrate on experiments. </details>
<details>	<summary>邮件日期</summary>	2020年11月26日</details>

# 22、面向零镜头跨语言图像检索
- [ ] Towards Zero-shot Cross-lingual Image Retrieval 
时间：2020年11月24日                         第一作者：Pranav Aggarwal                       [链接](https://arxiv.org/abs/2012.05107).                     
## 摘要：最近人们对多模态语言和视觉问题的兴趣激增。在语言方面，由于大多数多模态数据集都是单语的，所以这些模型主要关注英语。我们试图通过在文本方面进行跨语言预训练的零镜头方法来弥补这一差距。我们提出了一个简单而实用的方法来建立一个跨语言图像检索模型，该模型在单语训练数据集上进行训练，但可以在推理过程中以零镜头的跨语言方式使用。我们还引入了一个新的目标函数，通过相互推送不同的文本来收紧文本嵌入簇。最后，我们介绍了一个新的1K多语种MSCOCO2014字幕测试数据集（XTD10），该数据集采用7种语言，我们使用众包平台收集。我们使用它作为跨语言评估零炮模型性能的测试集。XTD10数据集在以下位置公开：https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10
<details>	<summary>英文摘要</summary>	There has been a recent spike in interest in multi-modal Language and Vision problems. On the language side, most of these models primarily focus on English since most multi-modal datasets are monolingual. We try to bridge this gap with a zero-shot approach for learning multi-modal representations using cross-lingual pre-training on the text side. We present a simple yet practical approach for building a cross-lingual image retrieval model which trains on a monolingual training dataset but can be used in a zero-shot cross-lingual fashion during inference. We also introduce a new objective function which tightens the text embedding clusters by pushing dissimilar texts from each other. Finally, we introduce a new 1K multi-lingual MSCOCO2014 caption test dataset (XTD10) in 7 languages that we collected using a crowdsourcing platform. We use this as the test set for evaluating zero-shot model performance across languages. XTD10 dataset is made publicly available here: https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10 </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 21、一种更具生物学意义的人工神经网络局部学习规则
- [ ] A More Biologically Plausible Local Learning Rule for ANNs 
时间：2020年11月24日                         第一作者：Shashi Kant Gupta                       [链接](https://arxiv.org/abs/2011.12012).                     
## 摘要：反向传播算法因其生物学合理性而经常引起争论。然而，为了寻求更具生物学意义的学习，人们提出了各种神经结构的学习方法。他们中的大多数人都试图解决“重量传输问题”，并试图通过一些替代方法在体系结构中向后传播错误。在这项工作中，我们研究了一种稍有不同的方法，它只使用局部信息来捕获脉冲定时信息，而不会传播错误。所提出的学习规则来自于脉冲时间依赖的可塑性和神经元联系的概念。对具有两个隐藏层的MNIST和IRIS数据集的二元分类进行的初步评估表明，其性能与反向传播相当。与通过交叉熵损失反向传播学习的模型相比，使用该方法学习的模型对FGSM攻击具有更好的鲁棒性。学习的局部性为网络中大规模的分布式并行学习提供了可能。最后，提出的方法是一个更符合生物学的方法，可能有助于理解生物神经元如何学习不同的抽象。
<details>	<summary>英文摘要</summary>	The backpropagation algorithm is often debated for its biological plausibility. However, various learning methods for neural architecture have been proposed in search of more biologically plausible learning. Most of them have tried to solve the "weight transport problem" and try to propagate errors backward in the architecture via some alternative methods. In this work, we investigated a slightly different approach that uses only the local information which captures spike timing information with no propagation of errors. The proposed learning rule is derived from the concepts of spike timing dependant plasticity and neuronal association. A preliminary evaluation done on the binary classification of MNIST and IRIS datasets with two hidden layers shows comparable performance with backpropagation. The model learned using this method also shows a possibility of better adversarial robustness against the FGSM attack compared to the model learned through backpropagation of cross-entropy loss. The local nature of learning gives a possibility of large scale distributed and parallel learning in the network. And finally, the proposed method is a more biologically sound method that can probably help in understanding how biological neurons learn different abstractions. </details>
<details>	<summary>注释</summary>	8 pages (4 main + 1 reference + 3 supplementary) </details>
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 20、从头开始训练低潜伏期深脉冲神经网络的批标准化研究
- [ ] Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch 
时间：2020年11月24日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2010.01729).                     
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 19、结合可学习膜时间常数提高脉冲神经网络的学习能力
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2020年11月24日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 18、脉冲神经元的自然梯度学习
- [ ] Natural-gradient learning for spiking neurons 
时间：2020年11月23日                         第一作者：Elena Kreutzer                       [链接](https://arxiv.org/abs/2011.11710).                     
## 摘要：在许多突触可塑性的规范理论中，权重的更新隐含地依赖于所选择的权重参数化。例如，这个问题与神经元形态有关：在功能上对躯体放电的影响相当的突触，由于其在树突树上的位置不同，其脊柱大小可能有很大差异。基于欧氏梯度下降的经典理论很容易由于这种参数化依赖而导致不一致。这些问题是在黎曼几何的框架下解决的，在黎曼几何中，我们提出塑性应遵循自然梯度下降。在这一假设下，我们推导出了一个突触学习规则，该规则将功能效率与树突状民主、乘法标度和异突触可塑性等生物学现象的解释结合起来。因此，我们认为，在寻找功能性突触可塑性的过程中，进化可能产生了自己版本的自然梯度下降。
<details>	<summary>英文摘要</summary>	In many normative theories of synaptic plasticity, weight updates implicitly depend on the chosen parametrization of the weights. This problem relates, for example, to neuronal morphology: synapses which are functionally equivalent in terms of their impact on somatic firing can differ substantially in spine size due to their different positions along the dendritic tree. Classical theories based on Euclidean gradient descent can easily lead to inconsistencies due to such parametrization dependence. The issues are solved in the framework of Riemannian geometry, in which we propose that plasticity instead follows natural gradient descent. Under this hypothesis, we derive a synaptic learning rule for spiking neurons that couples functional efficiency with the explanation of several well-documented biological phenomena such as dendritic democracy, multiplicative scaling and heterosynaptic plasticity. We therefore suggest that in its search for functional synaptic plasticity, evolution might have come up with its own version of natural gradient descent. </details>
<details>	<summary>注释</summary>	Joint senior authorship: Walter M. Senn and Mihai A. Petrovici </details>
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 17、多层记忆脉冲神经网络的片上错误触发学习
- [ ] On-Chip Error-triggered Learning of Multi-layer Memristive Spiking Neural Networks 
时间：2020年11月21日                         第一作者：Melika Payv                       [链接](https://arxiv.org/abs/2011.10852).                     
## 摘要：神经形态计算的最新突破表明，梯度下降学习的局部形式与脉冲神经网络（SNNs）和突触可塑性是相容的。虽然SNNs可以用神经形态的VLSI可伸缩地实现，但是仍然缺少一种可以在原地使用梯度下降进行学习的体系结构。在本文中，我们提出了一个局部的，梯度为基础的，错误触发学习算法与在线三元权值更新。所提出的算法可以在线训练多层snn与记忆神经形态的硬件表现出小损失的性能相比，国家的最新技术。我们还提出了一种基于忆阻纵横制阵列的硬件结构来执行所需的向量矩阵乘法。采用标准180nmcmos工艺，在亚阈值范围内设计了在线训练所需的外围电路，包括突触前、突触后和写电路。
<details>	<summary>英文摘要</summary>	Recent breakthroughs in neuromorphic computing show that local forms of gradient descent learning are compatible with Spiking Neural Networks (SNNs) and synaptic plasticity. Although SNNs can be scalably implemented using neuromorphic VLSI, an architecture that can learn using gradient-descent in situ is still missing. In this paper, we propose a local, gradient-based, error-triggered learning algorithm with online ternary weight updates. The proposed algorithm enables online training of multi-layer SNNs with memristive neuromorphic hardware showing a small loss in the performance compared with the state of the art. We also propose a hardware architecture based on memristive crossbar arrays to perform the required vector-matrix multiplications. The necessary peripheral circuitry including pre-synaptic, post-synaptic and write circuits required for online training, have been designed in the sub-threshold regime for power saving with a standard 180 nm CMOS process. </details>
<details>	<summary>注释</summary>	15 pages, 11 figures, Journal of Emerging Technology in Circuits and Systems (JETCAS) </details>
<details>	<summary>邮件日期</summary>	2020年11月24日</details>

# 16、快速而深入：具有第一脉冲时间的节能神经形态学习
- [ ] Fast and deep: energy-efficient neuromorphic learning with first-spike times 
时间：2020年11月19日                         第一作者：Julian G\"oltz                       [链接](https://arxiv.org/abs/1912.11443).                     
<details>	<summary>注释</summary>	20 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2020年11月20日</details>

# 15、基于生物似然无监督延迟学习的脉冲神经网络时间特征提取
- [ ] Bio-plausible Unsupervised Delay Learning for Extracting Temporal Features in Spiking Neural Networks 
时间：2020年11月18日                         第一作者：Alireza Nadafian                       [链接](https://arxiv.org/abs/2011.09380).                     
## 摘要：神经元间传导延迟的可塑性在学习中起着基础性作用。然而，大脑中这种调节的确切机制仍然是一个开放的问题。了解突触延迟的精确调节可以帮助我们开发有效的大脑启发计算模型，提供与实验证据一致的见解。在这篇论文中，我们提出一个无监督的生物学上合理的学习规则来调整神经网络中的突触延迟。然后，我们提供了一些数学证明来证明我们的学习规则赋予神经元学习重复时空模式的能力。此外，将基于STDP的脉冲神经网络与我们提出的延迟学习规则相结合，应用于随机点运动图的实验结果表明了所提出的延迟学习规则在提取时间特征方面的有效性。
<details>	<summary>英文摘要</summary>	The plasticity of the conduction delay between neurons plays a fundamental role in learning. However, the exact underlying mechanisms in the brain for this modulation is still an open problem. Understanding the precise adjustment of synaptic delays could help us in developing effective brain-inspired computational models in providing aligned insights with the experimental evidence. In this paper, we propose an unsupervised biologically plausible learning rule for adjusting the synaptic delays in spiking neural networks. Then, we provided some mathematical proofs to show that our learning rule gives a neuron the ability to learn repeating spatio-temporal patterns. Furthermore, the experimental results of applying an STDP-based spiking neural network equipped with our proposed delay learning rule on Random Dot Kinematogram indicate the efficacy of the proposed delay learning rule in extracting temporal features. </details>
<details>	<summary>邮件日期</summary>	2020年11月19日</details>

# 14、脉冲神经网络的时间代理反向传播算法
- [ ] Temporal Surrogate Back-propagation for Spiking Neural Networks 
时间：2020年11月18日                         第一作者：Yukun Yang                       [链接](https://arxiv.org/abs/2011.09964).                     
## 摘要：脉冲神经网络（SNN）通常比人工神经网络（ANN）更节能，其工作方式与我们的大脑有很大的相似性。近年来，BP算法在神经网络训练中显示出了强大的能力。然而，由于脉冲行为是不可微的，BP不能直接应用于SNN。虽然已有的工作证明了在空间和时间方向上通过替代梯度或随机性来逼近BP梯度的几种方法，但是它们忽略了每一步之间重置机制引入的时间依赖性。本文以理论完善为目标，深入研究了缺失项的影响。通过增加重置机制的时间依赖性，新算法对玩具数据集的学习率调整更具鲁棒性，但对CIFAR-10等较大的学习任务没有太大的改进。从经验上讲，缺失项的好处不值得额外的计算开销。在许多情况下，可以忽略缺少的项。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) are usually more energy-efficient as compared to Artificial neural networks (ANN), and the way they work has a great similarity with our brain. Back-propagation (BP) has shown its strong power in training ANN in recent years. However, since spike behavior is non-differentiable, BP cannot be applied to SNN directly. Although prior works demonstrated several ways to approximate the BP-gradient in both spatial and temporal directions either through surrogate gradient or randomness, they omitted the temporal dependency introduced by the reset mechanism between each step. In this article, we target on theoretical completion and investigate the effect of the missing term thoroughly. By adding the temporal dependency of the reset mechanism, the new algorithm is more robust to learning-rate adjustments on a toy dataset but does not show much improvement on larger learning tasks like CIFAR-10. Empirically speaking, the benefits of the missing term are not worth the additional computational overhead. In many cases, the missing term can be ignored. </details>
<details>	<summary>注释</summary>	4 pases, 3 figures, 3 tables, 10 eqs </details>
<details>	<summary>邮件日期</summary>	2020年11月20日</details>

# 13、一种用于术中心电图高频振荡检测的脉冲神经网络（SNN）
- [ ] A Spiking Neural Network (SNN) for detecting High Frequency Oscillations (HFOs) in the intraoperative ECoG 
时间：2020年11月17日                         第一作者：Karla Burelo                        [链接](https://arxiv.org/abs/2011.08783).                     
## 摘要：癫痫手术需要彻底切除致痫脑组织，才能实现癫痫发作的自由。在术中的ECoG记录中，由致痫组织产生的高频振荡（HFOs）可以用来调整切除边缘。然而，实时自动检测HFOs仍然是一个开放的挑战。在这里，我们提出了一个脉冲神经网络（SNN）的自动HFO检测，是最适合神经形态的硬件实现。我们训练SNN来检测术中ECoG在线测量的HFO信号，使用一个独立标记的数据集。我们针对快速纹波频率范围（250-500hz）的HFO检测，并将网络结果与标记的HFO数据进行比较。我们赋予SNN一种新的伪影抑制机制来抑制突变，并在ECoG数据集上验证了其有效性。该SNN检测到的HFO率（术前记录中位数为6.6 HFO/min）与数据集中公布的HFO率（58 min，16次记录）相当。所有8例患者术后癫痫发作结果的“预测”准确率均为100%。这些结果为建立一个可在癫痫手术中用于指导致痫区切除的实时便携式电池供电HFO检测系统提供了进一步的进展。
<details>	<summary>英文摘要</summary>	To achieve seizure freedom, epilepsy surgery requires the complete resection of the epileptogenic brain tissue. In intraoperative ECoG recordings, high frequency oscillations (HFOs) generated by epileptogenic tissue can be used to tailor the resection margin. However, automatic detection of HFOs in real-time remains an open challenge. Here we present a spiking neural network (SNN) for automatic HFO detection that is optimally suited for neuromorphic hardware implementation. We trained the SNN to detect HFO signals measured from intraoperative ECoG on-line, using an independently labeled dataset. We targeted the detection of HFOs in the fast ripple frequency range (250-500 Hz) and compared the network results with the labeled HFO data. We endowed the SNN with a novel artifact rejection mechanism to suppress sharp transients and demonstrate its effectiveness on the ECoG dataset. The HFO rates (median 6.6 HFO/min in pre-resection recordings) detected by this SNN are comparable to those published in the dataset (58 min, 16 recordings). The postsurgical seizure outcome was "predicted" with 100% accuracy for all 8 patients. These results provide a further step towards the construction of a real-time portable battery-operated HFO detection system that can be used during epilepsy surgery to guide the resection of the epileptogenic zone. </details>
<details>	<summary>注释</summary>	11 pages, 3 figures, 2 tables. The results of this publication were obtained by simulating our hardware platform, built for online processing of biological signals. This hardware combines neural recording headstages with a multi-core neuromorphic processor arxiv.org/abs/2009.11245 </details>
<details>	<summary>邮件日期</summary>	2020年11月18日</details>

# 12、结合可学习膜时间常数提高脉冲神经网络的学习能力
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2020年11月16日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>邮件日期</summary>	2020年11月17日</details>

# 11、具有Alpha突触功能的脉冲神经网络中的时间编码：反向传播学习
- [ ] Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function: Learning with Backpropagation 
时间：2020年11月16日                         第一作者：Iulia M. Comsa                       [链接](https://arxiv.org/abs/1907.13223).                     
<details>	<summary>注释</summary>	Open-source code related to this paper is available at https://github.com/google/ihmehimmeli v2: Added references and added some clarifications for the methods </details>
<details>	<summary>邮件日期</summary>	2020年11月18日</details>

# 10、LIAF-Net：轻量级高效时空信息处理的漏泄集成模拟消防网络
- [ ] LIAF-Net: Leaky Integrate and Analog Fire Network for Lightweight and Efficient Spatiotemporal Information Processing 
时间：2020年11月12日                         第一作者：Zhenzhi Wu                       [链接](https://arxiv.org/abs/2011.06176).                     
## 摘要：基于漏积分火灾（LIF）模型的脉冲神经网络（SNNs）已被应用于节能的时空处理任务中。由于生物似有理的神经元动力学和简单性，LIF-SNN受益于事件驱动处理，然而，通常面临性能下降的尴尬。这可能是因为在LIF-SNN中，神经元通过脉冲传递信息。为了解决这一问题，本文提出了一种漏积分模拟火灾（LIAF）神经元模型，使得模拟值可以在神经元之间传输，并在此基础上建立了一个称为LIAF网络的深层网络，以实现高效的时空处理。在时域上，LIAF遵循传统的LIF动态机制来保持其时间处理能力。在空间域中，LIAF能够通过卷积积分或全连通积分对空间信息进行集成。作为一个时空层，LIAF也可以与传统的人工神经网络（ANN）层联合使用。实验结果表明，在bAbI问答（QA）任务中，LIAF网络的性能与选通递归单元（GRU）和长短时记忆（LSTM）相当，在时空动态视觉传感器（DVS）数据集（包括MNIST-DVS、CIFAR10-DVS和DVS128手势）上，LIAF网络的性能达到了最先进的水平，但数量要少得多与传统的LSTM、GRU、卷积LSTM（ConvLSTM）或3D卷积（Conv3D）构建的网络相比，突触权值和计算开销都有较大的提高。与传统的LIF-SNN相比，LIAF网络在所有这些实验中也显示出显著的精度提高。总之，LIAF-Net提供了一个结合ANNs和SNNs优点的轻量级高效时空信息处理框架。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) based on Leaky Integrate and Fire (LIF) model have been applied to energy-efficient temporal and spatiotemporal processing tasks. Thanks to the bio-plausible neuronal dynamics and simplicity, LIF-SNN benefits from event-driven processing, however, usually faces the embarrassment of reduced performance. This may because in LIF-SNN the neurons transmit information via spikes. To address this issue, in this work, we propose a Leaky Integrate and Analog Fire (LIAF) neuron model, so that analog values can be transmitted among neurons, and a deep network termed as LIAF-Net is built on it for efficient spatiotemporal processing. In the temporal domain, LIAF follows the traditional LIF dynamics to maintain its temporal processing capability. In the spatial domain, LIAF is able to integrate spatial information through convolutional integration or fully-connected integration. As a spatiotemporal layer, LIAF can also be used with traditional artificial neural network (ANN) layers jointly. Experiment results indicate that LIAF-Net achieves comparable performance to Gated Recurrent Unit (GRU) and Long short-term memory (LSTM) on bAbI Question Answering (QA) tasks, and achieves state-of-the-art performance on spatiotemporal Dynamic Vision Sensor (DVS) datasets, including MNIST-DVS, CIFAR10-DVS and DVS128 Gesture, with much less number of synaptic weights and computational overhead compared with traditional networks built by LSTM, GRU, Convolutional LSTM (ConvLSTM) or 3D convolution (Conv3D). Compared with traditional LIF-SNN, LIAF-Net also shows dramatic accuracy gain on all these experiments. In conclusion, LIAF-Net provides a framework combining the advantages of both ANNs and SNNs for lightweight and efficient spatiotemporal information processing. </details>
<details>	<summary>注释</summary>	14 pages, 9 figures, submitted to IEEE Transactions on Neural Networks and Learning Systems ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2020年11月13日</details>

# 9、利用生物似然奖赏传播调整卷积脉冲神经网络
- [x] Tuning Convolutional Spiking Neural Network with Biologically-plausible Reward Propagation 
时间：2020年11月12日                         第一作者：Tielin Zhang                        [链接](https://arxiv.org/abs/2010.04434).                     
<details>	<summary>邮件日期</summary>	2020年11月13日</details>

# 8、基于VCSEL神经元的全光神经形态二值卷积算法
- [ ] All-optical neuromorphic binary convolution with a spiking VCSEL neuron for image gradient magnitudes 
时间：2020年11月09日                         第一作者：Yahui Zhang                       [链接](https://arxiv.org/abs/2011.04438).                     
## 摘要：首次提出了一种基于光子脉冲垂直腔面发射激光器（VCSEL）神经元的全光二值卷积方法，并进行了实验验证。从数字图像中提取并使用矩形脉冲进行时间编码的光输入被注入VCSEL神经元中，VCSEL神经元提供快速（<100ps长）脉冲发射数的卷积结果。实验和数值结果表明，采用单脉冲VCSEL神经元实现了二值卷积，全光二值卷积可用于计算图像梯度大小，检测边缘特征，分离源图像中的垂直分量和水平分量。我们还证明了这种全光脉冲二值卷积系统对噪声具有很强的鲁棒性，并且可以处理高分辨率的图像。此外，该系统还具有速度快、能量效率高、硬件实现简单等优点，突出了脉冲光子VCSEL神经元在高速神经图像处理系统和未来光子脉冲卷积神经网络中的应用潜力。
<details>	<summary>英文摘要</summary>	All-optical binary convolution with a photonic spiking vertical-cavity surface-emitting laser (VCSEL) neuron is proposed and demonstrated experimentally for the first time. Optical inputs, extracted from digital images and temporally encoded using rectangular pulses, are injected in the VCSEL neuron which delivers the convolution result in the number of fast (<100 ps long) spikes fired. Experimental and numerical results show that binary convolution is achieved successfully with a single spiking VCSEL neuron and that all-optical binary convolution can be used to calculate image gradient magnitudes to detect edge features and separate vertical and horizontal components in source images. We also show that this all-optical spiking binary convolution system is robust to noise and can operate with high-resolution images. Additionally, the proposed system offers important advantages such as ultrafast speed, high energy efficiency and simple hardware implementation, highlighting the potentials of spiking photonic VCSEL neurons for high-speed neuromorphic image processing systems and future photonic spiking convolutional neural networks. </details>
<details>	<summary>注释</summary>	jxxsy@126.com; antonio.hurtado@strath.ac.uk </details>
<details>	<summary>邮件日期</summary>	2020年11月10日</details>

# 7、用gpu快速模拟高度连接的棘波皮层模型
- [ ] Fast simulations of highly-connected spiking cortical models using GPUs 
时间：2020年11月09日                         第一作者：Bruno Golosio                       [链接](https://arxiv.org/abs/2007.14236).                     
<details>	<summary>邮件日期</summary>	2020年11月10日</details>

# 6、你只刺一次：提高能源效率神经形态推理到神经网络水平的准确性
- [ ] You Only Spike Once: Improving Energy-Efficient Neuromorphic Inference to ANN-Level Accuracy 
时间：2020年11月08日                         第一作者：Srivatsa P                        [链接](https://arxiv.org/abs/2006.09982).                     
<details>	<summary>注释</summary>	10 pages, 4 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work is an extended version of the paper accepted to the 2nd Workshop on Accelerated Machine Learning (AccML 2020) </details>
<details>	<summary>邮件日期</summary>	2020年11月10日</details>

# 5、深脉冲神经网络反向传播的校正线性突触后电位函数
- [x] Rectified Linear Postsynaptic Potential Function for Backpropagation in Deep Spiking Neural Networks 
时间：2020年11月04日                         第一作者：Malu Zhang                       [链接](https://arxiv.org/abs/2003.11837).                     
<details>	<summary>注释</summary>	This work has been submitted to the IEEE for possible publication. Copyrightmay be transferred without notice, after which this version may no longer beaccessible </details>
<details>	<summary>邮件日期</summary>	2020年11月05日</details>

# 4、脉冲耦合振荡器网络中的受控微扰诱导开关
- [x] Controlled Perturbation-Induced Switching in Pulse-Coupled Oscillator Networks 
时间：2020年11月02日                         第一作者：Fabio Schittler Neves                        [链接](https://arxiv.org/abs/2011.00888).                     
## 摘要：脉冲耦合系统，如脉冲神经网络，表现出非平凡不变集的形式吸引但不稳定的鞍周期轨道的单位同步成组。这些轨道之间的异宿连接原则上可以支持这些网络中的切换过程，并支持新的神经计算。对于耦合振子的小网络，我们在此研究在何种条件下以及系统对称性如何强制或禁止某些可能由扰动引起的开关跃迁。对于由五个振子组成的网络，我们导出了两个团簇对称性的显式跃迁规则，这些规则偏离了已知的连续耦合振子的跃迁规则。第三种对称产生异宿网络，它由所有具有这种对称性的不稳定吸引子以及它们之间的连接组成。我们的结果表明，脉冲耦合系统能够可靠地产生符合特定转换规则的复杂时空模式。我们简要地讨论了脉冲神经系统计算的可能含义。
<details>	<summary>英文摘要</summary>	Pulse-coupled systems such as spiking neural networks exhibit nontrivial invariant sets in the form of attracting yet unstable saddle periodic orbits where units are synchronized into groups. Heteroclinic connections between such orbits may in principle support switching processes in those networks and enable novel kinds of neural computations. For small networks of coupled oscillators we here investigate under which conditions and how system symmetry enforces or forbids certain switching transitions that may be induced by perturbations. For networks of five oscillators we derive explicit transition rules that for two cluster symmetries deviate from those known from oscillators coupled continuously in time. A third symmetry yields heteroclinic networks that consist of sets of all unstable attractors with that symmetry and the connections between them. Our results indicate that pulse-coupled systems can reliably generate well-defined sets of complex spatiotemporal patterns that conform to specific transition rules. We briefly discuss possible implications for computation with spiking neural systems. </details>
<details>	<summary>邮件日期</summary>	2020年11月03日</details>

# 3、RANC：可重构的神经形态计算体系结构
- [ ] RANC: Reconfigurable Architecture for Neuromorphic Computing 
时间：2020年11月01日                         第一作者：Joshua Mack                       [链接](https://arxiv.org/abs/2011.00624).                     
## 摘要：神经形态结构已经被引入作为能量有效的脉冲神经网络执行的平台。这些体系结构所提供的大规模并行性也引起了非机器学习应用领域的兴趣。为了提升硬件设计者和应用开发者的进入壁垒，我们提出了RANC：一种可重构的神经形态计算体系结构，一个开源的高度灵活的生态系统，通过C++仿真和硬件通过FPGA仿真，能够快速地在软件中对神经形态结构进行实验。我们展示了RANC生态系统的实用性，通过展示其重现IBM的TrueNorth行为的能力，并通过与IBM的Compass模拟环境和已发表文献的直接比较进行验证。RANC允许基于应用程序洞察优化架构，以及原型化可以完全支持新类应用程序的未来神经形态架构。通过基于Alveo U250 FPGA的定量分析，研究了体系结构变化对提高应用程序映射效率的影响，证明了RANC的高度参数化和可配置性。本文介绍了合成孔径雷达分类和矢量矩阵乘法应用的路由后资源使用和吞吐量分析，并展示了一个可扩展到模拟259K个不同神经元和733m个不同突触的神经形态结构。
<details>	<summary>英文摘要</summary>	Neuromorphic architectures have been introduced as platforms for energy efficient spiking neural network execution. The massive parallelism offered by these architectures has also triggered interest from non-machine learning application domains. In order to lift the barriers to entry for hardware designers and application developers we present RANC: a Reconfigurable Architecture for Neuromorphic Computing, an open-source highly flexible ecosystem that enables rapid experimentation with neuromorphic architectures in both software via C++ simulation and hardware via FPGA emulation. We present the utility of the RANC ecosystem by showing its ability to recreate behavior of the IBM's TrueNorth and validate with direct comparison to IBM's Compass simulation environment and published literature. RANC allows optimizing architectures based on application insights as well as prototyping future neuromorphic architectures that can support new classes of applications entirely. We demonstrate the highly parameterized and configurable nature of RANC by studying the impact of architectural changes on improving application mapping efficiency with quantitative analysis based on Alveo U250 FPGA. We present post routing resource usage and throughput analysis across implementations of Synthetic Aperture Radar classification and Vector Matrix Multiplication applications, and demonstrate a neuromorphic architecture that scales to emulating 259K distinct neurons and 73.3M distinct synapses. </details>
<details>	<summary>注释</summary>	18 pages, 12 figures, accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. For associated source files see https://github.com/UA-RCL/RANC </details>
<details>	<summary>邮件日期</summary>	2020年11月03日</details>

# 2、基于Loihi处理器的mav光流着陆神经形态控制
- [x] Neuromorphic control for optic-flow-based landings of MAVs using the Loihi processor 
时间：2020年11月01日                         第一作者：Julien Dupeyroux                       [链接](https://arxiv.org/abs/2011.00534).                     
## 摘要：像Loihi这样的神经形态处理器为微型飞行器（mav）这样的受限系统提供了一个有希望的替代传统计算模块，使其具有强大、高效和自主的技能，如起飞和着陆、避障和追踪。然而，在机器人平台上使用这种处理器的一个主要挑战是模拟和真实世界之间的现实差距。在这项研究中，我们首次提出了一个完全嵌入式应用的Loihi神经芯片原型在飞行机器人。为了实现自主着陆，提出了一种基于腹部光流场发散的脉冲神经网络（SNN）来计算推力指令。进化是使用PySNN库在基于Python的模拟器中执行的。该网络结构仅由分布在3层中的35个神经元组成。仿真和Loihi之间的定量分析表明，推力设定值的均方根误差低至0.005 g，同时，隐藏层的脉冲序列匹配率为99.8%，输出层的脉冲序列匹配率为99.7%。所提出的方法成功地填补了现实差距，为未来机器人中的神经形态应用提供了重要的见解。补充材料可在https://mavlab.tudelft.nl/loihi/。
<details>	<summary>英文摘要</summary>	Neuromorphic processors like Loihi offer a promising alternative to conventional computing modules for endowing constrained systems like micro air vehicles (MAVs) with robust, efficient and autonomous skills such as take-off and landing, obstacle avoidance, and pursuit. However, a major challenge for using such processors on robotic platforms is the reality gap between simulation and the real world. In this study, we present for the very first time a fully embedded application of the Loihi neuromorphic chip prototype in a flying robot. A spiking neural network (SNN) was evolved to compute the thrust command based on the divergence of the ventral optic flow field to perform autonomous landing. Evolution was performed in a Python-based simulator using the PySNN library. The resulting network architecture consists of only 35 neurons distributed among 3 layers. Quantitative analysis between simulation and Loihi reveals a root-mean-square error of the thrust setpoint as low as 0.005 g, along with a 99.8% matching of the spike sequences in the hidden layer, and 99.7% in the output layer. The proposed approach successfully bridges the reality gap, offering important insights for future neuromorphic applications in robotics. Supplementary material is available at https://mavlab.tudelft.nl/loihi/. </details>
<details>	<summary>邮件日期</summary>	2020年11月03日</details>

# 1、用直接训练的更大的脉冲神经网络进行更深入的研究
- [x] Going Deeper With Directly-Trained Larger Spiking Neural Networks 
时间：2020年10月29日                         第一作者：Hanle Zheng                       [链接](https://arxiv.org/abs/2011.05280).                     
## 摘要：脉冲神经网络（Spiking neural networks，SNNs）在时空信息和事件驱动信号处理的生物似然编码方面有着广阔的应用前景，非常适合于神经形态硬件的节能实现。然而，SNNs独特的工作模式使其比传统网络更难训练。目前，探索高性能深层snn的培养主要有两条途径。第一种方法是将预先训练好的神经网络模型转换为SNN模型，SNN模型通常需要较长的编码窗口才能收敛，并且在训练过程中不能利用时空特征来求解时间任务。另一种是直接在时空域训练snn。但是由于触发函数的二元脉冲活动和梯度消失或爆炸的问题，目前的方法局限于浅层结构，因此难以利用大规模数据集（如ImageNet）。为此，我们提出了一种基于时空反向传播的阈值相关批处理归一化（tdBN）方法，称为STBP-tdBN，它能够直接训练非常深的SNN并在神经形态硬件上有效地实现其推理。通过提出的方法和详细的快捷连接，我们将直接训练的snn从浅层（<10层）扩展到非常深的结构（50层）。在此基础上，从理论上分析了基于块动态等距理论的方法的有效性。最后，我们报告了更高的准确率结果，包括93.15%的CIFAR-10，67.8%的DVS-CIFAR10和67.05%的ImageNet与很少的时间步长。据我们所知，这是第一次在ImageNet上探索直接训练的高性能深度snn。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are promising in a bio-plausible coding for spatio-temporal information and event-driven signal processing, which is very suited for energy-efficient implementation in neuromorphic hardware. However, the unique working mode of SNNs makes them more difficult to train than traditional networks. Currently, there are two main routes to explore the training of deep SNNs with high performance. The first is to convert a pre-trained ANN model to its SNN version, which usually requires a long coding window for convergence and cannot exploit the spatio-temporal features during training for solving temporal tasks. The other is to directly train SNNs in the spatio-temporal domain. But due to the binary spike activity of the firing function and the problem of gradient vanishing or explosion, current methods are restricted to shallow architectures and thereby difficult in harnessing large-scale datasets (e.g. ImageNet). To this end, we propose a threshold-dependent batch normalization (tdBN) method based on the emerging spatio-temporal backpropagation, termed "STBP-tdBN", enabling direct training of a very deep SNN and the efficient implementation of its inference on neuromorphic hardware. With the proposed method and elaborated shortcut connection, we significantly extend directly-trained SNNs from a shallow structure ( < 10 layer) to a very deep structure (50 layers). Furthermore, we theoretically analyze the effectiveness of our method based on "Block Dynamical Isometry" theory. Finally, we report superior accuracy results including 93.15 % on CIFAR-10, 67.8 % on DVS-CIFAR10, and 67.05% on ImageNet with very few timesteps. To our best knowledge, it's the first time to explore the directly-trained deep SNNs with high performance on ImageNet. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures, conference or other essential info </details>
<details>	<summary>邮件日期</summary>	2020年11月11日</details>

